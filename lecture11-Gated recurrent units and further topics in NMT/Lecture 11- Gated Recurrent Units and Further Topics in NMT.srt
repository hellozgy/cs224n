1
00:00:00,000 --> 00:00:04,673
[MUSIC]

2
00:00:04,673 --> 00:00:07,669
Stanford University.

3
00:00:07,669 --> 00:00:08,759
>> Okay hi everyone.

4
00:00:08,759 --> 00:00:11,250
Let's get started again.

5
00:00:11,250 --> 00:00:19,640
We're back with we're into
week six now and Lecture 11.

6
00:00:20,740 --> 00:00:25,390
This is basically the third
now last of our lectures.

7
00:00:25,390 --> 00:00:30,910
It's sort of essentially concentrating on
what we can do with recurrent models and

8
00:00:30,910 --> 00:00:33,630
sequence to sequence architectures.

9
00:00:33,630 --> 00:00:39,490
I thought what I'd do in the first
part of the lecture is have one

10
00:00:39,490 --> 00:00:44,440
more attempt at explaining some
of the ideas about GRUs and

11
00:00:44,440 --> 00:00:48,140
LSTMs and where do they come from and
how do they work?

12
00:00:48,140 --> 00:00:51,680
I'd sort of decide to do
that anyway on the weekend,

13
00:00:51,680 --> 00:00:57,560
just because I know that when I first
started seeing some of these gated models,

14
00:00:57,560 --> 00:01:02,240
that it took a long time for
them to make much sense to me, and

15
00:01:02,240 --> 00:01:05,140
not just seem like a complete surprise and
mystery.

16
00:01:05,140 --> 00:01:06,540
That's the way they work so

17
00:01:06,540 --> 00:01:10,950
I hope I can do a bit of good at
explaining that one more time.

18
00:01:10,950 --> 00:01:15,410
That feeling was reconfirmed when we
started seeing some of the people

19
00:01:15,410 --> 00:01:20,135
who've filled in the midterm survey so
thanks to all the people who filled it in.

20
00:01:20,135 --> 00:01:21,255
For people who haven't,

21
00:01:21,255 --> 00:01:25,295
I'm still happy to have you fill it
in over the last couple of days.

22
00:01:25,295 --> 00:01:29,975
While there were a couple of people
who put LSTMs in the list of

23
00:01:29,975 --> 00:01:33,290
concepts they felt that they
understood really well.

24
00:01:33,290 --> 00:01:36,160
Dozens of people put LSTMs and

25
00:01:36,160 --> 00:01:41,050
GRUs into the list of concepts
they felt kind of unsure about.

26
00:01:41,050 --> 00:01:46,400
This first part is for you and if you're
one of the ones that already understand

27
00:01:46,400 --> 00:01:50,620
it really well, I guess you'll just
have to skip ahead to the second part.

28
00:01:50,620 --> 00:01:53,870
Then we'll have the research
highlight which should be fun today.

29
00:01:53,870 --> 00:01:57,820
And then, so
moving on from that it's then completing,

30
00:01:57,820 --> 00:02:00,610
saying a bit more about
machine translation.

31
00:02:00,610 --> 00:02:05,390
It's a bit that we sort of had skipped and
probably should have explained earlier

32
00:02:05,390 --> 00:02:08,610
which is how do people evaluate
machine translation systems?

33
00:02:08,610 --> 00:02:12,553
Because we've been showing you numbers and
graphs and so on and never discussed that.

34
00:02:12,553 --> 00:02:17,706
And then I wanna sort of say a bit
more about a couple of things that

35
00:02:17,706 --> 00:02:22,870
come up when trying to build new
machines translation systems.

36
00:02:22,870 --> 00:02:26,920
And in some sense, this is sort of
done on the weed stuff it's not

37
00:02:26,920 --> 00:02:31,520
that this is sort of one central concept
that you can possibly finish your

38
00:02:31,520 --> 00:02:34,504
neural networks class
without having learned.

39
00:02:34,504 --> 00:02:38,956
But on the other hand, I think that all of
these sort of kind of things that come up

40
00:02:38,956 --> 00:02:43,341
if you are actually trying to build
something where you've actually got a deep

41
00:02:43,341 --> 00:02:46,995
learning system that you can use to
do useful stuff in the world and

42
00:02:46,995 --> 00:02:50,950
that they're useful, good,
new concepts to know.

43
00:02:50,950 --> 00:02:52,370
Okay.

44
00:02:52,370 --> 00:02:54,940
Lastly just the reminders and
various things.

45
00:02:54,940 --> 00:02:57,740
The midterm, we have got it all graded.

46
00:02:57,740 --> 00:03:00,600
And our plan is that we are going to

47
00:03:00,600 --> 00:03:03,990
return it to the people
who are here after class.

48
00:03:03,990 --> 00:03:08,740
Where in particular, there's another
event that's on here after class,

49
00:03:08,740 --> 00:03:13,240
so where we're going to return it
after class is outside the door.

50
00:03:13,240 --> 00:03:17,065
That you should be able to find
TAs with boxes of midterms and

51
00:03:17,065 --> 00:03:18,606
be able to return them.

52
00:03:18,606 --> 00:03:23,206
Assignment three, yeah so this has
been a little bit of a stretch for

53
00:03:23,206 --> 00:03:26,193
everybody on assignment three I realized,

54
00:03:26,193 --> 00:03:30,572
because sort of the midterm got
in the way and people got behind.

55
00:03:30,572 --> 00:03:35,408
And we've also actually we're hoping
to be sort of right ready to go with

56
00:03:35,408 --> 00:03:40,325
giving people GPU resources on Azure and
that's kinda've gone behind,

57
00:03:40,325 --> 00:03:44,845
they're trying to work on that right
now so with any luck maybe by the end

58
00:03:44,845 --> 00:03:49,020
of today we might have the GPU
resources part in place.

59
00:03:49,020 --> 00:03:53,770
I mean, at any rate, you should absolutely
be getting start on the assignment and

60
00:03:53,770 --> 00:03:55,740
writing the code.

61
00:03:55,740 --> 00:04:00,010
But we also do really hope that
before you finish this assignment,

62
00:04:00,010 --> 00:04:04,400
you take a chance to try out Azure,
Docker and

63
00:04:04,400 --> 00:04:08,120
getting stuff working on GPUs because
that's really good experience to have.

64
00:04:09,440 --> 00:04:15,010
Then final projects,
the thing that we all noticed about our

65
00:04:15,010 --> 00:04:20,360
office hours last week after the midterm
is that barely anybody came to them.

66
00:04:20,360 --> 00:04:26,276
We'd really like to urge for this week,
please come along to office hours again.

67
00:04:26,276 --> 00:04:30,732
And especially if you're doing
a final project, we'd really,

68
00:04:30,732 --> 00:04:35,349
really like you to turn up and
talk to us about your final projects and

69
00:04:35,349 --> 00:04:40,048
in particular tonight after class and
a bit of dinner which is again,

70
00:04:40,048 --> 00:04:43,309
we're going be doing
unlimited office hours.

71
00:04:43,309 --> 00:04:45,017
Feel free to come and see him, and

72
00:04:45,017 --> 00:04:48,799
possibly even depending on how you feel
about it, you might even go off and

73
00:04:48,799 --> 00:04:52,950
have dinner first and then come back and
see him to spread things out a little bit.

74
00:04:54,230 --> 00:04:57,570
Are there any questions
people are dying to know,

75
00:04:57,570 --> 00:04:59,770
or do I head straight into
content at that point?

76
00:05:03,870 --> 00:05:05,230
I'll head straight into content.

77
00:05:06,340 --> 00:05:10,900
Basically I wanted to sort of spend
a bit of time going through, again,

78
00:05:10,900 --> 00:05:16,750
the sort of ideas of where did these
kinds of fancy recurrent units come from?

79
00:05:16,750 --> 00:05:20,700
What are they going to try and achieve and
how do they go about doing it?

80
00:05:21,700 --> 00:05:26,336
Our starting point is, what we have
with a recurrent neural network is that

81
00:05:26,336 --> 00:05:29,735
we've got something that's
evolving through time.

82
00:05:29,735 --> 00:05:37,398
And at the end of that we're at some
point in that here where time t plus n.

83
00:05:37,398 --> 00:05:41,842
And then what we want to do
is have some sense of well,

84
00:05:41,842 --> 00:05:48,790
this stuff that we saw at time t, is that
affecting what happens at time t plus n?

85
00:05:48,790 --> 00:05:55,280
That's the kind of thing of is it
the fact that we saw at time t

86
00:05:55,280 --> 00:06:02,940
this verb squash that is having
some effect on the n words later,

87
00:06:02,940 --> 00:06:08,470
that this is being someone saying
the word window because this is some

88
00:06:08,470 --> 00:06:13,900
kind of association between squashing and
windows or is that completely irrelevant?

89
00:06:13,900 --> 00:06:17,780
We wanna sort of measure
how what you're doing here

90
00:06:17,780 --> 00:06:22,690
affects what's happening maybe six,
eight, ten words later.

91
00:06:22,690 --> 00:06:28,020
And so the question is how can we
achieve that and how can we achieve it?

92
00:06:28,020 --> 00:06:32,400
And what Richard discussed and
there was some sort of complex math here

93
00:06:32,400 --> 00:06:36,280
which I'm not going to explain,
again, in great detail.

94
00:06:36,280 --> 00:06:41,250
But what we found is if we had a basic
recurrent neural network what we're

95
00:06:41,250 --> 00:06:46,160
doing at each time step in the basic
recurrent neural network is

96
00:06:46,160 --> 00:06:50,570
we've got some hidden state and
we're multiplying it by matrix and

97
00:06:50,570 --> 00:06:54,360
then we're adding some stuff to do with
the input and then we go onto next

98
00:06:54,360 --> 00:06:59,360
time stamp where we're multiplying that
hidden state by the same matrix again and

99
00:06:59,360 --> 00:07:03,520
adding some input stuff and then we
go onto the time step and we model.

100
00:07:03,520 --> 00:07:07,446
Multiplying that,
hidden stuff by the same matrix again.

101
00:07:07,446 --> 00:07:12,340
It keeping on doing these matrix
multiplies and when you keep on doing

102
00:07:12,340 --> 00:07:18,170
these matrix multiplies you can
potentially get into trouble.

103
00:07:18,170 --> 00:07:23,620
And the trouble you get into is
if your gradient is going to zero

104
00:07:23,620 --> 00:07:28,440
you kind of can't tell whether that
means that actually what happened

105
00:07:28,440 --> 00:07:33,650
in words ago is having no effect
on what you're seeing now.

106
00:07:33,650 --> 00:07:38,962
Or whether it is you hadn't set all
of the things in your matrixes norm

107
00:07:38,962 --> 00:07:45,204
exactly right and so that the gradient
is going to zero because it's vanishing.

108
00:07:50,594 --> 00:07:55,517
This is where the stuff about eigenvalues
and stuff like that comes in.

109
00:07:55,517 --> 00:07:59,360
But kind of the problem is with.

110
00:07:59,360 --> 00:08:03,681
Basic RNA, sort of a bit too much
like having to land your aircraft

111
00:08:03,681 --> 00:08:06,916
on the aircraft carrier or
something like that.

112
00:08:06,916 --> 00:08:10,014
That if you can get things
just the right size,

113
00:08:10,014 --> 00:08:13,112
things you can land on
the aircraft carrier but

114
00:08:13,112 --> 00:08:18,611
if somehow your eigenvalues are a bit too
small then you have vanishing gradients.

115
00:08:18,611 --> 00:08:23,752
And if they're a bit too large
you have exploding gradients and

116
00:08:23,752 --> 00:08:27,729
you sort of,
it's very hard to get it right and so

117
00:08:27,729 --> 00:08:34,437
this this naive transition function seems
to be the cause of a lot of the problems.

118
00:08:34,437 --> 00:08:37,611
With the naive transition
function in particular,

119
00:08:37,611 --> 00:08:42,209
what it means is that sorta we're doing
this sequence of matrix multipliers.

120
00:08:42,210 --> 00:08:46,110
So we're keeping on multiplying
by matrix at each time step.

121
00:08:46,110 --> 00:08:49,650
And so, that means that when
we're then trying to learn.

122
00:08:49,650 --> 00:08:53,715
How much effect things have
on our decisions up here.

123
00:08:53,715 --> 00:08:57,992
We're doing that by backpropagating
through this whole sequence of

124
00:08:57,992 --> 00:09:00,105
intermediate nodes.

125
00:09:00,105 --> 00:09:06,167
And so, the whole idea of all of these
gated recurrent models is to say,

126
00:09:06,167 --> 00:09:11,725
well, somehow, we'd like to be
able to get more direct evidence

127
00:09:11,725 --> 00:09:16,675
of the effect of early time
steps on much later time steps,

128
00:09:16,675 --> 00:09:24,083
without having to do this long sequence
matrix multiplies, which almost certainly.

129
00:09:24,083 --> 00:09:27,820
Give us the danger of
killing off the evidence.

130
00:09:27,820 --> 00:09:30,880
So essentially what we wanna have is,

131
00:09:30,880 --> 00:09:35,070
we want to kinda consider the time
sequence that's our straight line.

132
00:09:35,070 --> 00:09:40,176
We also want to allow these shortcut
connections so ht can directly

133
00:09:40,176 --> 00:09:46,102
affect ht +2 because if we could do
that we then when we're backpropagating

134
00:09:46,102 --> 00:09:52,134
we'll then be able to measure in the
backward phase the effect of ht on ht + 2.

135
00:09:52,134 --> 00:09:53,147
And therefore,

136
00:09:53,147 --> 00:09:57,280
we would be much more likely to
learn these long term dependencies.

137
00:09:58,790 --> 00:10:00,920
So that seems a good idea.

138
00:10:03,590 --> 00:10:06,470
So I'm gonna do the kinda gated
recurrent units first, and

139
00:10:06,470 --> 00:10:10,170
then kinda build onto LSTMs,
which are even more complex.

140
00:10:10,170 --> 00:10:14,240
So essentially that's what we're
doing in the gated recurrent unit.

141
00:10:14,240 --> 00:10:18,740
And we're only making it a little
bit more complex by saying, well,

142
00:10:18,740 --> 00:10:23,863
rather than just uniformly
putting in stuff from time -1 and

143
00:10:23,863 --> 00:10:29,900
time -2, maybe we can have adaptive
shortcut connections where we're

144
00:10:30,920 --> 00:10:35,760
deciding how much attention to pay to
the past, as well as to the present.

145
00:10:35,760 --> 00:10:39,580
And so, that's essentially what you
get with the gated recurrent unit.

146
00:10:39,580 --> 00:10:45,250
So the key equation of the gated
recurrent unit is this first one.

147
00:10:45,250 --> 00:10:49,990
So it's sort of saying, well, we're
going to do the normal neural network

148
00:10:49,990 --> 00:10:53,060
recurrent units stuff,
that's the stuff in green.

149
00:10:53,060 --> 00:10:58,600
So for the stuff in green, we take the
current input and multiply it by a matrix.

150
00:10:58,600 --> 00:11:01,810
We take the previous hidden statement and
multiply it by a matrix.

151
00:11:01,810 --> 00:11:04,200
We add all of those things with a bias and

152
00:11:04,200 --> 00:11:09,450
put it through a tanh, that's exactly the
standard recurrent neural network update.

153
00:11:09,450 --> 00:11:16,742
So we're going to do that candidate
update just like a regular RNN.

154
00:11:16,742 --> 00:11:20,840
But to actually work out what
function we're computing,

155
00:11:20,840 --> 00:11:25,774
we're then going to adaptively learn
how much and on which dimensions

156
00:11:25,774 --> 00:11:30,873
to use that candidate update and
how much that we just gonna shortcut it,

157
00:11:30,873 --> 00:11:35,530
and just stick with what we had
from the previous time step.

158
00:11:35,530 --> 00:11:39,860
And while that stuff in the previous
time step will have been to some

159
00:11:39,860 --> 00:11:44,590
extent computed by this regular and
updated the previous time step.

160
00:11:44,590 --> 00:11:47,580
But of course, that was also a mixture, so

161
00:11:47,580 --> 00:11:52,480
to some extent, it will have been directly
inherited from the time step before that.

162
00:11:52,480 --> 00:11:56,480
And so,
we kind of adaptively allowing things from

163
00:11:56,480 --> 00:12:00,900
far past time steps just to
be passed straight through,

164
00:12:00,900 --> 00:12:05,520
with no further multiplications
into the current time step.

165
00:12:05,520 --> 00:12:09,070
So a lot of the key to is it
that we have this plus here.

166
00:12:09,070 --> 00:12:13,628
The stuff that is on this side
of the plus, we're just saying,

167
00:12:13,628 --> 00:12:18,272
just move along the stuff you had
before onto the next time step,

168
00:12:18,272 --> 00:12:23,174
which has the effect that we're
directly having stuff from the past

169
00:12:23,174 --> 00:12:26,367
be present to affect further on decisions.

170
00:12:26,367 --> 00:12:30,281
So that's most of what
we have in a GRU and

171
00:12:30,281 --> 00:12:37,420
a GRU is then just a little bit more
complex than that because if we do this,

172
00:12:37,420 --> 00:12:43,890
it's sort of all additive,
you kinda kick stuff around forever.

173
00:12:43,890 --> 00:12:46,782
You're deciding which to pay attention to,
but

174
00:12:46,782 --> 00:12:50,319
once you've paid attention to it,
it's around forever.

175
00:12:50,319 --> 00:12:54,951
And that's because you're sort
of just adding stuff on here.

176
00:12:54,951 --> 00:13:00,327
And so, the final step is to say well
actually, maybe we want to sort of prune

177
00:13:00,327 --> 00:13:06,010
away some of the past stuff adaptively so
it doesn't hang around forever.

178
00:13:06,010 --> 00:13:10,290
And so, to do that, we're adding
this second gate, the reset gate.

179
00:13:10,290 --> 00:13:16,542
And so, the reset gate gives you a vector
of, again, numbers between zero and

180
00:13:16,542 --> 00:13:21,967
one, which is calculated like a kind
of a standard recurrent unit.

181
00:13:21,967 --> 00:13:27,099
But it's sort of saying,
well to some extent, what we want to do is

182
00:13:27,099 --> 00:13:33,850
be able to delete some of the stuff that
was in ht- 1 when it's no longer relevant.

183
00:13:33,850 --> 00:13:36,230
And so,
we doing this sort of hadamard product,

184
00:13:36,230 --> 00:13:40,560
the element wise product of the reset
gate and the previous hidden state.

185
00:13:40,560 --> 00:13:43,530
And so,
we can forget parts of the hidden state.

186
00:13:43,530 --> 00:13:47,600
And the parts that we're forgetting is
embedded in this kind of candidate update.

187
00:13:47,600 --> 00:13:52,644
The part that's being just
passed along from the past to

188
00:13:52,644 --> 00:13:58,025
have direct updates is still
just exactly as it was before.

189
00:13:58,025 --> 00:14:02,310
So to have one attempt to
be more visual at that.

190
00:14:02,310 --> 00:14:06,442
So if we have a basic vanilla tanh-RNN,

191
00:14:06,442 --> 00:14:13,003
one way that you could think about
that is we have a hidden state,

192
00:14:13,003 --> 00:14:18,470
and what our execution of our
unit is doing as a program

193
00:14:18,470 --> 00:14:23,330
is saying you read the whole
of that register h,

194
00:14:23,330 --> 00:14:29,433
you do your RNN update, and
you write the whole thing back.

195
00:14:29,433 --> 00:14:33,240
So you've got this one memory register.

196
00:14:33,240 --> 00:14:38,110
You read it all, do a standard recurrent
update, and write it all back.

197
00:14:38,110 --> 00:14:40,920
So that's sort of very inflexible.

198
00:14:40,920 --> 00:14:45,240
And you're just sort of repeating that
over and over again at each time step.

199
00:14:45,240 --> 00:14:50,170
So in contrast to that,
when you have a GRU unit, that is then,

200
00:14:50,170 --> 00:14:55,230
allowing you to sort of learn
this adaptive flexibility.

201
00:14:55,230 --> 00:14:59,990
So first of all,
with the reset gate, you can learn

202
00:14:59,990 --> 00:15:05,370
a subset of the hidden state that
you want to read and make use of.

203
00:15:05,370 --> 00:15:07,750
And the rest of it will
then get thrown away.

204
00:15:07,750 --> 00:15:10,255
So you have an ability to forget stuff.

205
00:15:10,255 --> 00:15:15,325
And then,
once you've sort of read your subset,

206
00:15:15,325 --> 00:15:20,032
you'll then going to do
on it your standard RNN

207
00:15:20,032 --> 00:15:23,910
computation of how to update things.

208
00:15:23,910 --> 00:15:27,820
But then secondly,
you're gonna select the writable subset.

209
00:15:27,820 --> 00:15:29,550
So this is saying,

210
00:15:29,550 --> 00:15:33,310
some of the hidden state we're
just gonna carry on from the past.

211
00:15:33,310 --> 00:15:37,218
We're only now going to
edit part of the register.

212
00:15:37,218 --> 00:15:41,851
And saying part of the register,
I guess is a lying and simplifying a bit,

213
00:15:41,851 --> 00:15:45,665
because really,
you've got this vector of real numbers and

214
00:15:45,665 --> 00:15:50,601
some said the part of the register is
70% updating this dimension and 20%

215
00:15:50,601 --> 00:15:56,300
updating this dimension that values could
be one or zero but normally they won't be.

216
00:15:56,300 --> 00:15:58,890
So I choose the writable subset And

217
00:15:58,890 --> 00:16:03,970
then it's that part of it that I'm
then updating with my new candidate

218
00:16:03,970 --> 00:16:07,630
update which is then written back,
adding on to it.

219
00:16:08,820 --> 00:16:11,785
And so
both of those concepts in the gating,

220
00:16:11,785 --> 00:16:16,210
the one gate is selecting what to read for
your candidate update.

221
00:16:16,210 --> 00:16:23,252
And the other gate is saying, which
parts of the hidden state to overwrite?

222
00:16:23,252 --> 00:16:26,234
Does that sort of make
sense how that's a useful,

223
00:16:26,234 --> 00:16:30,007
more powerful way of thinking
about having a recurrent model?

224
00:16:34,286 --> 00:16:35,486
Yes, a question?

225
00:16:43,306 --> 00:16:48,770
Yeah, so how you select the readable
subset is based on this reset gate?

226
00:16:48,770 --> 00:16:53,413
So, the reset gate decides
which parts of the hidden

227
00:16:53,413 --> 00:16:57,116
state to read to update the hidden state.

228
00:16:57,116 --> 00:17:03,132
So, the reset gate calculates which parts
to read based on the current input and

229
00:17:03,132 --> 00:17:05,380
the previous hidden state.

230
00:17:05,380 --> 00:17:12,875
So it's gonna say, okay, I wanna pay a lot
of attention to dimensions 7 and 52.

231
00:17:12,875 --> 00:17:16,030
And so, those are the ones and
a little to others.

232
00:17:16,030 --> 00:17:20,502
And so those are the ones that
will be being read here and

233
00:17:20,502 --> 00:17:24,880
used in the calculation of
the new candidate update,

234
00:17:24,881 --> 00:17:30,931
which is then sort of mixed together
with carrying on what you had before.

235
00:17:30,931 --> 00:17:32,284
Any, yes.

236
00:17:46,050 --> 00:17:50,288
So, the question was explain this again.

237
00:17:50,288 --> 00:17:51,068
I'll try.

238
00:17:51,068 --> 00:17:53,894
[LAUGH] I will try.

239
00:17:53,894 --> 00:17:55,390
I will try and do that.

240
00:17:55,390 --> 00:17:59,600
Let me go back to this slide first,
cuz this has most of that,

241
00:17:59,600 --> 00:18:01,840
except the last piece, right.

242
00:18:01,840 --> 00:18:09,990
So here, what we want to do is we're
carrying along a hidden state over time.

243
00:18:09,990 --> 00:18:14,843
And at each point in time,
we're going to say, well,

244
00:18:14,843 --> 00:18:19,590
based on the new input and
the previous hidden state,

245
00:18:19,590 --> 00:18:24,125
we want to try and
calculate a new hidden state, but

246
00:18:24,125 --> 00:18:28,893
we don't fully want to
calculate a new hidden state.

247
00:18:28,893 --> 00:18:33,930
Sometimes, it will be useful just to
carry over information from further back.

248
00:18:33,930 --> 00:18:39,007
That's how we're going to get longer term
memory into our current neural network.

249
00:18:39,007 --> 00:18:44,027
Cuz if we kind of keep on doing
multiplications at each time step

250
00:18:44,027 --> 00:18:48,880
along a basic RNN,
we lose any notion of long-term memory.

251
00:18:48,880 --> 00:18:54,145
And essentially, we can't remember things
for more than seven to ten time steps.

252
00:18:54,145 --> 00:19:01,167
So that is sort of the top level equation
to say, well, what we gonna calculate.

253
00:19:01,167 --> 00:19:07,216
We want to calculate a mixture
of a candidate update and

254
00:19:07,216 --> 00:19:13,780
keeping what we had there before and
how do we do that?

255
00:19:13,780 --> 00:19:19,370
Well, what we're going to learn is
this ut vector, the update gate and

256
00:19:19,370 --> 00:19:23,610
the elements of that vector
are gonna be between zero and one.

257
00:19:23,610 --> 00:19:26,387
And if they're close to one,
it's gonna say,

258
00:19:26,387 --> 00:19:30,847
overwrite the current hidden state with
what we calculated this time step.

259
00:19:30,847 --> 00:19:33,718
And if they're close to zero,
it's gonna say,

260
00:19:33,718 --> 00:19:37,180
keep this element vector
just what it used to be.

261
00:19:37,180 --> 00:19:42,039
And so how we calculate the update
gate is using our regular kind

262
00:19:42,039 --> 00:19:46,439
of recurrent unit where it
looks at the current input and

263
00:19:46,439 --> 00:19:51,663
it looks at the recent history and
it calculates a value with the only

264
00:19:51,663 --> 00:19:56,522
difference that we use here sigmoid,
so that's between 0 and

265
00:19:56,522 --> 00:20:01,230
1 rather than tanh that puts
that at between minus 1 and 1.

266
00:20:01,230 --> 00:20:06,986
And so the kind of hope
here intuitively is suppose

267
00:20:06,986 --> 00:20:12,193
we have a unit that is
sort of sensitive to what

268
00:20:12,193 --> 00:20:17,674
verb we're on,
then what we wanna say is well,

269
00:20:17,674 --> 00:20:24,820
we're going through this sentence and
we've seen a verb.

270
00:20:24,820 --> 00:20:30,215
We wanted that unit, well, sorry,
these dimension of the vector.

271
00:20:30,215 --> 00:20:33,951
Let's say, their five dimensions of the
vector that sort of record what kind of

272
00:20:33,951 --> 00:20:35,350
verb it's just seen.

273
00:20:35,350 --> 00:20:42,074
We want those dimensions of the vector
to just stay recording what verb was

274
00:20:42,074 --> 00:20:47,901
seen until such time as in the input,
a band new verb appears.

275
00:20:47,901 --> 00:20:53,480
And it's at precisely that point, we wanna
say, okay, now is the time to update.

276
00:20:53,480 --> 00:20:56,767
Forget about what used to be
stored in those five dimensions.

277
00:20:56,767 --> 00:21:00,810
Now, you should store
a representation of the new verb.

278
00:21:00,810 --> 00:21:04,212
And so, that's exactly what
the update gate could do here.

279
00:21:04,212 --> 00:21:10,210
It could be looking at the input and
say, okay, I found a new verb.

280
00:21:10,210 --> 00:21:15,617
So dimensions 47 to 52 should
be being given a value of 1 and

281
00:21:15,617 --> 00:21:22,860
that means that they'll be storing a value
calculated from this candidate update,

282
00:21:22,860 --> 00:21:27,057
and ignoring what they
used to store in the past.

283
00:21:27,057 --> 00:21:30,817
But if the update gate finds
it's looking at a preposition or

284
00:21:30,817 --> 00:21:34,441
at a term in our It'll say,
no, not interested in those.

285
00:21:34,441 --> 00:21:38,459
So it'll make the update
value close to 0 and

286
00:21:38,459 --> 00:21:43,430
that means that dimensions
47 to 52 will continue to

287
00:21:43,430 --> 00:21:48,736
store the verb that you last saw
even if it was ten words ago.

288
00:21:48,736 --> 00:21:49,986
I haven't quite finish.

289
00:21:49,986 --> 00:21:52,480
So that was that part of it, so yes.

290
00:21:52,480 --> 00:21:54,364
So, the candidate update.

291
00:21:54,364 --> 00:21:55,790
So, that's the update gate.

292
00:21:55,790 --> 00:22:00,747
And when we do update, the candidate
update is just exactly the same as

293
00:22:00,747 --> 00:22:05,704
it always was in our current new
network that you're calculating this

294
00:22:05,704 --> 00:22:09,655
function of the important
the previous hidden state and

295
00:22:09,655 --> 00:22:13,207
put it through a tanh
together from minus 1 to 1.

296
00:22:13,207 --> 00:22:17,069
Then the final idea here is that well,

297
00:22:17,069 --> 00:22:23,222
if you just have this,
if you're doing a candidate update,

298
00:22:23,222 --> 00:22:28,411
you're always using
the previous hidden state and

299
00:22:28,411 --> 00:22:32,770
the new input word in
exactly the same way.

300
00:22:33,880 --> 00:22:39,240
Whereas really for my example, what I was
saying was if you have detected a new

301
00:22:39,240 --> 00:22:45,015
verb in the input, you should be storing
that new verb in dimensions 47 to 52 and

302
00:22:45,015 --> 00:22:48,916
you should just be ignoring
what you used to have there.

303
00:22:48,916 --> 00:22:52,393
And so it's sort of seems like
at least in some circumstances

304
00:22:52,393 --> 00:22:56,006
what you'd like to do is throw
away your current hidden state,

305
00:22:56,006 --> 00:22:59,350
so you could replace it
with some new hidden state.

306
00:22:59,350 --> 00:23:03,117
And so that's what this second gate,
the reset gate does.

307
00:23:03,117 --> 00:23:07,829
So the reset gate can also look at
the current import in the previous hidden

308
00:23:07,829 --> 00:23:11,520
state and
it choses a value between zero, and one.

309
00:23:11,520 --> 00:23:15,288
And if the reset gate choses
a value close to zero,

310
00:23:15,288 --> 00:23:20,587
you're essentially just throwing
away the previous hidden state and

311
00:23:20,587 --> 00:23:24,283
calculating something
based on your new input.

312
00:23:24,283 --> 00:23:28,456
And the suggestion there for
language analogy is well,

313
00:23:28,456 --> 00:23:35,316
if it's something like you're recording,
the last seen verb in dimensions 47 to 52.

314
00:23:35,316 --> 00:23:40,175
When you see a new verb, well, the right
thing to do is to throw away what you

315
00:23:40,175 --> 00:23:45,115
have in your history from 47 to 52 and
just calculate something new based

316
00:23:45,115 --> 00:23:49,694
on the input, but that's not always
gonna be what you want to do.

317
00:23:49,694 --> 00:23:54,618
For example, in English, English is
famous for having a lot of verb particle

318
00:23:54,618 --> 00:23:59,406
combinations which cause enormous
difficulty to non-native speakers.

319
00:23:59,406 --> 00:24:05,894
So that's all of these things
like make up, make out, take up.

320
00:24:05,894 --> 00:24:08,121
All of these combinations of a verb and

321
00:24:08,121 --> 00:24:11,890
a preposition have a special
meaning that you just have to know.

322
00:24:11,890 --> 00:24:17,300
It isn't really, you can't tell
from the words most of the time.

323
00:24:17,300 --> 00:24:22,549
So if you are wanting to work out
what the meaning of make out is,

324
00:24:22,549 --> 00:24:28,415
so you've seen make and
you put in that into dimensions 47 to 52.

325
00:24:28,415 --> 00:24:33,114
But if dimensions 47 to 52 are really
storing main predicate meaning,

326
00:24:33,114 --> 00:24:38,261
if you see the word out coming next, you
don't wanna throw away make because it's

327
00:24:38,261 --> 00:24:43,968
a big difference in meaning whether
it's make out or take out will give out.

328
00:24:43,968 --> 00:24:47,880
What you wanna do is you wanna combine
both of them together to try and

329
00:24:47,880 --> 00:24:49,770
calculate the predicate's meaning.

330
00:24:49,770 --> 00:24:55,110
So in that case, you want your reset
gate to have a value near one so you're

331
00:24:55,110 --> 00:24:59,490
still keeping it and you're keeping the
new import and calculating another value.

332
00:25:02,170 --> 00:25:05,364
Okay, that was my attempt to explain GRUs,
and now the question.

333
00:25:18,954 --> 00:25:23,882
So the question is okay, but
why this gated recurrent

334
00:25:23,882 --> 00:25:28,930
unit not suffer from
the vanishing gradient problem?

335
00:25:29,990 --> 00:25:35,833
And really the secret is
right here in this plus sign.

336
00:25:39,044 --> 00:25:44,063
If you allowed me to simplify slightly,

337
00:25:44,063 --> 00:25:51,750
and this is actually a version
of a network that has been used.

338
00:25:51,750 --> 00:25:57,210
It's essentially, not more details,
but this aspect of it actually

339
00:25:57,210 --> 00:26:02,499
corresponds to the very original
form of an LSTM that was proposed.

340
00:26:02,499 --> 00:26:10,500
Suppose I just delete this this- ut here,
so this just was 1.

341
00:26:10,500 --> 00:26:16,240
So what we have here is ht- 1,
so kind of like the reset gate,

342
00:26:16,240 --> 00:26:21,030
the update gate is only
being used on this side.

343
00:26:21,030 --> 00:26:25,892
It's saying should you pay any
attention to the new candidate,

344
00:26:25,892 --> 00:26:29,325
but you're always plussing it with ht-1.

345
00:26:29,325 --> 00:26:33,955
If you'll imagine that
slightly simplified form,

346
00:26:33,955 --> 00:26:37,741
well, if you think about your gradients,

347
00:26:37,741 --> 00:26:43,105
then what we've got here is when
we're kind of working at h,

348
00:26:43,105 --> 00:26:46,277
this has been used to calculate ht.

349
00:26:46,277 --> 00:26:51,053
Ht-1 is being used to calculate ht, so

350
00:26:51,053 --> 00:26:54,284
ht equals a plus ht-1, so

351
00:26:54,284 --> 00:26:59,482
there's a completely linear relationship

352
00:26:59,482 --> 00:27:05,247
with a coefficient of one between ht and
ht-1.

353
00:27:05,247 --> 00:27:09,108
Okay, and so
therefore when you do your calculus and

354
00:27:09,108 --> 00:27:13,781
you back prop that, right,
you have something with slope 1.

355
00:27:13,781 --> 00:27:19,415
That ht is just directly reflecting ht-1.

356
00:27:19,415 --> 00:27:24,100
And that's the perfect case for
gradients to flow beautifully.

357
00:27:24,100 --> 00:27:28,910
Nothing is lost, it's just going
straight back down the line.

358
00:27:28,910 --> 00:27:34,240
And so that's why it can carry
information for a very long time.

359
00:27:34,240 --> 00:27:40,015
So once we put in this update gate,
what we're having is the providing

360
00:27:40,015 --> 00:27:45,770
ut is close to zero,
this is gonna be approximately one,

361
00:27:45,770 --> 00:27:50,190
and so the gradients are just gonna flow
straight back to the line in an arbitrary

362
00:27:50,190 --> 00:27:54,650
distance and
you can have long distance dependencies.

363
00:27:54,650 --> 00:27:58,490
Crucially, it's not like you're
multiplying by a matrix every time,

364
00:27:58,490 --> 00:28:01,730
which causes all with vanishing gradients.

365
00:28:01,730 --> 00:28:07,454
It's just almost one there,
straight linear sequence.

366
00:28:07,454 --> 00:28:14,308
Now of course, if at some point ut is
close to 1, so this is close to zero,

367
00:28:14,308 --> 00:28:19,235
well then almost nothing
is flowing in from ht-1.

368
00:28:19,235 --> 00:28:22,320
But that's then saying there
is no long term dependency.

369
00:28:22,320 --> 00:28:24,720
That's what the model learn.

370
00:28:24,720 --> 00:28:29,507
So nothing flows a long way back.

371
00:28:29,507 --> 00:28:30,432
Is that a question?

372
00:28:30,432 --> 00:28:31,270
Yeah.

373
00:28:39,236 --> 00:28:46,710
So the question is,
isn't ht tilted ut both dependent on ht-1.

374
00:28:46,710 --> 00:28:47,976
And yeah, they are.

375
00:28:47,976 --> 00:28:54,882
Just like the ut you're calculating
it here in terms of ht-1.

376
00:28:54,882 --> 00:29:00,931
So in some sense the answer is yeah,
you are right but

377
00:29:00,931 --> 00:29:06,040
it's sort of turns out not matter, right?

378
00:29:06,040 --> 00:29:10,390
So the thing I think is If I put words
in to your mouth, the thing that you're

379
00:29:10,390 --> 00:29:15,570
thinking about is well, this ut
look right down at the bottom here,

380
00:29:15,570 --> 00:29:20,054
you'll calculate it by matrix
vector multiply from ht-1.

381
00:29:20,054 --> 00:29:24,830
And well then, where the ht-1 come from,

382
00:29:24,830 --> 00:29:29,960
it came from ht-2 and there was some
more matrix vector multiplies here,

383
00:29:29,960 --> 00:29:34,220
so there is a pathway going
through the gates where

384
00:29:34,220 --> 00:29:38,610
you're keep on doing matrix vector
multiplies, and that is true.

385
00:29:38,610 --> 00:29:42,189
But, it turns out that sort
of doesn't really matter,

386
00:29:42,189 --> 00:29:47,143
because of the fact that there is this
direct pathway, where you're getting

387
00:29:47,143 --> 00:29:51,886
this straight linear flow of gradient
information, going back in time.

388
00:29:54,335 --> 00:29:55,373
Any other question?

389
00:29:55,373 --> 00:30:00,141
Yes, I don't think I'll get any further
in this class if I'm not careful.

390
00:30:10,658 --> 00:30:12,200
I'm sorry if that's true.

391
00:30:13,550 --> 00:30:18,900
So the question was, why when you
Is before ut and one, one is ut.

392
00:30:18,900 --> 00:30:19,763
We swapped.

393
00:30:19,763 --> 00:30:23,180
>> [INAUDIBLE]
>> Yeah, if that's true, sorry about that.

394
00:30:23,180 --> 00:30:25,415
That was bad, boo boo mistake,

395
00:30:25,415 --> 00:30:27,925
cuz obviously we should be
trying to be consistent.

396
00:30:27,925 --> 00:30:31,889
But, it totally doesn't matter.

397
00:30:31,889 --> 00:30:36,433
This is sort of, in some sense, whether
you're thinking of it as the forget

398
00:30:36,433 --> 00:30:40,790
gate or a remember gate, and
you can kind of have it either way round.

399
00:30:40,790 --> 00:30:44,849
And that doesn't effect how the math and
the learning works.

400
00:30:48,104 --> 00:30:49,726
Any other questions?

401
00:30:51,829 --> 00:30:56,458
I'm happy to talk about this because I do
actually think it's useful to understand

402
00:30:56,458 --> 00:31:00,754
this stuff cuz in some sense these kind
of gated units have been the biggest and

403
00:31:00,754 --> 00:31:04,940
most useful idea for making practical
systems in the last couple of years.

404
00:31:04,940 --> 00:31:05,624
Yes.

405
00:31:11,282 --> 00:31:16,522
I actually have a picture for
an LSTM later on.

406
00:31:16,522 --> 00:31:20,296
It depends on a lot of particularities,
but

407
00:31:20,296 --> 00:31:24,536
it sort of seems like
somewhere around 100.

408
00:31:24,536 --> 00:31:29,671
Sorry the question was how long does a GRU
actually end up remembering for and I

409
00:31:29,671 --> 00:31:35,500
kind of think order of magnitude the kind
number you want in your head is 100 steps.

410
00:31:35,500 --> 00:31:40,826
So they don't remember forever I think
that's something people also get wrong.

411
00:31:40,826 --> 00:31:46,685
If we go back to the other one,
that I hope to get to eventually,

412
00:31:46,685 --> 00:31:49,570
the name is kind of a mouthful.

413
00:31:49,570 --> 00:31:54,401
I think it was actually very
deliberately named, where it was called,

414
00:31:54,401 --> 00:31:56,330
long short term memory.

415
00:31:56,330 --> 00:32:01,140
Right there was no idea in people's
heads that this was meant to be

416
00:32:01,140 --> 00:32:05,640
the model of long term
memory in the human brain.

417
00:32:05,640 --> 00:32:08,740
Long term memory is
fundamentally different and

418
00:32:08,740 --> 00:32:11,770
needs to be modeled in other ways and
maybe later in the class,

419
00:32:11,770 --> 00:32:16,340
we'll say a little a bit about the kind
of ideas people thinking about this.

420
00:32:16,340 --> 00:32:19,556
What this was about was saying okay,

421
00:32:19,556 --> 00:32:24,860
well people have a short term memory and
it lasts for a while.

422
00:32:24,860 --> 00:32:29,729
Whereas the problem was our current
neural networks are losing all of there

423
00:32:29,729 --> 00:32:31,435
memory in ten time steps.

424
00:32:31,435 --> 00:32:36,336
So if we could get that pushed out
another order of magnitude during

425
00:32:36,336 --> 00:32:40,362
100 time steps that would
be really useful to give us

426
00:32:40,362 --> 00:32:43,707
a more human like sense
of short term memory.

427
00:32:43,707 --> 00:32:44,207
Sorry, yeah?

428
00:32:49,930 --> 00:32:55,603
So the question is,
do GRUs train faster than LSTMs?

429
00:32:55,603 --> 00:32:59,625
I don't think that's true,
does Richard have an opinion?

430
00:32:59,625 --> 00:33:05,292
>> [INAUDIBLE]
>> Yes,

431
00:33:05,292 --> 00:33:10,426
so Richard says less computation
the computational cost is faster,

432
00:33:10,426 --> 00:33:15,393
but I sort of feel that sometimes
LSTMs have a slight edge on speed.

433
00:33:15,393 --> 00:33:17,375
No huge difference,
let's say that's the answer.

434
00:33:17,375 --> 00:33:23,270
Any other, was there another
question that people want to ask?

435
00:33:24,770 --> 00:33:26,260
Okay, I'll go on.

436
00:33:26,260 --> 00:33:31,743
You can ask them again in a minute and
I go on.

437
00:33:31,743 --> 00:33:36,551
Okay, so then finally I wanted to sort

438
00:33:36,551 --> 00:33:40,619
of say a little bit about LSTMs.

439
00:33:40,619 --> 00:33:45,795
So LSTMs are more complex because there
are more equations down the right side.

440
00:33:45,795 --> 00:33:52,630
And there's more gates but they're barely
different when it comes down to it.

441
00:33:52,630 --> 00:34:00,170
And to some extent, they look more
different than they are because of

442
00:34:00,170 --> 00:34:05,810
certain arbitrary choices of notation
that was made when LSTMs were introduced.

443
00:34:05,810 --> 00:34:10,853
So when LSTMs were introduced,
Hochreiter & Schmidhuber

444
00:34:10,853 --> 00:34:15,480
sort of decided to say, well,
we have this privileged notion of

445
00:34:15,480 --> 00:34:20,250
memory in the LSTM,
which we're going to call the cell.

446
00:34:20,250 --> 00:34:24,516
And so people use C for
the cell of the LSTM.

447
00:34:24,516 --> 00:34:30,098
But the crucial thing to notice
Is that the cell of the LSTM

448
00:34:30,099 --> 00:34:35,679
is behaving like the hidden
state of the GRU, so really,

449
00:34:35,679 --> 00:34:41,329
the h of the GRU is equivalent
to the c of the LSTM.

450
00:34:41,330 --> 00:34:45,380
Whereas the h of the LSTM is

451
00:34:45,380 --> 00:34:49,320
something different that's related
to sort what's exposed to the world.

452
00:34:49,320 --> 00:34:55,980
So the center of the LSTM,
this equation for updating the cell.

453
00:34:55,980 --> 00:35:01,810
Is do a first approximation exactly
the same as this most crucial equation for

454
00:35:01,810 --> 00:35:04,690
updating the hidden state of the GRU.

455
00:35:04,690 --> 00:35:09,119
Now, if you stare a bit,
they're not quite the same,

456
00:35:09,119 --> 00:35:12,880
the way they are different is very small.

457
00:35:12,880 --> 00:35:18,156
So in the LSTM you have two gates
a forget gate and then an input gate so

458
00:35:18,156 --> 00:35:23,818
both of those for each of the dimension
have a value between zero and one.

459
00:35:23,818 --> 00:35:28,276
So you can simultaneously keep
everything from the past and

460
00:35:28,276 --> 00:35:32,111
keep everything from your
new calculated value and

461
00:35:32,111 --> 00:35:36,290
sum them together which is
a little bit different.

462
00:35:36,290 --> 00:35:41,720
To the GRU where you're sort of doing
this tradeoff as to how much to take

463
00:35:41,720 --> 00:35:47,180
directly, copy across the path versus
how much to use your candidate update.

464
00:35:47,180 --> 00:35:51,317
So it split those into two functions,
so you get the sum of them both.

465
00:35:51,317 --> 00:35:54,621
But other than that,
it's exactly the same, right?

466
00:35:54,621 --> 00:35:57,453
Where's my mouse?

467
00:35:57,453 --> 00:36:02,066
The candidate update is
exactly the same as what's

468
00:36:02,066 --> 00:36:06,140
being listed in terms of c tilde and
h tilde but

469
00:36:06,140 --> 00:36:10,644
the candidate update is exactly,
well, sorry,

470
00:36:10,644 --> 00:36:15,792
it's not quite I guess it's
the reset gate the candidate

471
00:36:15,792 --> 00:36:21,680
update is virtually the same as
the standard LSTM style unit.

472
00:36:21,680 --> 00:36:25,944
And then for the gates,
the gates are sort of the same,

473
00:36:25,944 --> 00:36:28,911
that they're using these sort of R and

474
00:36:28,911 --> 00:36:34,864
N style calculations to get a value
between zero for one for each dimension.

475
00:36:34,864 --> 00:36:39,822
So the differences
are that we added one more

476
00:36:39,822 --> 00:36:44,244
gate because we kinda having forget and

477
00:36:44,244 --> 00:36:49,202
input gates here and
the other difference is

478
00:36:49,202 --> 00:36:54,428
to have the ability to
sort of that the GRUs sort

479
00:36:54,428 --> 00:36:59,252
of has this reset gate where it's saying,

480
00:36:59,252 --> 00:37:07,670
I might ignore part of the past when
calculating My candidate update.

481
00:37:07,670 --> 00:37:11,310
The LSTM is doing it
a little bit differently.

482
00:37:11,310 --> 00:37:17,390
So the LSTM in the candidate update,
it's always using the current input.

483
00:37:17,390 --> 00:37:22,397
But for this other half here, it's not

484
00:37:22,397 --> 00:37:28,029
using ct minus 1, it's using ht minus 1.

485
00:37:28,029 --> 00:37:33,560
So the LSTM has this extra
ht which is derived from ct.

486
00:37:34,780 --> 00:37:39,767
And the way that it's derived from ct
is that there's an extra tanh here but

487
00:37:39,767 --> 00:37:42,786
then you're scaling with this output gate.

488
00:37:42,786 --> 00:37:49,500
So the output gate is sort of equivalent
of the reset gate of the GRU.

489
00:37:49,500 --> 00:37:53,691
But effectively,
it's one one time step earlier,

490
00:37:53,691 --> 00:37:57,691
cuz on the LSTM side,
on the preceding time step,

491
00:37:57,691 --> 00:38:03,215
you also calculate an ht by ignoring
some stuff with the output gate,

492
00:38:03,215 --> 00:38:07,025
whereas in the GRU, for
the current time step,

493
00:38:07,025 --> 00:38:13,149
you're multiplying with the reset gate
times your previous hidden state.

494
00:38:13,149 --> 00:38:14,194
That sorta makes sense?

495
00:38:14,194 --> 00:38:15,320
A question.

496
00:38:29,960 --> 00:38:31,496
Right, yes, the don't forget gate.

497
00:38:31,496 --> 00:38:36,390
[LAUGH] You're right, so
it's the question about was the ft.

498
00:38:36,390 --> 00:38:37,830
Is it really a forget gate?

499
00:38:37,830 --> 00:38:41,320
No, as presented here,
it's a don't forget gate.

500
00:38:41,320 --> 00:38:45,750
Again, you could do the 1 minus trick if
you wanted to and call this 1 minus f1,

501
00:38:45,750 --> 00:38:50,060
but yeah, as presented here,
if the value is close to 1,

502
00:38:50,060 --> 00:38:52,700
it means don't forget, yeah, absolutely.

503
00:39:03,280 --> 00:39:09,071
So this one here is genuinely
an update gate because if If the value

504
00:39:09,071 --> 00:39:15,330
of it is close to 1,
you're updating with the candidate update.

505
00:39:15,330 --> 00:39:17,490
And if the value is close to zero,

506
00:39:17,490 --> 00:39:20,145
you're keeping the previous
contents of the hidden state.

507
00:39:20,145 --> 00:39:25,977
>> [INAUDIBLE]
reset.

508
00:39:25,977 --> 00:39:29,966
>> Right, so the reset gate is
sort of a don't reset gate.

509
00:39:29,966 --> 00:39:30,733
[LAUGH] Yeah, okay.

510
00:39:30,733 --> 00:39:36,712
[LAUGH] I'm having a hard time
with the terminology here [LAUGH].

511
00:39:36,712 --> 00:39:39,334
You are right.

512
00:39:39,334 --> 00:39:42,540
Another question?

513
00:40:03,349 --> 00:40:09,424
So okay, so the question was
sometimes you're using ct-1,

514
00:40:09,424 --> 00:40:13,098
and sometimes you're using ht-1.

515
00:40:13,098 --> 00:40:14,632
What's going on there?

516
00:40:14,632 --> 00:40:22,380
And the question is in what sense
is ct less exposed in the LSTM?

517
00:40:22,380 --> 00:40:27,200
Right, so there was something I glossed
over in my LSTM presentation, and

518
00:40:27,200 --> 00:40:28,620
I'm being called on it.

519
00:40:28,620 --> 00:40:33,740
Is look, actually for the LSTM, it's ht-1

520
00:40:33,740 --> 00:40:39,531
that's being used everywhere for
all three gates.

521
00:40:39,531 --> 00:40:44,258
So really, when I sort of said
that what we're doing here,

522
00:40:44,258 --> 00:40:49,850
calculating ht, that's sort of
similar to the reset gate in the GRU.

523
00:40:51,180 --> 00:40:53,600
I kind of glossed over that a little.

524
00:40:53,600 --> 00:40:58,770
It's sort of true in terms of thinking of
the calculation of the candidate update

525
00:40:58,770 --> 00:41:03,945
cuz this ht- 1 will then go
into the candidate update.

526
00:41:03,945 --> 00:41:09,370
But's a bit more than that, cuz actually,
stuff that you throw away with your

527
00:41:09,370 --> 00:41:14,910
output gate at one time step is
then also gonna be thrown away

528
00:41:14,910 --> 00:41:20,420
in the calculation of every
gate at the next time step.

529
00:41:20,420 --> 00:41:28,822
Yeah, and so then the second question is
in what sense is the cell less exposed?

530
00:41:28,822 --> 00:41:30,529
And that's sort of the answer to that.

531
00:41:30,529 --> 00:41:35,524
The sense in which the cell
is less exposed is

532
00:41:35,524 --> 00:41:40,789
the only place that
the cell is directly used,

533
00:41:40,789 --> 00:41:46,594
is to sort of linearly add
on the cell at the previous

534
00:41:46,594 --> 00:41:51,290
time step plus its candidate update.

535
00:41:51,290 --> 00:41:53,700
For all the other computations,

536
00:41:53,700 --> 00:41:58,430
you're sort of partially hiding
the cell using this output gate.

537
00:42:01,220 --> 00:42:02,798
Another question, sure.

538
00:42:28,964 --> 00:42:32,339
Hm, okay, so the question is, gee,

539
00:42:32,339 --> 00:42:38,143
why do you need this tanh here,
couldn't you just drop that one?

540
00:42:42,068 --> 00:42:42,718
Whoops.

541
00:42:51,190 --> 00:42:52,369
Hm.

542
00:42:54,697 --> 00:42:58,506
I'm not sure I have such a good
answer to that question.

543
00:42:58,506 --> 00:43:08,506
>> [INAUDIBLE]

544
00:43:17,112 --> 00:43:19,802
>> Okay, so Richard's suggestion is,

545
00:43:19,802 --> 00:43:23,078
well this ct is kind of
like a linear layer, and

546
00:43:23,078 --> 00:43:28,970
therefore it's kind of insured if you
should add a non linearity after it.

547
00:43:28,970 --> 00:43:31,580
And that gives you a bit more power.

548
00:43:32,750 --> 00:43:36,217
Maybe that's right.

549
00:43:36,217 --> 00:43:40,540
Well, we could try it both ways and
see if it makes a difference, or

550
00:43:40,540 --> 00:43:43,670
maybe Shane already has,
I'm not sure [LAUGH].

551
00:43:43,670 --> 00:43:45,890
Any other questions?

552
00:43:45,890 --> 00:43:49,276
Make them a softball
one that I can answer.

553
00:43:49,276 --> 00:43:56,398
>> [LAUGH]
>> Okay,

554
00:43:56,398 --> 00:44:02,250
so I had a few more
pictures that went through

555
00:44:02,250 --> 00:44:08,450
the parts of the LSTM
with one more picture.

556
00:44:08,450 --> 00:44:12,520
I'm starting to think I should maybe
not dwell on this in much detail.

557
00:44:12,520 --> 00:44:17,000
Cuz we've sort of talked about
the fact that there are the gates for

558
00:44:17,000 --> 00:44:18,680
all the things.

559
00:44:18,680 --> 00:44:25,696
We're working out the candidate update,
just like an RNN.

560
00:44:25,696 --> 00:44:30,234
The only bit that I just wanna
say one more time is I think

561
00:44:30,234 --> 00:44:34,870
it's fair to say that the whole
secret of these things,

562
00:44:34,870 --> 00:44:41,190
is that you're doing this addition
where you're adding together.

563
00:44:41,190 --> 00:44:44,590
When in the addition,
it's sort of a weighted addition.

564
00:44:44,590 --> 00:44:46,170
But in the addition,

565
00:44:46,170 --> 00:44:51,740
one choice is you're just copying
stuff from the previous time step.

566
00:44:51,740 --> 00:44:56,794
And to the extent that you're copying
stuff from the previous time step,

567
00:44:56,794 --> 00:45:00,640
you have a gradient of 1,
which you're just pushing.

568
00:45:00,640 --> 00:45:04,149
So you can push error directly
back across that, and

569
00:45:04,149 --> 00:45:08,120
you can keep on doing that for
any number of time steps.

570
00:45:08,120 --> 00:45:13,074
So it's that plus, having that plus
with the previous time step rather

571
00:45:13,074 --> 00:45:16,260
than having it all multiplied by matrix.

572
00:45:16,260 --> 00:45:22,839
That is the central idea that makes LSTMs
be able to have long short-term memory.

573
00:45:22,839 --> 00:45:27,715
And I mean, that has proven to
be an incredibly powerful idea,

574
00:45:27,715 --> 00:45:32,131
and so in general,
it doesn't sound that profound, but

575
00:45:32,131 --> 00:45:37,467
that idea has been sort of driving
a lot of the developments of what's

576
00:45:37,467 --> 00:45:42,163
been happening in deep learning
in the last couple of years.

577
00:45:42,163 --> 00:45:49,455
So we don't really talk about,
in this class, about vision systems.

578
00:45:49,455 --> 00:45:52,431
You can do that next quarter in 231N.

579
00:45:52,431 --> 00:45:57,979
But one of the leading ideas and has
been used recently in better systems for

580
00:45:57,979 --> 00:46:02,910
doing kind of vision systems with
deep learning has been the idea of

581
00:46:02,910 --> 00:46:07,700
residual networks,
commonly shortened as ResNets.

582
00:46:07,700 --> 00:46:14,060
And to a first approximation, so

583
00:46:14,060 --> 00:46:18,800
ResNets is saying gee,
we want to be able to build 100 layer

584
00:46:18,800 --> 00:46:23,400
deep neural networks and
be able to train those successfully.

585
00:46:23,400 --> 00:46:25,310
And to a first approximation,

586
00:46:25,310 --> 00:46:31,490
the way ResNets are doing that is exactly
the same idea here with the plus sign.

587
00:46:31,490 --> 00:46:34,215
It's saying, as you go up each layer,

588
00:46:34,215 --> 00:46:40,340
we're going to calculate some non-linear
function using a regular neural net layer.

589
00:46:40,340 --> 00:46:42,471
But will offer the alternative,

590
00:46:42,471 --> 00:46:46,504
which is that you can just shunt
stuff up from the layer before,

591
00:46:46,504 --> 00:46:51,430
add those two together, and
repeat over again and go up 100 layers.

592
00:46:51,430 --> 00:46:56,336
And so this plus sign,
you may have learned in third grade, but

593
00:46:56,336 --> 00:47:02,274
turns out plus signs have been a really
useful part of modern deep learning.

594
00:47:02,274 --> 00:47:09,726
Okay, Yeah, here is my little picture,
which I'll just show.

595
00:47:09,726 --> 00:47:14,305
I think you'll have to sort
of then slow it down to

596
00:47:14,305 --> 00:47:19,222
understand that this is sort
of going backwards from

597
00:47:19,222 --> 00:47:24,472
Time 128 as to how long
information lasts in an LSTM,

598
00:47:24,472 --> 00:47:29,020
and it sort of looks
like this if I play it.

599
00:47:29,020 --> 00:47:33,525
And so if we then try and drag it back,
I think, then I can play it more slowly.

600
00:47:33,525 --> 00:47:38,467
All right, so that almost instantaneously,
the RNN has less

601
00:47:38,467 --> 00:47:44,308
information because of
the Matrix multiply.

602
00:47:44,308 --> 00:47:47,780
But as you go back,
that by the time you've gone back so

603
00:47:47,780 --> 00:47:52,900
at ten times steps, the RNN is
essentially lost the information.

604
00:47:52,900 --> 00:47:57,030
Whereas the LSTM even be going back,

605
00:47:57,030 --> 00:48:01,220
it starts loose information, but you know
you sort of gain back this sort of more

606
00:48:01,220 --> 00:48:06,150
like, time step 30 or
something before it's kind of

607
00:48:06,150 --> 00:48:09,970
lost all of its information which is sort
of the intuition I suggested before.

608
00:48:09,970 --> 00:48:16,346
But something like 100 time
steps you can get out of a LSTM.

609
00:48:16,346 --> 00:48:21,375
Almost up for a halftime break,
and the research highlight,

610
00:48:21,375 --> 00:48:25,632
but before that couple other
things I wanted to say,

611
00:48:25,632 --> 00:48:29,620
here's just a little bit
of practical advice.

612
00:48:29,620 --> 00:48:36,840
So both for assignment for or
for many people's final projects.

613
00:48:36,840 --> 00:48:40,829
They're gonna be wanting
to train recurrent neural

614
00:48:40,829 --> 00:48:44,150
networks with LSTMs on a largest scale.

615
00:48:44,150 --> 00:48:46,890
So here is some of the tips
that you should know, yes.

616
00:48:46,890 --> 00:48:50,407
So if you wanna build a big
recurrent new network,

617
00:48:50,407 --> 00:48:53,099
definitely use either GRU or an LSTM.

618
00:48:53,099 --> 00:48:56,959
So for any of these recurrent networks,

619
00:48:56,959 --> 00:49:01,622
initialization is really,
really important.

620
00:49:01,622 --> 00:49:07,117
That if your net, recurrent your network
should work, if your network isn't

621
00:49:07,117 --> 00:49:12,780
working, often times it's because
the initial initialization is bad.

622
00:49:12,780 --> 00:49:17,960
So what are the kind of initialization
ideas that often tend to be important?

623
00:49:17,960 --> 00:49:23,230
It's turned to be really useful for
the recurrent matrices, that's the one

624
00:49:23,230 --> 00:49:27,320
where you're multiplying by the previous
hidden state of previous cell state.

625
00:49:27,320 --> 00:49:29,580
It's really useful to
make that one orthogonal.

626
00:49:29,580 --> 00:49:33,600
So there's chance to use your good
old-fashioned linear algebra.

627
00:49:33,600 --> 00:49:37,310
There aren't actually that many
parameters in a recurrent neural net.

628
00:49:37,310 --> 00:49:41,635
And giving an orthogonal
initialization has proved to

629
00:49:41,635 --> 00:49:46,437
be a better way to kinda get
them learning something useful.

630
00:49:46,437 --> 00:49:49,576
Even with sort of these
ideas with GRUs and LSTMs,

631
00:49:49,576 --> 00:49:54,750
you're gonna kinda keep multiplying
things in a recurrent neural network.

632
00:49:54,750 --> 00:49:58,810
So normally, you wanna have
your initialization is small.

633
00:49:58,810 --> 00:50:02,250
If you start off with two large
values that can destroy things,

634
00:50:02,250 --> 00:50:05,830
try making the numbers smaller.

635
00:50:05,830 --> 00:50:08,140
Here's a little trick, so

636
00:50:08,140 --> 00:50:13,840
a lot of the times we initialize
things near zero, randomly.

637
00:50:13,840 --> 00:50:19,413
An exception to that is when you're
setting the bias of a forget gate,

638
00:50:19,413 --> 00:50:24,135
it normally works out much better
if you set the bias gate for

639
00:50:24,135 --> 00:50:28,764
the forget gate to a decent size
positive number like one or

640
00:50:28,764 --> 00:50:32,079
two or
a random number close to one or two.

641
00:50:32,079 --> 00:50:36,992
That's sort of effectively saying
you should start off paying

642
00:50:36,992 --> 00:50:40,060
a lot of attention to the distant past.

643
00:50:40,060 --> 00:50:43,380
That's sort of biasing it
to keep long term memory.

644
00:50:43,380 --> 00:50:46,030
And that sort of encourages
you to get a good model.

645
00:50:46,030 --> 00:50:48,620
Which effectively uses long term memory.

646
00:50:48,620 --> 00:50:53,626
And if the long term past stuff isn't
useful, it can shrink that down.

647
00:50:53,626 --> 00:50:57,283
But if the forget gate starts
off mainly forgetting stuff,

648
00:50:57,283 --> 00:51:01,320
it'll just forget stuff and
never change to any other behavior.

649
00:51:02,730 --> 00:51:06,190
In general, these algorithms work much

650
00:51:06,190 --> 00:51:08,670
better with modern adaptive
learning rate algorithms.

651
00:51:08,670 --> 00:51:10,880
We've already been using
Adam in the assignments.

652
00:51:10,880 --> 00:51:16,787
The ones like Adam, AdaDelta,
RMSprop work a lot better than basic SGD.

653
00:51:16,787 --> 00:51:19,490
You do wanna clip
the norms of the gradients.

654
00:51:19,490 --> 00:51:22,999
You can use a number like five,
that'll work fine.

655
00:51:22,999 --> 00:51:25,711
And so,
we've used dropout in the assignments, but

656
00:51:25,711 --> 00:51:28,797
we haven't actually ever talked
about it much in lectures.

657
00:51:28,797 --> 00:51:34,810
For RNNs of any sort,
it's trivial to do dropout vertically.

658
00:51:34,810 --> 00:51:38,280
And that usually improves performance.

659
00:51:38,280 --> 00:51:39,470
It doesn't work and

660
00:51:39,470 --> 00:51:44,660
I either do drop out horizontally
along the recurrent connections.

661
00:51:44,660 --> 00:51:48,359
Because if you have reasonable
percentage of drop out and

662
00:51:48,359 --> 00:51:52,829
you run it horizontally then within
the few time steps, almost every

663
00:51:52,829 --> 00:51:57,936
dimension will be dropped in one of them,
and so you have no information flow.

664
00:51:57,936 --> 00:52:03,044
There have been more recent work
that's talked about ways that you

665
00:52:03,044 --> 00:52:08,243
can successfully do horizontal
dropout in recurrent networks in,

666
00:52:08,243 --> 00:52:13,167
including orthongal's PhD student
in England who did work on so

667
00:52:13,167 --> 00:52:16,933
called base in drop out
that works well for that.

668
00:52:16,933 --> 00:52:20,935
But quite commonly, it's still the case
that people just drop out vertically and

669
00:52:20,935 --> 00:52:23,220
don't drop out at all horizontally.

670
00:52:23,220 --> 00:52:27,340
The final bit of advice is be
patient if you're running,

671
00:52:27,340 --> 00:52:31,580
if you're learning recurrent
nets over large data sets,

672
00:52:31,580 --> 00:52:34,370
it often takes quite a while and
you don't wanna give up.

673
00:52:34,370 --> 00:52:37,720
Sometimes if you just train them
long enough start to learn stuff.

674
00:52:37,720 --> 00:52:42,641
This is one of the reasons why we
really want to get you guys started

675
00:52:42,641 --> 00:52:45,862
using GPUs because the fact of the matter,

676
00:52:45,862 --> 00:52:50,785
if you're actually trying to do
things on decent size data sets,

677
00:52:50,785 --> 00:52:56,353
you just don't wanna be trying to train
in LSTM or GRU without Using a GPU.

678
00:52:56,353 --> 00:53:02,330
One other last tip that we should
mention some time is ensembling.

679
00:53:02,330 --> 00:53:08,190
If you'd like your numbers to be 2%
higher, very effective strategy,

680
00:53:08,190 --> 00:53:12,390
which again, makes it good to have a GPU,
is don't train just one model,

681
00:53:12,390 --> 00:53:16,250
train ten models and
you average their predictions and

682
00:53:16,250 --> 00:53:19,640
that that normally gives you
quite significant gains.

683
00:53:19,640 --> 00:53:24,378
So here are some results
from MT Systems trained.

684
00:53:24,378 --> 00:53:26,436
Montreal again.

685
00:53:26,436 --> 00:53:30,318
So it's different language pairs.

686
00:53:30,318 --> 00:53:32,661
The red ones is a single model.

687
00:53:32,661 --> 00:53:37,610
The purple ones are training 8 models,
and in this case,

688
00:53:37,610 --> 00:53:42,165
it's actually just majority
voting them together.

689
00:53:42,165 --> 00:53:45,643
But you can also sort of
average their predictions and

690
00:53:45,643 --> 00:53:50,898
you can see it's just giving very nice
gains in performance using the measure for

691
00:53:50,898 --> 00:53:54,392
mt performance which I'll
explain after the break.

692
00:53:54,392 --> 00:54:02,260
But we're now gonna have Michael up
to talk about the research highlight.

693
00:54:02,260 --> 00:54:04,073
And I'll quickly explain
it until the video is in

694
00:54:04,073 --> 00:54:04,634
there-
>> Okay.

695
00:54:04,634 --> 00:54:06,009
>> After the picture.

696
00:54:06,009 --> 00:54:07,500
>> Okay.

697
00:54:08,520 --> 00:54:09,192
Hi, everyone.

698
00:54:09,192 --> 00:54:13,380
I'm gonna be presenting the paper
Lip Reading Sentences in the Wild.

699
00:54:14,630 --> 00:54:19,070
So our task is basically taking a video,
which we preprocessed into

700
00:54:19,070 --> 00:54:23,900
a sequence of lip-centered images,
with or without audio.

701
00:54:23,900 --> 00:54:28,439
And we're trying to predict like the words
that are being said in the video.

702
00:54:28,439 --> 00:54:31,275
>> Just slide after that one.

703
00:54:34,277 --> 00:54:36,515
Maybe it doesn't

704
00:55:03,061 --> 00:55:06,060
>> The government will pay for both sides.

705
00:55:06,060 --> 00:55:09,260
>> We have to look at whether it
>> Not.

706
00:55:09,260 --> 00:55:12,352
Said security had been
stepped up in Britain.

707
00:55:29,494 --> 00:55:34,152
>> Cool, so anyway,
it's hard to do lip reading.

708
00:55:34,152 --> 00:55:38,340
So anyway, and for the rest of this I'll
talk about what architecture they use,

709
00:55:38,340 --> 00:55:42,323
which is, they deem the watch,
listen, attend, and spell model.

710
00:55:42,323 --> 00:55:45,746
>> Gonna talk about some of these training
strategies that might also be helpful for

711
00:55:45,746 --> 00:55:46,711
your final projects.

712
00:55:46,711 --> 00:55:48,915
There's also the dataset and

713
00:55:48,915 --> 00:55:54,124
the results was actually surpassing
like a professional lip reader.

714
00:55:54,124 --> 00:55:58,210
So, the architecture basically
breaks down into three components.

715
00:55:58,210 --> 00:56:03,298
We have a watch component which takes
in the visual and the listening

716
00:56:03,298 --> 00:56:09,367
component which takes in the audio and
these feed information to the attend, and

717
00:56:09,367 --> 00:56:14,470
spell module which outputs
the prediction one character at a time.

718
00:56:15,696 --> 00:56:20,410
And they also use this with like, just the
watch module or just the listen module.

719
00:56:22,510 --> 00:56:26,610
To go into slightly more detail,
for the watch module,

720
00:56:26,610 --> 00:56:32,130
we take a sliding window over
like the face centered images and

721
00:56:32,130 --> 00:56:35,450
feed that into a CNN,
which then the output of

722
00:56:35,450 --> 00:56:41,240
the CNN gets fed into an LSTM
much size over the time steps.

723
00:56:41,240 --> 00:56:46,760
We output a single state vector S of v,
as well as the set of

724
00:56:46,760 --> 00:56:52,517
output vectors L of v and
the listen module is very similar.

725
00:56:52,517 --> 00:56:55,388
We take the pre-processed speech and

726
00:56:55,388 --> 00:57:00,591
we again site over using the LSTM,
and we have another state vector,

727
00:57:00,591 --> 00:57:05,630
and another set of output vectors,
and then in the decoding step.

728
00:57:06,940 --> 00:57:11,620
So we have an LSTM as a really
steps of during the decoding and

729
00:57:11,620 --> 00:57:16,660
the initial hidden state is initialized
as the concatenation of the two hidden

730
00:57:16,660 --> 00:57:21,520
states from the two previous
modules as well as we have

731
00:57:21,520 --> 00:57:26,340
like a dual attention mechanism
which takes in the output

732
00:57:26,340 --> 00:57:31,030
vectors from each of their respective
modules, and we take those together, and

733
00:57:31,030 --> 00:57:35,210
we make our prediction using a softmax
over a multi-layer procepteron.

734
00:57:37,320 --> 00:57:40,360
And so, one strategy that uses
called curriculum learning.

735
00:57:40,360 --> 00:57:45,971
So ordinarily, when you're training
this sequence to sequence models,

736
00:57:45,971 --> 00:57:50,165
you might be tend to just use
one full sentence at a time.

737
00:57:50,165 --> 00:57:55,780
Tip by what they do on curriculum learning
is you start with the word length like

738
00:57:55,780 --> 00:58:01,652
segment and then you can slowly increase
the length of your training sequences and

739
00:58:01,652 --> 00:58:07,013
what happens is you're actually like
the idea is you're trying to learn,

740
00:58:07,013 --> 00:58:10,587
like slowly build up the learning for
the model and

741
00:58:10,587 --> 00:58:16,414
what happens is it ends up converging
faster as well as decreasing overfitting.

742
00:58:16,414 --> 00:58:20,253
Another thing that they use
is called scheduled sampling.

743
00:58:20,253 --> 00:58:24,649
So ordinarily during training,
you'll be using

744
00:58:24,649 --> 00:58:29,359
the ground truth input like
character sequence, but

745
00:58:29,359 --> 00:58:34,593
during the test time you
wouldn't be using that you'd just

746
00:58:34,593 --> 00:58:40,190
be using your previous prediction
after every time step.

747
00:58:40,190 --> 00:58:43,980
So what you do in scheduled sampling is
kind of like bridge the difference in

748
00:58:43,980 --> 00:58:47,440
scenarios between training and
testing is that you actually just for

749
00:58:47,440 --> 00:58:51,470
a random small probability,
like sample from the previous input

750
00:58:51,470 --> 00:58:55,140
instead of the ground truth input for
that time step during training.

751
00:58:58,180 --> 00:59:04,936
So the dataset was taken from the authors
collected it from the BBC News and

752
00:59:04,936 --> 00:59:09,620
they have like dataset that's much
larger than the previous ones

753
00:59:09,620 --> 00:59:13,790
out there with over 17,000
vocabulary words and

754
00:59:13,790 --> 00:59:18,160
the other the quite a bit like processing
to like some other things on the lips, and

755
00:59:18,160 --> 00:59:21,120
do like the alignment of the audio,
and the visuals.

756
00:59:23,810 --> 00:59:29,067
So, just to talk about the results, I
guess the most eye popping result is that

757
00:59:29,067 --> 00:59:34,169
they gave the test set to actually like
a company that does like professional

758
00:59:34,169 --> 00:59:39,428
lip reading and they're only able to get
about like one in four words correct or

759
00:59:39,428 --> 00:59:44,549
as this model was able to get one in two,
roughly, based on word error rate.

760
00:59:44,549 --> 00:59:48,984
And they also did some other
experience as well with looking at,

761
00:59:48,984 --> 00:59:52,041
if you combine the lips
version with the audio,

762
00:59:52,041 --> 00:59:57,469
you get like a slightly better model which
shows that using both modalities improves

763
00:59:57,469 --> 01:00:02,149
the model as well as looking at what
happens if you add noise to the model.

764
01:00:02,149 --> 01:00:03,450
Great.
Thanks.

765
01:00:03,450 --> 01:00:09,398
>> [APPLAUSE]
>> Thanks, Michael.

766
01:00:09,398 --> 01:00:11,379
Yeah, so obviously,
a lot of details there.

767
01:00:11,379 --> 01:00:16,211
But again, that's kind of an example of
what's been happening with deep learning

768
01:00:16,211 --> 01:00:21,111
where you're taking this basic model
architecture, things like LSTM and saying,

769
01:00:21,111 --> 01:00:24,492
here's another problem,
let's try it on that as well and

770
01:00:24,492 --> 01:00:26,933
it turns out to work fantastically well.

771
01:00:26,933 --> 01:00:28,365
Let's say, 20 minutes left.

772
01:00:28,365 --> 01:00:31,869
I'll see how high I can get in teaching
everything else about it on machine

773
01:00:31,869 --> 01:00:32,613
translation.

774
01:00:32,613 --> 01:00:36,828
So it's something I did just want
to explain is so, back here and

775
01:00:36,828 --> 01:00:41,450
in general, when we've been showing
machine translation results.

776
01:00:41,450 --> 01:00:44,212
We've been divvying these
graphs that up is good and

777
01:00:44,212 --> 01:00:48,347
what it's been measuring with these
numbers are things called blue scores.

778
01:00:48,347 --> 01:00:54,051
So, I wanted to give you some idea of how
and why we evaluate machine translation.

779
01:00:54,051 --> 01:01:00,266
So the central thing to know about machine
translation is if you take a paragraph or

780
01:01:00,266 --> 01:01:04,410
text and give it to ten
different humans translators,

781
01:01:04,410 --> 01:01:08,340
you'll get back ten
different translations.

782
01:01:08,340 --> 01:01:13,068
There's no correct answer
as to how to translate

783
01:01:13,068 --> 01:01:16,537
a sentence into another language.

784
01:01:16,537 --> 01:01:21,007
And in practice, most of the time
all translations are imperfect and

785
01:01:21,007 --> 01:01:26,091
it's kind of deciding what you wanna pay
most attention to is that do you want to

786
01:01:26,091 --> 01:01:31,179
maximally preserve the metaphor that
the person used in the source language or

787
01:01:31,179 --> 01:01:34,956
do you wanna more directly
convey the meaning it conveys,

788
01:01:34,956 --> 01:01:40,666
because that metaphor won't really be
familiar to people in the target language.

789
01:01:40,666 --> 01:01:43,475
Do you want to choose sort
of short direct words,

790
01:01:43,475 --> 01:01:46,362
because it's written in a short,
direct style?

791
01:01:46,362 --> 01:01:48,565
Or do you more want to sort of,

792
01:01:48,565 --> 01:01:52,984
you choose a longer word that's
a more exact translation?

793
01:01:52,984 --> 01:01:56,872
There's all of these decisions and
things and in some sense a translator is

794
01:01:56,872 --> 01:01:59,710
optimizing over if we do it
in machine learning terms,

795
01:01:59,710 --> 01:02:02,194
but the reality is it's
sort of not very clear.

796
01:02:02,194 --> 01:02:03,980
There are a lot of choices.

797
01:02:03,980 --> 01:02:07,784
You have lots of syntactic choices
as whether you make it a passive or

798
01:02:07,784 --> 01:02:09,864
an active and word order, and so on.

799
01:02:09,864 --> 01:02:10,796
No right answer.

800
01:02:10,796 --> 01:02:14,638
So we just can't have it like a lot
things of saying, here's the accuracy,

801
01:02:14,638 --> 01:02:16,680
that was what you were meant to use.

802
01:02:16,680 --> 01:02:18,273
So, how do you do it?

803
01:02:18,273 --> 01:02:22,450
So, one way to do MT evaluation
is to do it manually.

804
01:02:22,450 --> 01:02:26,654
You get human beings to look
at translations and to say,

805
01:02:26,654 --> 01:02:28,089
how good they are.

806
01:02:28,089 --> 01:02:29,901
And to this day, basically,

807
01:02:29,901 --> 01:02:34,540
that's regarded as the gold standard
of machine translation evaluation,

808
01:02:34,540 --> 01:02:38,740
because we don't have a better
way to fully automate things.

809
01:02:38,740 --> 01:02:43,212
So one way of doing that is things
like Likert scales where you're

810
01:02:43,212 --> 01:02:46,791
getting humans to judge
translations to adequacy,

811
01:02:46,791 --> 01:02:52,077
which is how well they convey the meaning
of the source and fluency which is for

812
01:02:52,077 --> 01:02:56,492
how natural the output sentence
sounds in the target language.

813
01:02:56,492 --> 01:03:02,275
Commonly, a way that's more easily
measurable that people prefer is actually

814
01:03:02,275 --> 01:03:08,058
if you're comparing systems for goodness
is that you directly ask human beings

815
01:03:08,058 --> 01:03:13,603
to do pairwise judgments of which is
better translation A or translation B.

816
01:03:13,603 --> 01:03:17,587
I mean, it turns out that even
that is incredibly hard for

817
01:03:17,587 --> 01:03:23,071
humans to do as someone who has sat around
doing this task of human evaluation.

818
01:03:23,071 --> 01:03:26,028
I mean, all the time, it's kind of okay,

819
01:03:26,028 --> 01:03:30,793
this one made a bad word choice here and
this one got the wrong verb form

820
01:03:30,793 --> 01:03:34,178
there which of these do I
regard as a worse error.

821
01:03:34,178 --> 01:03:38,730
So it's a difficult thing, but
we use the data we can from human beings.

822
01:03:38,730 --> 01:03:41,820
Okay, that's still the best
thing that we can do.

823
01:03:41,820 --> 01:03:43,530
It has problems.

824
01:03:43,530 --> 01:03:50,770
Basically, it's slow and expensive to get
human beings to judge translation quality.

825
01:03:50,770 --> 01:03:52,540
So what else could we do?

826
01:03:52,540 --> 01:03:57,454
Well, another obvious idea is to say,
well, If we can embed machine

827
01:03:57,454 --> 01:04:02,992
translation into some task, we can just
see which is more easily a valuable.

828
01:04:02,992 --> 01:04:09,360
We could just see which MT system
lets us do the final task better.

829
01:04:09,360 --> 01:04:13,600
So, we'd like to do question answering
over foreign language documents.

830
01:04:13,600 --> 01:04:17,317
We'll just to get our question
answers correct score, and

831
01:04:17,317 --> 01:04:19,633
they'll be much easier to measure.

832
01:04:19,633 --> 01:04:22,405
And that's something that you can do, but

833
01:04:22,405 --> 01:04:25,877
it turns out that that often
isn't very successful.

834
01:04:25,877 --> 01:04:29,726
Cuz commonly your accuracy
on the downstream task is

835
01:04:29,726 --> 01:04:34,453
very little affected by many of
the fine points of translation.

836
01:04:34,453 --> 01:04:38,940
An extreme example of that is sort of
like cross-lingual information retrieval.

837
01:04:38,940 --> 01:04:41,690
When you're just wanting
to retrieve relevant

838
01:04:41,690 --> 01:04:44,000
documents to a query in another language.

839
01:04:44,000 --> 01:04:48,113
That providing you can kind of produce
some of the main content words in

840
01:04:48,113 --> 01:04:53,131
the translation, it really doesn't matter
how you screw up the details of syntax and

841
01:04:53,131 --> 01:04:54,265
verb inflection.

842
01:04:54,265 --> 01:04:55,910
It's not really gonna affect your score.

843
01:04:57,650 --> 01:05:00,650
Okay, so what people have
wanted to have is a direct

844
01:05:02,070 --> 01:05:06,340
metric that is fast and cheap to apply.

845
01:05:06,340 --> 01:05:11,270
And for a long time, I think no one
thought there was such a thing.

846
01:05:11,270 --> 01:05:15,960
And so
then starting in the very early 2000s,

847
01:05:15,960 --> 01:05:20,920
people at IBM suggested
this first idea of, hey,

848
01:05:20,920 --> 01:05:26,740
here's a cheap way in which we can
measure word translation quality.

849
01:05:26,740 --> 01:05:29,560
And so they called it the BLEU metric.

850
01:05:29,560 --> 01:05:34,324
And so
here was the idea of how they do that.

851
01:05:34,324 --> 01:05:39,540
What they said is let us
produce reference translations.

852
01:05:39,540 --> 01:05:44,440
We know that there are many, many possible
ways that something can be translated.

853
01:05:44,440 --> 01:05:53,150
But let's get a human being to
produce a reference translation.

854
01:05:53,150 --> 01:05:57,974
So what we are going to do is then we're
going to have a reference translation by

855
01:05:57,974 --> 01:06:01,940
a human, and
we're going to have a machine translation.

856
01:06:01,940 --> 01:06:06,622
And to a first approximation we're
going to say that the machine

857
01:06:06,622 --> 01:06:11,490
translation is good to the extent
that you can find word n-grams.

858
01:06:11,490 --> 01:06:15,691
So sequences of words like three
words in a row, two words in a row,

859
01:06:15,691 --> 01:06:20,310
which also appear in the reference
translation anywhere.

860
01:06:20,310 --> 01:06:21,946
So what are the elements of this?

861
01:06:21,946 --> 01:06:28,780
So by having multi-word sequences,
that's meant to be trying to

862
01:06:28,780 --> 01:06:33,660
judge whether you have some understanding
of the sort of right syntax and arguments.

863
01:06:33,660 --> 01:06:37,270
Because you're much more likely
to match a four word sequence

864
01:06:37,270 --> 01:06:40,530
if it's not just you've
got a bag of keywords.

865
01:06:40,530 --> 01:06:44,090
You actually understand something
of the syntax of the sentence.

866
01:06:44,090 --> 01:06:48,710
The fact that you can match it anywhere
is meant to be dealing with the fact that

867
01:06:48,710 --> 01:06:51,860
human languages normally have
quite flexible word order.

868
01:06:51,860 --> 01:06:57,539
So it's not adequate to insist that
the phrases appear in the same word order.

869
01:06:57,539 --> 01:07:02,503
Of course, in general in English, a lot of
the time you can say, last night I went

870
01:07:02,503 --> 01:07:07,040
to my friend's place, or,
I went to my friend's place last night.

871
01:07:07,040 --> 01:07:09,890
And it seems like you should
get credit for last night

872
01:07:09,890 --> 01:07:12,440
regardless of whether you put it at
the beginning or the end of the sentence.

873
01:07:13,910 --> 01:07:17,833
So, that was the general idea
in slightly more detail.

874
01:07:17,833 --> 01:07:20,570
The BLEU measure is a precision score.

875
01:07:20,570 --> 01:07:25,025
So it's looking at whether
n-grams that are in the machine

876
01:07:25,025 --> 01:07:29,312
translation also appear in
the reference translation.

877
01:07:29,312 --> 01:07:31,710
There are a couple of fine points then.

878
01:07:31,710 --> 01:07:36,740
You are only allowed to count for
a certain n and n-gram once.

879
01:07:36,740 --> 01:07:41,800
So if in your translation,
the airport appears three times,

880
01:07:41,800 --> 01:07:44,660
but there's only one
the airport in the reference,

881
01:07:44,660 --> 01:07:48,640
you're only allowed to count one of
them as correct, not all three of them.

882
01:07:48,640 --> 01:07:53,900
And then there's this other trick that we
have, this thing called a brevity penalty.

883
01:07:53,900 --> 01:07:57,576
Because if it's purely
a precision-oriented measure,

884
01:07:57,576 --> 01:08:01,932
saying is what appears in the machine
translation in the reference.

885
01:08:01,932 --> 01:08:03,772
There are games you could play,

886
01:08:03,772 --> 01:08:07,398
like you could just translate
every passage with the word the.

887
01:08:07,398 --> 01:08:11,287
Because if it's English the word the is
pretty sure to appear somewhere in

888
01:08:11,287 --> 01:08:14,122
the reference translation,
and get precision one.

889
01:08:14,122 --> 01:08:16,130
And that seems like it's cheating.

890
01:08:16,130 --> 01:08:21,475
So if you're making what your translation
is shorter than the human translations,

891
01:08:21,475 --> 01:08:25,040
you'll lose.

892
01:08:25,040 --> 01:08:31,250
Okay, so more formally, so you're doing
this with n-grams up to a certain size.

893
01:08:31,250 --> 01:08:35,330
Commonly it's four so you use single
words, pairs of words, triples,

894
01:08:35,330 --> 01:08:36,670
and four words.

895
01:08:36,670 --> 01:08:38,895
You work out this kind
of precision of each.

896
01:08:38,895 --> 01:08:43,074
And then you're working out a kind
of a weighted geometric mean

897
01:08:43,075 --> 01:08:44,854
of those precisions.

898
01:08:44,854 --> 01:08:47,752
And you multiplying that
by brevity penalty.

899
01:08:47,752 --> 01:08:52,701
And the brevity penalty penalizes
you if your translation

900
01:08:52,702 --> 01:08:56,546
is shorter than the reference translation.

901
01:08:56,546 --> 01:09:00,590
There are some details here, but
maybe I'll just skip them and go ahead.

902
01:09:00,590 --> 01:09:06,609
So there's one other idea then which is,
well, what about this big problem that,

903
01:09:06,609 --> 01:09:11,090
well, there are a lot of different
ways to translate things.

904
01:09:11,091 --> 01:09:15,339
And there's no guarantee that your
translation could be great, and

905
01:09:15,339 --> 01:09:18,870
it might just not match
the human's translation.

906
01:09:18,870 --> 01:09:24,225
And so the answer to that that
the original IBM paper suggested

907
01:09:24,225 --> 01:09:30,096
was what we should do is collect
a bunch of reference translations.

908
01:09:30,096 --> 01:09:34,089
And the suggested number that's
been widely used was four.

909
01:09:34,090 --> 01:09:39,680
And so then, most likely,
if you're giving a good translation,

910
01:09:39,680 --> 01:09:43,069
it'll appear in one of
the reference translations.

911
01:09:43,069 --> 01:09:45,891
And then, you'll get a matching n-gram.

912
01:09:45,892 --> 01:09:49,109
Now, of course,
that's the sort of a statistical argument.

913
01:09:49,109 --> 01:09:51,780
Cuz you might have a really
good translation and

914
01:09:51,781 --> 01:09:53,999
none of the four translators chose it.

915
01:09:53,999 --> 01:09:57,785
And the truth is then in
that case you just lose.

916
01:09:57,785 --> 01:10:02,745
And indeed what's happened in more
recent work is quite a lot of the time,

917
01:10:02,745 --> 01:10:07,718
actually, the BLEU measure is only
run with one reference translation.

918
01:10:07,718 --> 01:10:10,133
And that's seems a little bit cheap.

919
01:10:10,133 --> 01:10:14,922
And it's certainly the case that if you're
running with one reference translation,

920
01:10:14,922 --> 01:10:19,448
you're either just lucky or unlucky as to
whether you guessed to translate the way

921
01:10:19,448 --> 01:10:21,270
the translator translates.

922
01:10:21,270 --> 01:10:25,643
But you can make a sort of a statistical
argument which by and large is valid.

923
01:10:25,643 --> 01:10:28,549
That if you're coming up
with good translations,

924
01:10:28,549 --> 01:10:33,359
providing there's no correlation somehow
between one system and the translator.

925
01:10:33,359 --> 01:10:38,015
That you'd still expect on balance that
you'll get a higher score if you're

926
01:10:38,015 --> 01:10:40,950
consistently giving better translations.

927
01:10:40,950 --> 01:10:43,260
And broadly speaking, that's right.

928
01:10:43,260 --> 01:10:49,445
Though this problem of correlation does
actually start to rear its head, right?

929
01:10:49,445 --> 01:10:54,124
That if the reference translator always
translated the things as US, and

930
01:10:54,124 --> 01:10:59,336
one system translates with US, and the
other one translates with United States.

931
01:10:59,336 --> 01:11:01,211
Kind of one person will get lucky, and

932
01:11:01,211 --> 01:11:04,147
the other one will get unlucky
in a kind of a correlated way.

933
01:11:04,147 --> 01:11:07,004
And that can create problems.

934
01:11:07,004 --> 01:11:12,655
So even though it was very simple
when BLEU was initially introduced,

935
01:11:12,655 --> 01:11:17,445
it seemed to be miraculously
good that it just corresponded

936
01:11:17,445 --> 01:11:22,705
really well with human judgments
of translation quality.

937
01:11:22,705 --> 01:11:26,885
Rarely do you see an empirical
data set that's as linear as that.

938
01:11:26,885 --> 01:11:29,595
And so this seemed really awesome.

939
01:11:29,595 --> 01:11:33,755
Like many things that
are surrogate metrics,

940
01:11:33,755 --> 01:11:38,770
there are a lot of surrogate metrics that
work really well If no one is trying

941
01:11:38,770 --> 01:11:43,860
to optimize them but don't work so well
once people are trying to optimize them.

942
01:11:43,860 --> 01:11:45,620
So what happen then was,

943
01:11:45,620 --> 01:11:49,690
everyone evaluated their systems
on BLEU scores and so therefore,

944
01:11:49,690 --> 01:11:53,970
all researchers worked on how to make
their systems have better BLEU scores.

945
01:11:53,970 --> 01:11:58,850
And then what happened is this
correlation graph went way down.

946
01:11:58,850 --> 01:12:03,820
And so the truth is now that current,
and this relates to the sort of

947
01:12:03,820 --> 01:12:06,900
when I was saying the Google
results were exaggerated.

948
01:12:06,900 --> 01:12:14,380
The truth is that current MT systems
produce BLEU scores that are very similar

949
01:12:14,380 --> 01:12:18,440
to human translations for many language
pairs which reflects the fact that

950
01:12:18,440 --> 01:12:23,540
different human beings are quite creative
and vary in how they translate sensors.

951
01:12:23,540 --> 01:12:27,770
But in truth, the quality of machine
translation is still well below

952
01:12:27,770 --> 01:12:31,651
the quality of human translation.

953
01:12:31,651 --> 01:12:35,195
Okay, few minutes left to
say a bit more about MT.

954
01:12:35,195 --> 01:12:37,595
I think I can't get through
all this material, but

955
01:12:37,595 --> 01:12:41,185
let me just give you a little
bit of a sense of some of it.

956
01:12:42,200 --> 01:12:46,910
Okay, so one of the big problems you
have if you've tried to build something,

957
01:12:46,910 --> 01:12:49,170
any kind of generation system,

958
01:12:49,170 --> 01:12:55,070
where you're generating words is you have
a problem that there are a lot of words.

959
01:12:55,070 --> 01:12:58,170
Languages have very large vocabularies.

960
01:12:58,170 --> 01:13:02,480
So from the hidden state,
what we're doing is multiplying by this

961
01:13:02,480 --> 01:13:07,740
matrix of Softmax parameters,
which is the size of the vocabulary

962
01:13:07,740 --> 01:13:12,470
times the size of the hidden
state doing this Softmax.

963
01:13:12,470 --> 01:13:14,680
And that's giving us
the probability of different words.

964
01:13:16,020 --> 01:13:19,080
And so the problem is if you wanna
have a very large vocabulary,

965
01:13:19,080 --> 01:13:24,630
you spend a huge amount of time just doing
these Softmaxes over, and over again.

966
01:13:24,630 --> 01:13:29,640
And so, for instance, you saw that in
the kind of pictures of the Google system,

967
01:13:29,640 --> 01:13:34,190
that over half of their
computational power was just going

968
01:13:34,190 --> 01:13:38,620
into calculating these Softmax so
that's being a real problem.

969
01:13:38,620 --> 01:13:42,730
So something people have worked
on quite a lot is how can we

970
01:13:42,730 --> 01:13:45,130
string the cost of that computation.

971
01:13:46,240 --> 01:13:49,650
Well one thing we can do is say,
ha, let's use a smaller vocabulary.

972
01:13:49,650 --> 01:13:54,350
Let's only use a 50,000 word
vocabulary for our MT system, and

973
01:13:54,350 --> 01:13:56,990
some of the early MT
work did precisely that.

974
01:13:56,990 --> 01:14:01,970
But the problem is, that if you do that,
you start with lively sentences.

975
01:14:01,970 --> 01:14:06,650
And instead what you get is unk,
unk, unk because all of

976
01:14:06,650 --> 01:14:12,270
the interesting words in the sentence fall
outside of your 50,000 word vocabulary.

977
01:14:12,270 --> 01:14:18,190
And those kind of sentences are not very
good ones to show that human beings,

978
01:14:18,190 --> 01:14:20,600
because they don't like them very much.

979
01:14:20,600 --> 01:14:25,740
So, it seems like we need to
somehow do better than that.

980
01:14:25,740 --> 01:14:30,540
So, there's been work on, well, how can
we more effectively do the softmaxes

981
01:14:30,540 --> 01:14:33,470
without having to do as much computation.

982
01:14:33,470 --> 01:14:36,400
And so,
there have been some ideas on that.

983
01:14:36,400 --> 01:14:41,420
One idea is to sort of have a hierarchical
Softmax where we do the standard

984
01:14:41,420 --> 01:14:44,510
computer scientist trick
of putting a tree structure

985
01:14:44,510 --> 01:14:46,830
to improve our amount of computation.

986
01:14:46,830 --> 01:14:50,770
So if you can sort of divide the
vocabulary into sort of tree pieces and

987
01:14:50,770 --> 01:14:54,850
divide down branches of the tree,
we can do less computation.

988
01:14:56,070 --> 01:15:00,167
Remember, we did noise
contrast to destination for

989
01:15:00,167 --> 01:15:03,998
words of that was a way
of avoiding computation.

990
01:15:03,998 --> 01:15:06,860
Those are possible ways to do things.

991
01:15:06,860 --> 01:15:09,360
They are not very
GPU-friendly unfortunately.

992
01:15:09,360 --> 01:15:13,920
Once you start taking branches down
the tree, you then can't do the kind

993
01:15:13,920 --> 01:15:17,820
of nice just bang bang bang type
of computations down to GPU.

994
01:15:17,820 --> 01:15:21,930
So there's been on work on coming
up with alternatives to that, and

995
01:15:21,930 --> 01:15:24,930
I wanted to mention one example of this.

996
01:15:24,930 --> 01:15:29,862
And an idea of this is well,
maybe we can actually

997
01:15:29,862 --> 01:15:34,815
sort of just work with small
vocabularies at any one time.

998
01:15:34,815 --> 01:15:39,625
So when we're training our models,
we could train using subsets of

999
01:15:39,625 --> 01:15:45,060
the vocabulary because there's a lot
of rare words but they're rare.

1000
01:15:45,060 --> 01:15:51,220
So if you pick any slice of the training
data most rare words won't be in it.

1001
01:15:51,220 --> 01:15:55,010
Commonly if you look at your
whole vocabulary about 40% of

1002
01:15:55,010 --> 01:15:57,770
your word types occur only once.

1003
01:15:57,770 --> 01:16:02,030
That means if you cut your
data set into 20 pieces,

1004
01:16:02,030 --> 01:16:04,690
19 of those 20 will not contain that word.

1005
01:16:04,690 --> 01:16:09,140
And then,
we also wanna be smart on testing.

1006
01:16:09,140 --> 01:16:15,470
So we wanna be able to, at test time
as well, generate sort of a smaller

1007
01:16:15,470 --> 01:16:20,990
set of words for our soft max, and so we
can be fast at both train and test time.

1008
01:16:20,990 --> 01:16:22,290
Well, how can you do that?

1009
01:16:22,290 --> 01:16:28,630
Well, so at training time,
we want to have a small vocabulary.

1010
01:16:28,630 --> 01:16:33,730
And so we can do that by partitioning the
vocab, for partitioning the training data,

1011
01:16:33,730 --> 01:16:38,449
each slice of the training data,
we'll have a much lower vocabulary.

1012
01:16:39,580 --> 01:16:44,896
And then we could partition randomly or
we could even smarter and

1013
01:16:44,896 --> 01:16:49,532
we can cut it into pieces
that have similar vocabulary.

1014
01:16:49,532 --> 01:16:52,634
If we put all the basketball
articles in one file and

1015
01:16:52,634 --> 01:16:57,775
all the foot walled articles in another
pile, will shrink the vocabulary further.

1016
01:16:57,775 --> 01:17:03,235
And so they look at ways of doing that,
so in practice that they can get down and

1017
01:17:03,235 --> 01:17:07,855
order a magnitude or more in the size
of the vocab that they need for

1018
01:17:07,855 --> 01:17:10,720
each slice of the data, that's great.

1019
01:17:13,040 --> 01:17:15,980
Okay, so what do we do at test time?

1020
01:17:15,980 --> 01:17:18,800
Well, what we wanna do
it at test time as well,

1021
01:17:18,800 --> 01:17:23,800
when we're actually translating,
we want to use as much smaller vocabulary.

1022
01:17:23,800 --> 01:17:26,250
Well, here's an idea of
how you could do that.

1023
01:17:26,250 --> 01:17:28,890
Firstly, we say, they're are just common

1024
01:17:28,890 --> 01:17:32,300
function words that we always
gonna want to have available.

1025
01:17:32,300 --> 01:17:35,030
So we pick the K most frequent words and

1026
01:17:35,030 --> 01:17:37,420
say we're always gonna
have them in our Softmax.

1027
01:17:37,420 --> 01:17:39,930
But then for the rest of it,

1028
01:17:39,930 --> 01:17:44,730
what we're actually gonna do is
sort of have a lexicon on the side

1029
01:17:44,730 --> 01:17:49,500
where we're gonna know about likely
translations for each source word.

1030
01:17:49,500 --> 01:17:54,300
So that we'll have stored ways that would
be reasonable to translate she loves

1031
01:17:54,300 --> 01:17:56,440
cats into French.

1032
01:17:56,440 --> 01:17:59,900
And so when we're translating a sentence,
we'll look out for

1033
01:17:59,900 --> 01:18:04,520
each word in the source sentence what
are likely translations of it and

1034
01:18:04,520 --> 01:18:07,770
throw those into our candidates for
the Softmax.

1035
01:18:09,540 --> 01:18:14,750
And so then we've got a sort
of a candidate list of words.

1036
01:18:14,750 --> 01:18:17,660
And when translating
a particular soft sentence,

1037
01:18:17,660 --> 01:18:21,110
we'll only run our
Softmax over those words.

1038
01:18:21,110 --> 01:18:28,290
And then again, we can save well over
an order of magnitude computations.

1039
01:18:28,290 --> 01:18:34,230
So, K prime is about 10 or 20 and
K is sort of a reasonable size vocab.

1040
01:18:34,230 --> 01:18:39,001
We can again, sort of cut at least in
the order of magnitude the size of

1041
01:18:39,001 --> 01:18:42,721
our soft mixers and
act as if we had large vocabulary.

1042
01:18:44,558 --> 01:18:50,060
There are other ways to do that too,
which are on this slide.

1043
01:18:50,060 --> 01:18:54,950
And what I was then going to go on,
and we'll decide whether it does or

1044
01:18:54,950 --> 01:18:57,120
doesn't happen based on the syllabus.

1045
01:18:57,120 --> 01:19:02,930
I mean, you could sort of say,
well, that's still insufficient

1046
01:19:02,930 --> 01:19:07,760
because I sort of said that you have
to deal with a large vocabulary.

1047
01:19:07,760 --> 01:19:13,960
And you've sort of told us how to deal
with a large vocabulary more efficiently.

1048
01:19:13,960 --> 01:19:18,690
But you've still got problems, because
in any new piece of text you give it,

1049
01:19:18,690 --> 01:19:23,460
you're going to have things like new
names turn up, new numbers turn up, and

1050
01:19:23,460 --> 01:19:26,530
you're going to want to
deal with those as well.

1051
01:19:26,530 --> 01:19:30,620
And so
it seems like somehow we want to be able

1052
01:19:30,620 --> 01:19:35,720
to just deal with new stuff at test time,
at translation time.

1053
01:19:35,720 --> 01:19:39,270
Which effectively means that
kind of theoretically we have

1054
01:19:39,270 --> 01:19:40,590
an infinite vocabulary.

1055
01:19:40,590 --> 01:19:44,260
And so, there's also been a bunch of
work on newer machine translation and

1056
01:19:44,260 --> 01:19:45,470
dealing with that.

1057
01:19:45,470 --> 01:19:49,090
But unfortunately, this class time is not

1058
01:19:49,090 --> 01:19:53,740
long enough to tell you about it right
now, so I'll stop here for today.

1059
01:19:53,740 --> 00:00:00,000
And don't forget, outside you can
collect your midterm on the way out.

