1
00:00:00,000 --> 00:00:04,797
[MUSIC]

2
00:00:04,797 --> 00:00:05,844
Stanford University.

3
00:00:05,844 --> 00:00:09,458
>> All right, hello, everybody.

4
00:00:09,458 --> 00:00:11,770
And welcome to lecture number nine.

5
00:00:11,770 --> 00:00:14,738
Today, we'll do a brief recap,
some organizational stuff, and

6
00:00:14,738 --> 00:00:18,554
then we'll talk about, I love when we call
them fancy, recurrent neural networks.

7
00:00:18,554 --> 00:00:23,491
But those are the most important
deep learning models of the day,

8
00:00:23,491 --> 00:00:26,290
LSTMs and GRU type models.

9
00:00:26,290 --> 00:00:31,140
They're very exciting and really form
the base model for pretty much every

10
00:00:31,140 --> 00:00:34,840
deep learning paper or almost all the deep
learning papers you see out there.

11
00:00:34,840 --> 00:00:40,720
So after today, you'll really have in your
hands the kind of tool that is the default

12
00:00:40,720 --> 00:00:45,040
tool for a lot of different deep learning
final P applications, so super exciting.

13
00:00:45,040 --> 00:00:49,031
And the best part is, you kind of know
most of the important math already of it,

14
00:00:49,031 --> 00:00:50,673
so we can just define the model.

15
00:00:50,673 --> 00:00:54,157
And everything else will kind of
follow through with these basic, and

16
00:00:54,157 --> 00:00:58,320
sometimes painful building blocks
that we went through before.

17
00:00:58,320 --> 00:01:00,640
All right, before we jump in,
some organizational stuff.

18
00:01:00,640 --> 00:01:02,770
We have a new office hour schedule and
places.

19
00:01:02,770 --> 00:01:07,042
Today, we're continuously trying to
optimize the whole process based on

20
00:01:07,042 --> 00:01:08,460
your feedback.

21
00:01:08,460 --> 00:01:09,760
Thanks for that.

22
00:01:09,760 --> 00:01:14,229
So I'll have office hours every day,
multiple times.

23
00:01:14,229 --> 00:01:19,214
I hope that will allow us to kind of
distribute the load a little bit,

24
00:01:19,214 --> 00:01:23,849
cuz I know sometimes lots of people
come to one office hour, and

25
00:01:23,849 --> 00:01:26,552
then there's a long wait there.

26
00:01:26,552 --> 00:01:28,470
Also, it's important that you register for

27
00:01:28,470 --> 00:01:32,740
your GPU teams by the end of today,
or ideally before.

28
00:01:32,740 --> 00:01:36,390
So that we can make
sure you all get a GPU.

29
00:01:36,390 --> 00:01:42,565
Ideally, we also encourage people
to have pairs for Problem set 3 and

30
00:01:42,565 --> 00:01:47,692
4 the project, at least pairs,
cuz we only have 300 or

31
00:01:47,692 --> 00:01:52,000
so GPUs and
almost 700 students in the class.

32
00:01:52,000 --> 00:01:55,735
So try to form teams, but do make sure
that you don't just have your partner or

33
00:01:55,735 --> 00:01:56,252
work on and

34
00:01:56,252 --> 00:02:00,110
implement all the GPU stuff, and you do
all the other parts of the problem set.

35
00:02:00,110 --> 00:02:04,763
Cuz then you really miss out on
a very important valuable skill for

36
00:02:04,763 --> 00:02:10,276
both research and applied deep learning,
if you don't know how to use a GPU.

37
00:02:10,276 --> 00:02:14,250
And sadly,
I have to get back to some work event.

38
00:02:14,250 --> 00:02:16,660
So I'll have a pretty
short office hour today.

39
00:02:16,660 --> 00:02:21,340
But then I know we have the deadline for
project proposals on Thursdays.

40
00:02:21,340 --> 00:02:25,022
So on Thursday,
I'm gonna have an unlimited office hour.

41
00:02:25,022 --> 00:02:28,690
I'm gonna start after class, and
will end when queue status is empty.

42
00:02:28,690 --> 00:02:33,460
So if you come half an hour late,
prepare to talk to me three hours later.

43
00:02:33,460 --> 00:02:38,461
So [LAUGH] the project is the coolest
part, so I don't wanna discourage

44
00:02:38,461 --> 00:02:42,887
people from doing the project
because we don't have enough.

45
00:02:42,887 --> 00:02:45,330
So it's gonna be great.

46
00:02:45,330 --> 00:02:48,016
I'll bring food.
You should bring food too,

47
00:02:48,016 --> 00:02:48,585
and-
>> [LAUGH]

48
00:02:48,585 --> 00:02:49,089
>> [LAUGH]

49
00:02:49,089 --> 00:02:51,160
>> It's kind of good fun, all right.

50
00:02:52,190 --> 00:02:57,049
If by any chance even after midnight,
people, we still have the queue status

51
00:02:57,049 --> 00:03:00,932
is still full, which I doubt at
that point, I think, I hope,

52
00:03:00,932 --> 00:03:05,819
then we can push the proposals out for
those, or you can submit the proposal.

53
00:03:05,819 --> 00:03:09,650
And then we'll figure out the mentor
situation, very soon thereafter.

54
00:03:09,650 --> 00:03:12,740
So all right, any questions
around any class organization?

55
00:03:21,032 --> 00:03:23,470
All right, then lets dive right in.

56
00:03:23,470 --> 00:03:26,927
So basically today,
we'll have a a very advanced,

57
00:03:26,927 --> 00:03:32,745
cutting edge blast from the past, because
while pedagogically, it'll make sense for

58
00:03:32,745 --> 00:03:37,563
us to first talk about a model from 2014,
from just three years ago.

59
00:03:37,563 --> 00:03:41,616
The main model we'll end up with,
the long-short-term-memories is actually

60
00:03:41,616 --> 00:03:45,092
a very old model, from 97, and
has kind of been dormant for a while.

61
00:03:45,092 --> 00:03:48,469
As very powerful model,
you need a lot of training data for it,

62
00:03:48,469 --> 00:03:50,460
you need fast machines for it.

63
00:03:50,460 --> 00:03:54,110
But now, that we have those two things,
this is a very powerful model for NLP.

64
00:03:54,110 --> 00:03:57,010
And if you ask one of the inventors,

65
00:03:57,010 --> 00:04:00,320
the second model is really just
a special case of the LSTM.

66
00:04:00,320 --> 00:04:03,760
But I think, pedagogically,
it makes sense to sort of first talk about

67
00:04:03,760 --> 00:04:06,842
the so-called Gated Recurrent Unit,
which is slightly simpler version.

68
00:04:06,842 --> 00:04:11,669
>> And we'll use machine translation which
is one of the sort of most useful tasks

69
00:04:11,669 --> 00:04:15,280
you might argue of NLP,
sort of a real life task.

70
00:04:15,280 --> 00:04:18,971
Something that actual people
outside academia, outside research,

71
00:04:18,971 --> 00:04:21,166
outside linguistics really care about.

72
00:04:21,166 --> 00:04:27,052
And by the end, you'll actually have the
skills to build one of the best machine

73
00:04:27,052 --> 00:04:32,640
translation models out there, modulo
a lot of time and some extra effort.

74
00:04:32,640 --> 00:04:36,365
But the biggest parts of 90% of
the top MT systems out there,

75
00:04:36,365 --> 00:04:40,161
you'll be able to understand at least and
probably build also,

76
00:04:40,161 --> 00:04:43,480
if you have the GPU
skills after this class.

77
00:04:43,480 --> 00:04:46,930
All right, so I'm not gonna go through
too many of the details, but in just in

78
00:04:46,930 --> 00:04:51,560
preparation to mentally make you also
think about the midterm that's coming up.

79
00:04:51,560 --> 00:04:53,610
Next lecture, we'll have midterm review.

80
00:04:53,610 --> 00:04:57,227
But ideally, these kinds of
equations that I'm throwing up here,

81
00:04:57,227 --> 00:04:58,945
you're pretty familiar with.

82
00:04:58,945 --> 00:05:00,728
At this point, you're like yeah,

83
00:05:00,728 --> 00:05:03,420
I just do some negative
sampling here from my Word2Vec.

84
00:05:04,450 --> 00:05:06,981
And I have my inside and
my outside vectors in the window.

85
00:05:06,981 --> 00:05:12,450
And similarly for glove, I have two
sets of vectors, you optimize this.

86
00:05:12,450 --> 00:05:14,441
You have a function here, dead limits,

87
00:05:14,441 --> 00:05:17,791
how important very frequent pairs
are in your concurrence matrix.

88
00:05:17,791 --> 00:05:22,130
You understand the max-margin
objective function.

89
00:05:22,130 --> 00:05:25,688
You have scores of good windows
from the large training corpus and

90
00:05:25,688 --> 00:05:26,857
corrupted windows.

91
00:05:26,857 --> 00:05:28,924
So all of these should be familiar.

92
00:05:28,924 --> 00:05:33,764
And if not, then you really should also
start thinking about sort of studying

93
00:05:33,764 --> 00:05:35,257
again for the midterm.

94
00:05:36,310 --> 00:05:39,076
The most basic definition of neural net,

95
00:05:39,076 --> 00:05:42,798
where we just have some score
at the end or some soft max.

96
00:05:42,798 --> 00:05:46,706
And really being comfortable
with these two final equations,

97
00:05:46,706 --> 00:05:51,423
that if you understand those, all
the rest of the models will basically be,

98
00:05:51,423 --> 00:05:56,770
in many cases, sort of fancy versions or
adapted versions of these two equations.

99
00:05:56,770 --> 00:05:59,424
So that's important.

100
00:05:59,424 --> 00:06:03,676
And then we'll have our standard recurrent
neural network that we already went

101
00:06:03,676 --> 00:06:07,493
through, and we kind of assume you
should know for the midterm as well.

102
00:06:07,493 --> 00:06:12,392
And our grade of Cross Entropy Error,
as one of the main loss or

103
00:06:12,392 --> 00:06:16,440
objective functions that we optimize.

104
00:06:16,440 --> 00:06:20,177
And when we optimize,
we usually use the Mini-batched SGD.

105
00:06:20,177 --> 00:06:21,080
We don't go through single example.

106
00:06:21,080 --> 00:06:25,136
We don't go through the entire
batch of our trained data, but

107
00:06:25,136 --> 00:06:28,496
we take 100 or so
of examples each time we train.

108
00:06:28,496 --> 00:06:33,590
So all those concepts, you should
feel reasonably comfortable now.

109
00:06:33,590 --> 00:06:37,154
And if not, then definitely come
back to the office hours, and

110
00:06:37,154 --> 00:06:39,450
start sort of studying for the midterm.

111
00:06:39,450 --> 00:06:44,151
All right, and we'll go over more
midterm details in the next lecture.

112
00:06:44,151 --> 00:06:48,500
All right, now, onto the main topic
of today, machine translation.

113
00:06:48,500 --> 00:06:53,273
So you might think for some NLP tasks
that you can get away with thinking of

114
00:06:53,273 --> 00:06:57,118
all the rules that, for
instance, sentiment analysis.

115
00:06:57,118 --> 00:07:00,138
A sentence might come up positive or
negative, right?

116
00:07:00,138 --> 00:07:03,540
You say, I have a list of all the positive
words, most of all the negative words.

117
00:07:03,540 --> 00:07:06,936
And I can think of the ways you can negate
positive words and things like that.

118
00:07:06,936 --> 00:07:10,996
And you could maybe conceive of
creating a sentiment analysis system

119
00:07:10,996 --> 00:07:15,260
of just all your intuitions
about linguistics and sentiment.

120
00:07:15,260 --> 00:07:18,860
That kind of approach is completely
ridiculous for machine translation.

121
00:07:18,860 --> 00:07:20,250
There's no way you would ever,

122
00:07:20,250 --> 00:07:24,010
nobody will ever be able to think of all
the different rules and exceptions for

123
00:07:24,010 --> 00:07:28,410
translating all possible sentences
of one language to another.

124
00:07:28,410 --> 00:07:32,920
So basically, the baseline that's
pretty well established is that

125
00:07:32,920 --> 00:07:36,790
all machine translation systems
are somewhat statistical in nature.

126
00:07:36,790 --> 00:07:38,670
We will always try to
take a very large corpus.

127
00:07:38,670 --> 00:07:41,137
In fact, we'll have so
called parallel copra,

128
00:07:41,137 --> 00:07:44,520
where we have a lot of the sentences or
paragraphs in one language.

129
00:07:44,520 --> 00:07:48,845
And we know that this paragraph in this
language translates to that paragraph

130
00:07:48,845 --> 00:07:50,119
in another language.

131
00:07:50,119 --> 00:07:55,040
One of the popular parallel copra
of All of, for a long time,

132
00:07:55,040 --> 00:07:57,060
for the last couple thousand
years is the Bible, for instance.

133
00:07:57,060 --> 00:07:58,451
You'll have Bible translated.

134
00:07:58,451 --> 00:07:59,789
It has nice paragraphs.

135
00:07:59,789 --> 00:08:02,243
And each paragraph is translated
in different languages.

136
00:08:02,243 --> 00:08:05,614
That would be one of
the first parallel corpora.

137
00:08:05,614 --> 00:08:07,629
The very first is actually
the Rosetta Stone.

138
00:08:07,629 --> 00:08:12,547
Which allowed people to have
at least some understanding

139
00:08:12,547 --> 00:08:15,626
of ancient Egyptian hieroglyphs.

140
00:08:15,626 --> 00:08:22,872
And it's pretty exciting if you're
into historical linguistics.

141
00:08:22,872 --> 00:08:27,742
And it allows basically to translate
those to the Demotic script and

142
00:08:27,742 --> 00:08:31,081
the ancient Greek also,
which we still know.

143
00:08:31,081 --> 00:08:35,490
And so we can gain some intuition about
what's going on in the other two.

144
00:08:35,490 --> 00:08:37,743
Now, in the next couple of slides,

145
00:08:37,743 --> 00:08:42,330
I will basically try to bring across
to you that traditional statistical

146
00:08:42,330 --> 00:08:46,956
machine translation systems are very,
very complex beasts.

147
00:08:46,956 --> 00:08:50,140
And it wouldn't have been impossible for
me to say at the end of the lecture,

148
00:08:50,140 --> 00:08:52,770
all right, now you could implement
this whole thing yourself,

149
00:08:52,770 --> 00:08:57,380
after just one lecture, going over MT cuz
there are a lot of different moving parts.

150
00:08:57,380 --> 00:08:59,910
So let's walk through this.

151
00:08:59,910 --> 00:09:04,390
You won't have to actually implement
traditional statistical MT system in

152
00:09:04,390 --> 00:09:05,570
this class.

153
00:09:05,570 --> 00:09:07,978
But I want you to appreciate
a little bit the history.

154
00:09:07,978 --> 00:09:13,495
And why deep learning is so impactful and
amazing for machine translation.

155
00:09:13,495 --> 00:09:18,210
Cuz it's replacing a lot of different
submodules in these very complex models.

156
00:09:20,570 --> 00:09:24,060
And sometimes it uses still
ideas from this, but not very.

157
00:09:24,060 --> 00:09:27,974
Most of them we don't need any more for
neural machine translation systems.

158
00:09:27,974 --> 00:09:30,342
All right, so let's set the stage.

159
00:09:30,342 --> 00:09:32,810
We have generally a source language.

160
00:09:32,810 --> 00:09:35,340
Let's call that f, such as French.

161
00:09:35,340 --> 00:09:38,599
And we have a target language, e,
in our case, let's say it's English.

162
00:09:38,599 --> 00:09:44,031
So we wanna translate from the source
French to the target language of English.

163
00:09:44,031 --> 00:09:49,205
And we'll usually describe
this here with a simple Bayes

164
00:09:49,205 --> 00:09:56,070
rule where we basically try to find the,
Target sentence,

165
00:09:56,070 --> 00:10:00,720
usually e here we assume is the whole
sentence in the target language.

166
00:10:00,720 --> 00:10:04,910
That gives us the largest conditional
probability conditioned on f.

167
00:10:04,910 --> 00:10:07,343
So this is an abstract formulation.

168
00:10:07,343 --> 00:10:10,496
We'll try to fill in how to actually
compute these probabilities

169
00:10:10,496 --> 00:10:13,960
in traditional and then later in
neural machine translation systems.

170
00:10:13,960 --> 00:10:15,510
So now we can use Bayes rule.

171
00:10:15,510 --> 00:10:19,320
Posterior equals its prior times
likelihood divided by marginal evidence.

172
00:10:19,320 --> 00:10:21,740
Marginal evidence here would just be for
the source language.

173
00:10:21,740 --> 00:10:22,371
So that doesn't change.

174
00:10:22,371 --> 00:10:25,340
So we can drop that,
argmax would not change from that.

175
00:10:25,340 --> 00:10:31,470
So basically, we'll try to
compute these two factors here.

176
00:10:31,470 --> 00:10:36,210
The probability of the French
sentence given, or

177
00:10:36,210 --> 00:10:42,940
the source language given the target,
times the probability of just the target.

178
00:10:44,760 --> 00:10:47,500
And now, we'll basically call
these two elements here.

179
00:10:47,500 --> 00:10:49,410
One is our translation model.

180
00:10:49,410 --> 00:10:51,140
And the other one is our language model.

181
00:10:51,140 --> 00:10:54,443
Remember language modeling where we
tried to get the probability of a longer

182
00:10:54,443 --> 00:10:54,967
sequence.

183
00:10:54,967 --> 00:10:58,360
This is a great use case for it.

184
00:10:58,360 --> 00:11:02,075
Basically, you can think of this
as you get some French sentence.

185
00:11:02,075 --> 00:11:04,689
Your translation model will try to find.

186
00:11:04,689 --> 00:11:06,830
Maybe this phrase,
I can translate into that.

187
00:11:06,830 --> 00:11:09,220
And this phrase,
I can translate into this.

188
00:11:09,220 --> 00:11:11,550
And then you have a bunch
of pieces of English.

189
00:11:11,550 --> 00:11:16,620
And then your language model will
essentially in the decoder be combined

190
00:11:16,620 --> 00:11:21,312
to try to get a single,
smooth sentence in the target language.

191
00:11:21,312 --> 00:11:26,362
So it'll help us to take all these pieces
that we have from the translation model.

192
00:11:26,362 --> 00:11:30,112
And make it into one sentence that
actually sounds reasonable and flows and

193
00:11:30,112 --> 00:11:31,830
is grammatical and all that.

194
00:11:31,830 --> 00:11:36,150
So the language model helps us to
weight grammatical sentences better.

195
00:11:36,150 --> 00:11:40,767
So, for instance, I go home will
sound better than I go house, right?

196
00:11:40,767 --> 00:11:44,965
Because I go home will have a more
likely higher probability, so

197
00:11:44,965 --> 00:11:47,930
more likely English
sentence to be uttered.

198
00:11:47,930 --> 00:11:51,711
Now, how do we actually train
all these different pieces?

199
00:11:51,711 --> 00:11:53,170
And how would you go about doing this?

200
00:11:53,170 --> 00:11:56,820
Well, if you wanted to translate,
do this translation model here.

201
00:11:56,820 --> 00:12:01,067
Then the first thing you'd have to do
is you'd find so called alignments.

202
00:12:01,067 --> 00:12:05,249
Which is basically, the goal of the
alignment step is to know which word or

203
00:12:05,249 --> 00:12:08,959
phrase in the source language would
translate to the other word or

204
00:12:08,959 --> 00:12:10,790
phrase in the target language.

205
00:12:10,790 --> 00:12:13,660
And that sub problem already.

206
00:12:13,660 --> 00:12:18,030
And now, again,
we have these three different systems.

207
00:12:18,030 --> 00:12:21,420
And now we're zooming in to
the step one of that system.

208
00:12:21,420 --> 00:12:27,093
Now that one is already hard
because alignment is non-trivial.

209
00:12:27,093 --> 00:12:31,908
These are actually some cool examples from
previous incarnation from Chris's class,

210
00:12:31,908 --> 00:12:33,871
224, and from previous years.

211
00:12:33,871 --> 00:12:37,020
Here are some examples of why
alignment is already hard.

212
00:12:37,020 --> 00:12:43,010
And this is for a language pair
that is actually quite similar.

213
00:12:43,010 --> 00:12:47,130
English and French share a lot
of common history, and so on,

214
00:12:47,130 --> 00:12:49,240
and they're more similar.

215
00:12:49,240 --> 00:12:54,290
But even if we have these two sentences
here, like Japan shaken by two new quakes.

216
00:12:54,290 --> 00:12:58,990
Or Le Japon secoue par
deux nouveaux seismes.

217
00:12:58,990 --> 00:13:03,730
Then we'll basically have
here a spurious word.

218
00:13:03,730 --> 00:13:06,850
So Le was actually not
translated to anything.

219
00:13:06,850 --> 00:13:10,110
And we would skip it in our alignment.

220
00:13:10,110 --> 00:13:13,750
So you see here this alignment matrix.

221
00:13:13,750 --> 00:13:16,400
And you'll notice that Le
just wasn't translated.

222
00:13:16,400 --> 00:13:23,195
We don't say the Japan, or
a Japan, or something like that.

223
00:13:23,195 --> 00:13:24,740
So it gets trickier, though.

224
00:13:24,740 --> 00:13:25,880
Cuz there are also so

225
00:13:25,880 --> 00:13:29,190
called zero fertility words
that are not translated at all.

226
00:13:29,190 --> 00:13:31,317
So we start in a source and
we just drop them.

227
00:13:31,317 --> 00:13:36,977
And, for some reason, the translators,
or for grammatical reasons and

228
00:13:36,977 --> 00:13:42,647
so on, they don't actually have any
equivalent in the target language.

229
00:13:42,647 --> 00:13:46,160
And to make it even more complex,
we can also have one-to-many alignments.

230
00:13:46,160 --> 00:13:51,630
So implemented in English is actually
mis en application in French.

231
00:13:51,630 --> 00:13:57,000
So made into an application
of sorts is just the word and

232
00:13:57,000 --> 00:13:58,122
the verb implemented here.

233
00:13:58,122 --> 00:13:59,997
So then we'll have to try to find.

234
00:13:59,997 --> 00:14:02,786
And now, as you try to think through
algorithms that might do this

235
00:14:02,786 --> 00:14:03,656
alignment for you.

236
00:14:03,656 --> 00:14:08,450
You'll have to think, so this word could
go to either this one word or no word.

237
00:14:08,450 --> 00:14:09,551
Or these three words together.

238
00:14:09,551 --> 00:14:11,210
Or maybe these two words together.

239
00:14:11,210 --> 00:14:14,980
And you can see how that would create, if
you tried to go through all the statistics

240
00:14:14,980 --> 00:14:18,610
and collect all of these probabilities,
of which phrase would go to what phrase.

241
00:14:18,610 --> 00:14:22,667
It'll get pretty hard to
actually combine them all.

242
00:14:22,667 --> 00:14:27,090
And language is just incredible and
very complex.

243
00:14:27,090 --> 00:14:28,972
And you also have many-to-one alignments.

244
00:14:28,972 --> 00:14:35,618
So aboriginal people are just
autochtones in French.

245
00:14:35,618 --> 00:14:38,403
So similar actually in German, [FOREIGN].

246
00:14:38,403 --> 00:14:40,260
So you'd have two words in German.

247
00:14:40,260 --> 00:14:45,218
And so, you have many-to-one
alignments making the combinatorial

248
00:14:45,218 --> 00:14:49,341
explosion even harder if you
try to find good alignments.

249
00:14:49,341 --> 00:14:52,039
And lastly,
you'll also have many-to-many alignments.

250
00:14:52,039 --> 00:14:56,114
You have certain phrases
like don't have any money.

251
00:14:56,114 --> 00:14:59,340
This just goes to sont demunis in French.

252
00:15:00,520 --> 00:15:04,838
And so it's a very, very complex
problem that has combinatorial

253
00:15:04,838 --> 00:15:08,853
explosion of all potential
combinations and it's tricky.

254
00:15:08,853 --> 00:15:14,315
All right, so now, really,
if you were to take a traditional class,

255
00:15:14,315 --> 00:15:19,407
you could have several lectures,
or at least an entire lecture,

256
00:15:19,407 --> 00:15:26,100
just on the various ways you could
implement cleverly an alignment model.

257
00:15:26,100 --> 00:15:30,010
And sometimes,
people use just single words.

258
00:15:30,010 --> 00:15:33,327
And other times, they actually use
parses like the one you're now familiar,

259
00:15:33,327 --> 00:15:34,188
syntactic parses.

260
00:15:34,188 --> 00:15:37,691
And try to find which,
no, not just words, but

261
00:15:37,691 --> 00:15:41,640
phrases from a parse would
map to the other language.

262
00:15:43,450 --> 00:15:45,720
And then, of course, it's not just that.

263
00:15:45,720 --> 00:15:50,250
And not usually are sentences and
languages nicely aligned, but

264
00:15:50,250 --> 00:15:53,660
you can also have complete reorderings.

265
00:15:53,660 --> 00:15:58,700
So German sometimes, for sub Clauses
actually has the verb at the end,

266
00:15:58,700 --> 00:16:03,390
so you flip a lot of the words, and you
can't just have this vocality assumption

267
00:16:03,390 --> 00:16:07,590
that words rough in this area will
translate to roughly a similar area,

268
00:16:07,590 --> 00:16:10,890
in terms of the sequence of
words in the other language.

269
00:16:13,717 --> 00:16:17,567
So yeah, ja nicht here,
ja is technically just yes in German,

270
00:16:17,567 --> 00:16:19,730
also not translated at all.

271
00:16:19,730 --> 00:16:22,870
And then actually going over there and
going, moving also.

272
00:16:24,320 --> 00:16:28,480
All right, now let's say we have
all these potential alignments, and

273
00:16:28,480 --> 00:16:31,690
now as we start from the source
language we say, all right.

274
00:16:31,690 --> 00:16:37,040
Let's say the source here is this German
sentence, geht ja nicht nach hause.

275
00:16:37,040 --> 00:16:42,880
Now could be translated
into many different words.

276
00:16:42,880 --> 00:16:48,446
So German it's technically just the he
of he, she, it, as the es in German.

277
00:16:48,446 --> 00:16:53,400
But sometimes English as
you do your alignment

278
00:16:53,400 --> 00:16:57,950
when not unreasonable one is just it or
comma he or

279
00:16:57,950 --> 00:17:01,422
he will be, cuz those were dropped
before in the alignment and so on.

280
00:17:01,422 --> 00:17:05,609
So you now have lots of candidates for
each possible word and for

281
00:17:05,609 --> 00:17:10,419
each possible phrase that you
might want to combine now in

282
00:17:10,420 --> 00:17:16,310
some principled way to
the final target translation.

283
00:17:16,310 --> 00:17:20,140
So you have again here a combinatorial
explosion of lots of potential ways you

284
00:17:20,140 --> 00:17:24,230
could translate each of the words or
phrases of various lengths.

285
00:17:24,230 --> 00:17:27,710
And so basically what that means is
you'll have a very hard search problem

286
00:17:29,180 --> 00:17:33,010
that also includes having to
have a good language model.

287
00:17:33,010 --> 00:17:37,880
So that as you put all these pieces
together, you essentially try to keep

288
00:17:37,880 --> 00:17:42,060
saying or combining phrases that
are grammatically plausible or

289
00:17:42,060 --> 00:17:44,010
sound reasonable to native speakers.

290
00:17:45,710 --> 00:17:48,707
And this often ends up being
so-called beam search,

291
00:17:48,707 --> 00:17:52,969
where you try to keep around a couple of
candidates as you go from left to right

292
00:17:52,969 --> 00:17:56,253
and you try to put all of these
different pieces together.

293
00:17:56,253 --> 00:17:59,685
Now again, this is totally not
doing traditional MT justice.

294
00:17:59,685 --> 00:18:04,085
Right, we just went in five minutes over
what could have been an entire lecture on

295
00:18:04,085 --> 00:18:07,725
statistical machine translation, or
maybe even many multiple lectures.

296
00:18:07,725 --> 00:18:11,110
So there are lots of important
details we skipped over.

297
00:18:11,110 --> 00:18:14,060
But the main gist here
is that there's a lot of

298
00:18:14,060 --> 00:18:18,550
human feature engineering that's required
and involved in all of these different

299
00:18:18,550 --> 00:18:21,550
pieces that used to require building
a machine translation system.

300
00:18:21,550 --> 00:18:26,020
And it also meant that there were whole
companies that you could form just for

301
00:18:26,020 --> 00:18:29,720
machine translation because nobody
could go through all that work and

302
00:18:29,720 --> 00:18:31,510
really build out a good system.

303
00:18:31,510 --> 00:18:36,233
Whereas now you have companies that have
worked for decades in this and they start

304
00:18:36,233 --> 00:18:40,900
using an open-source machine translation
system that anybody can download.

305
00:18:40,900 --> 00:18:43,898
And now a normal student, a PhD
student can spend a couple months and

306
00:18:43,898 --> 00:18:45,808
then he has like one of
the best MT systems.

307
00:18:45,808 --> 00:18:50,035
Which just completely would have been
completely impossible in their large

308
00:18:50,035 --> 00:18:54,214
groups that all work together in very
large systems before, in academia.

309
00:18:54,214 --> 00:18:57,120
So one of the main problems
of this kind of approach,

310
00:18:57,120 --> 00:19:00,291
is actually that not only is
it a very complex system, but

311
00:19:00,291 --> 00:19:04,348
it's also a system of independently
trained machine learning models.

312
00:19:04,348 --> 00:19:09,400
And, if there's one thing that I think
that I like most, when property of deep

313
00:19:09,400 --> 00:19:14,540
learning models, not just for MT, but
in all of NLP and maybe in all of AI.

314
00:19:14,540 --> 00:19:18,420
Is that we're usually in deep learning
try to have end to end trainable models

315
00:19:18,420 --> 00:19:22,260
where you have your final objective
function that you care about and

316
00:19:22,260 --> 00:19:24,390
everything is learned
jointly in one model.

317
00:19:25,420 --> 00:19:27,850
And this MT system is kind
of the opposite of that.

318
00:19:27,850 --> 00:19:30,050
You have an alignment model
you optimize for that, and

319
00:19:30,050 --> 00:19:35,630
then you have a reordering model maybe,
and then you have the language model.

320
00:19:35,630 --> 00:19:41,152
And they're all separate systems and you
couldn't jointly train all of it together.

321
00:19:41,152 --> 00:19:46,408
So that's kind of the very quick summary
for traditional machine transaction.

322
00:19:46,408 --> 00:19:47,625
Any high level questions
around traditional MT?

323
00:19:49,976 --> 00:19:55,267
All right, so now deep learning to

324
00:19:55,267 --> 00:20:00,379
the rescue, maybe, probably.

325
00:20:00,379 --> 00:20:07,710
So let's go through a sequence of
models and see if they would suffice.

326
00:20:07,710 --> 00:20:12,628
So the simplest one that we could
possibly do is kind of an encoder and

327
00:20:12,628 --> 00:20:16,220
decoder model that looks like this.

328
00:20:16,220 --> 00:20:20,200
Where we literally just have
a single recurrent neural network,

329
00:20:20,200 --> 00:20:22,910
where we have our word vectors so
let's say here

330
00:20:22,910 --> 00:20:28,390
we translate from German to English
Echt Kiste is awesome sauce in English.

331
00:20:28,390 --> 00:20:32,690
And we now have our word vectors
here we learned them in German, and

332
00:20:32,690 --> 00:20:35,160
we have our soft max classifier here.

333
00:20:35,160 --> 00:20:38,820
And we just have a single recurrent neural
network and once it sees the end of German

334
00:20:38,820 --> 00:20:44,681
sentence and there's no input left we'll
just try to output the translation.

335
00:20:46,923 --> 00:20:50,290
Not totally unreasonable,
it's an end-to-end trainable model.

336
00:20:50,290 --> 00:20:53,626
We'll have our standard cross entry
pair here that tries to just predict

337
00:20:53,626 --> 00:20:54,342
the next word.

338
00:20:54,342 --> 00:20:59,336
But the next word actually has
to be in a different language.

339
00:20:59,336 --> 00:21:02,898
Now, basically this last vector here,
if this was our main model,

340
00:21:02,898 --> 00:21:06,410
this last vector would have to
capture the entirety of the phrase.

341
00:21:06,410 --> 00:21:10,511
And sadly, I've already told you
that usually five or six words or so

342
00:21:10,511 --> 00:21:13,552
can be captured and
after that, we don't really,

343
00:21:13,552 --> 00:21:17,320
we can't memorize the entire
context of the sentence before.

344
00:21:17,320 --> 00:21:23,820
So this might work for like,
very short sentenced but maybe not.

345
00:21:23,820 --> 00:21:27,720
But let's define what this model
would be in its most basic form,

346
00:21:27,720 --> 00:21:30,900
cuz we'll work on top of this afterwards.

347
00:21:30,900 --> 00:21:34,480
So we have here our standard recurrent
neural network from the last lecture.

348
00:21:34,480 --> 00:21:39,000
Where we have our next hidden state,
it's just basically a linear

349
00:21:40,080 --> 00:21:43,330
network here followed by
non-element wise linearities.

350
00:21:43,330 --> 00:21:46,970
And we sum here the matrix
vector product with the vector,

351
00:21:46,970 --> 00:21:49,710
the previous hidden state in
our current word vector xt.

352
00:21:49,710 --> 00:21:55,470
And that's our encoder and
then in our decoder in the simplest form,

353
00:21:55,470 --> 00:21:58,130
again not the final model,
in the simplest form we could just

354
00:21:58,130 --> 00:22:01,550
drop this cuz the decoder doesn't
have an input at that time.

355
00:22:01,550 --> 00:22:05,320
Right, it's just we wanna now
translate and just generate an output.

356
00:22:05,320 --> 00:22:09,400
So during the decoder we drop
this matrix vector product and

357
00:22:09,400 --> 00:22:10,780
we just go each time step.

358
00:22:10,780 --> 00:22:15,810
It's just basically moving along based
on the previous hidden time step.

359
00:22:15,810 --> 00:22:20,470
And we'll have our final softmax output
here at each time step of the decoder.

360
00:22:22,750 --> 00:22:26,780
Now I also introduced this phi notation
here, and basically whenever you have,

361
00:22:26,780 --> 00:22:29,280
we'll see this only in
the next couple of slides.

362
00:22:29,280 --> 00:22:32,180
But whenever I write phi of two vectors,

363
00:22:32,180 --> 00:22:37,420
that means we'll have two separate W
matrices for each of these vectors.

364
00:22:37,420 --> 00:22:42,210
This is the little, shorter notation, and
then the default here would be well, just

365
00:22:42,210 --> 00:22:46,350
like I said, minimize the cross entropy
error for all the target words conditioned

366
00:22:46,350 --> 00:22:49,740
on all the source words that we hoped
would be captured in that hidden state.

367
00:22:51,130 --> 00:22:57,140
All right, any questions, concerns,
thoughts about how this model would do?

368
00:23:14,673 --> 00:23:19,250
So, the comment or question is that
neither are the traditional model or

369
00:23:19,250 --> 00:23:21,940
this model account for grammar.

370
00:23:21,940 --> 00:23:23,048
And in some ways, that's not true.

371
00:23:23,048 --> 00:23:28,035
So there are actually a lot of traditional
models that work on top of syntactic

372
00:23:28,035 --> 00:23:30,120
grammatical tree structures.

373
00:23:30,120 --> 00:23:34,373
And they do this alignment based
on the syntactic structure of

374
00:23:34,373 --> 00:23:37,290
prefer potentially the alignment step.

375
00:23:37,290 --> 00:23:39,400
But also for the generation and
the encoding step and

376
00:23:39,400 --> 00:23:40,128
all these different steps.

377
00:23:40,128 --> 00:23:45,860
So there are several ways you can infuse
grammar and chromatical sort of priors

378
00:23:45,860 --> 00:23:52,010
into neuro machine translation systems or
so syntactic machine translation systems.

379
00:23:52,010 --> 00:23:54,480
It turns out it's questionable
if that actually helps.

380
00:23:54,480 --> 00:23:58,440
In many cases for machine translation,
you have such a broad range of sentences.

381
00:23:58,440 --> 00:24:01,575
You actually might have un-grammatical
sentences sometimes, and

382
00:24:01,575 --> 00:24:03,870
you still want them to be translated.

383
00:24:03,870 --> 00:24:05,390
You have very short,

384
00:24:05,390 --> 00:24:08,505
complex ambiguous kinds of
sentences like headlines and so on.

385
00:24:08,505 --> 00:24:12,452
So it's tricky, the jury was sort of out.

386
00:24:12,452 --> 00:24:16,068
And some tactic models were battling
it out with non-tactic models until

387
00:24:16,068 --> 00:24:17,825
neural machine translation came.

388
00:24:17,825 --> 00:24:21,019
And now, it's not as important
of a question anymore.

389
00:24:21,019 --> 00:24:26,029
Now, for neural systems, we would assume
and hope that our hidden state actually

390
00:24:26,029 --> 00:24:31,340
captures some grammatical structures and
some grammatical intuitions that we have.

391
00:24:31,340 --> 00:24:34,880
But we don't explicitly give
that to the algorithm anymore.

392
00:24:34,880 --> 00:24:40,503
Which some people who are very good
at giving those kinds of features,

393
00:24:40,503 --> 00:24:43,473
your algorithms might think is sad.

394
00:24:43,473 --> 00:24:46,040
But at the same time,
it's good if we don't have to, right?

395
00:24:46,040 --> 00:24:49,880
It's less work for us, putting more
artificial back into artificial

396
00:24:49,880 --> 00:24:55,400
intelligence, less human
intelligence on designing grammars.

397
00:24:55,400 --> 00:24:57,511
Anyways, so any other questions?

398
00:24:57,511 --> 00:24:58,401
Yeah.

399
00:25:08,690 --> 00:25:12,579
Good question, so sometimes, the number of
input words is different to the numbers

400
00:25:12,579 --> 00:25:15,000
of output words, and that's very true.

401
00:25:15,000 --> 00:25:19,364
So one modification we would have to
make to this kind of model for sure,

402
00:25:19,364 --> 00:25:27,090
is actually say, have the last output word
here, BA, stop out putting up words work.

403
00:25:27,090 --> 00:25:29,860
Like a special token that says, I'm done.

404
00:25:29,860 --> 00:25:35,250
And one, you add that to your softmax
classifier sort of the last row.

405
00:25:35,250 --> 00:25:38,480
And then you hope that when it
predicts that token, it just stops.

406
00:25:40,000 --> 00:25:45,226
And that is good enough and
not uncommon actually for

407
00:25:45,226 --> 00:25:49,406
all these neural machine translations.

408
00:25:49,406 --> 00:25:52,592
The superscript S is just again,

409
00:25:52,592 --> 00:25:57,950
to distinguish the different
W matrices that we have for

410
00:25:57,950 --> 00:26:04,222
hidden connections, visible or
hidden inputs, and softmax W.

411
00:26:04,222 --> 00:26:07,206
All right, now sadly,
while neural MT is pretty cool, and

412
00:26:07,206 --> 00:26:11,400
it is simpler than traditional systems,
it's not quite that simple.

413
00:26:11,400 --> 00:26:14,340
So we'll have to be a little more clever.

414
00:26:14,340 --> 00:26:19,640
And so let's go through a series of
extensions to this model where in the end,

415
00:26:19,640 --> 00:26:23,830
we'll have a very big
powerful LSTM type model.

416
00:26:23,830 --> 00:26:27,211
So step one, is we'll actually have
different recurrent neural network weights

417
00:26:27,211 --> 00:26:28,396
for encoding and decoding.

418
00:26:28,396 --> 00:26:33,265
So instead of having the same W here,
we actually should have a different set

419
00:26:33,265 --> 00:26:37,320
of parameters, a different W for
the decoding step.

420
00:26:37,320 --> 00:26:40,533
That's still relatively similar.

421
00:26:40,533 --> 00:26:45,167
All right, so again, remember this
notation here of fi where every

422
00:26:45,167 --> 00:26:48,356
input has its own matrix
W associated with it.

423
00:26:48,356 --> 00:26:53,388
The second modification is
that the previous hidden state

424
00:26:53,388 --> 00:26:59,149
is kind of the standard that you
have as input for during decoding.

425
00:26:59,149 --> 00:27:02,298
But instead of just having
the previous hidden state,

426
00:27:02,298 --> 00:27:06,880
we'll actually also add the last
hidden vector of the encoding.

427
00:27:06,880 --> 00:27:10,360
So we call this c here,
but it's essentially ht.

428
00:27:10,360 --> 00:27:16,127
So at this input here, we don't just
have the previous hidden state,

429
00:27:16,127 --> 00:27:21,428
but we always take the last hidden
state from the encoding step.

430
00:27:21,428 --> 00:27:25,158
And we have, again,
a separate matrix for that.

431
00:27:25,158 --> 00:27:28,620
And then on top of that, we will also add,
and that's actually, if you think about

432
00:27:28,620 --> 00:27:32,000
it, it's a lot of parameters, we'll add
the previous predicted output word.

433
00:27:33,190 --> 00:27:35,280
So as we translate,

434
00:27:35,280 --> 00:27:40,670
we have three inputs for each hidden
state during the decoding step.

435
00:27:40,670 --> 00:27:43,453
We'll have the previous hidden state
as a standard recurrent neural network.

436
00:27:43,453 --> 00:27:45,800
We have the last hidden
state of the encoder.

437
00:27:46,800 --> 00:27:50,660
And we have the actual output word
we predicted just before that.

438
00:27:50,660 --> 00:27:55,413
And this will essentially help the model
to know that it just output a word, and

439
00:27:55,413 --> 00:27:58,596
it'll prevent it from
outputting that word again.

440
00:27:58,596 --> 00:28:02,071
Cuz it'll learn to
transform the hidden state,

441
00:28:02,071 --> 00:28:05,809
based on having just upload
a specific word before.

442
00:28:05,809 --> 00:28:06,495
Yeah?

443
00:28:14,213 --> 00:28:15,637
That's right, that's right, yeah.

444
00:28:15,637 --> 00:28:19,543
So whenever you have fi of xyz here,

445
00:28:19,543 --> 00:28:23,980
it'll just f of w times
x + u of y + v of z.

446
00:28:25,980 --> 00:28:28,730
So you just,
I don't wanna define all the matrices.

447
00:28:39,760 --> 00:28:40,760
That's a great question.

448
00:28:40,760 --> 00:28:45,344
So why do we need to make y,
t minus one a parameter,

449
00:28:45,344 --> 00:28:51,331
if we actually had computed yt
minus one from ht minus one, right?

450
00:28:51,331 --> 00:28:56,363
So two answers, one, it will allow
us to have the softmax weights

451
00:28:56,363 --> 00:29:02,250
also modify a little bit how that
hidden state behaves at test time.

452
00:29:02,250 --> 00:29:05,295
And two,
we actually can choose usually yt, and

453
00:29:05,295 --> 00:29:07,905
there are different ways you can do this.

454
00:29:07,905 --> 00:29:13,380
You could take the actual probability, the
multinomial distribution from the softmax.

455
00:29:13,380 --> 00:29:15,620
But here,
we'll actually make a hard choice, and

456
00:29:15,620 --> 00:29:19,120
we'll actually tell the model
we chose exactly this one.

457
00:29:19,120 --> 00:29:21,780
So instead of having the distribution,
we'll make a hard choice.

458
00:29:21,780 --> 00:29:25,108
And we say, this is the one word, the
highest probability that had the highest

459
00:29:25,108 --> 00:29:28,800
probability, we predicted that one,
and that's the one we give us input.

460
00:29:28,800 --> 00:29:30,150
So it turns out in practice,

461
00:29:30,150 --> 00:29:35,520
that helps to prevent the model
from repeating words many times.

462
00:29:35,520 --> 00:29:40,197
And again, it incorporates the softmax
weights in that computation indirectly.

463
00:29:40,197 --> 00:29:40,930
Yeah.

464
00:29:53,361 --> 00:29:55,210
That is a good catch.

465
00:29:55,210 --> 00:29:57,050
That is not how we define the model.

466
00:29:57,050 --> 00:29:58,601
Ignore those errors.

467
00:29:58,601 --> 00:29:59,730
Yeah, well done.

468
00:30:00,810 --> 00:30:05,410
In theory, again, so I didn't define
it but you can also, you can do

469
00:30:05,410 --> 00:30:09,360
the same thing with the softmax, and
this is what the picture actually shows.

470
00:30:09,360 --> 00:30:14,035
So instead of having a softmax of just W,

471
00:30:14,035 --> 00:30:17,480
ht for the probability of yt.

472
00:30:17,480 --> 00:30:22,410
You can also concatenate here your c,
and that's what the picture said.

473
00:30:22,410 --> 00:30:25,291
But I wanted to skip over the details so
you caught it, well done.

474
00:30:40,302 --> 00:30:41,555
So this model usually,

475
00:30:41,555 --> 00:30:44,820
so the question is, do we have
kind of a look ahead type thing?

476
00:30:44,820 --> 00:30:46,679
Or does the model output blanks?

477
00:30:46,679 --> 00:30:52,019
And the model basically has to
output the words in the right order.

478
00:30:52,019 --> 00:30:57,415
And it doesn't not have the ability
to do this whole reordering step or

479
00:30:57,415 --> 00:30:59,431
look ahead kind of thing.

480
00:30:59,431 --> 00:31:03,748
Or there's no sort of post processing
of reordering at the end, so

481
00:31:03,748 --> 00:31:08,028
this model isn't able to output
the verb at the right time stamp.

482
00:31:08,028 --> 00:31:11,672
It's over, okay, here we go.

483
00:31:11,672 --> 00:31:12,476
Now, of course,

484
00:31:12,476 --> 00:31:16,071
once it works well, everybody will try to
see if they can kind of improve it, and

485
00:31:16,071 --> 00:31:19,091
eventually you can do beam searches
too for these kinds of models.

486
00:31:19,091 --> 00:31:23,610
But surprisingly, in many cases, you
don't have to get a reasonable MT system.

487
00:31:28,489 --> 00:31:31,869
All right, now, I want you to
become more and more familiar,

488
00:31:31,869 --> 00:31:33,760
to be able to read the literature.

489
00:31:33,760 --> 00:31:36,290
So the same picture that we had here and

490
00:31:36,290 --> 00:31:40,090
the same equations we defined,
here's another way off looking at this.

491
00:31:42,390 --> 00:31:46,268
So with the exception that this
one doesn't have the c connection

492
00:31:46,268 --> 00:31:47,336
that you caught.

493
00:31:47,336 --> 00:31:48,963
So, Yeah, it's similar.

494
00:31:48,963 --> 00:31:53,031
It's the same exact model,
just a different way to look at it, and

495
00:31:53,031 --> 00:31:54,890
it's kind of good to see.

496
00:31:54,890 --> 00:31:56,960
Sometimes people explicitly write

497
00:31:58,200 --> 00:32:03,160
that you start out with a discreet
one of k and coding of the words.

498
00:32:03,160 --> 00:32:05,770
It's just like you want one-hot
vectors that we defined, and

499
00:32:05,770 --> 00:32:09,030
then you embed it into
continuous word vector space.

500
00:32:09,030 --> 00:32:13,714
You give those as input, you compute
your recurrent neural network, ht steps.

501
00:32:13,714 --> 00:32:17,762
And now,
you give those as input to the decoder.

502
00:32:17,762 --> 00:32:22,454
And that each time stamp of decoder, you
get the one word sample that you actually

503
00:32:22,454 --> 00:32:28,070
took as input, the previous hidden state
and to see vector, we defined before.

504
00:32:28,070 --> 00:32:30,860
So all these three already
are the inputs for

505
00:32:30,860 --> 00:32:33,733
each node in this
recurrent neural network.

506
00:32:33,733 --> 00:32:37,547
So just a different picture for

507
00:32:37,547 --> 00:32:41,800
the same model we just defined, so

508
00:32:41,800 --> 00:32:49,310
you learn picture in variances first,
model semantics.

509
00:32:49,310 --> 00:32:50,095
Now, it gets more powerful.

510
00:32:50,095 --> 00:32:53,594
It needs to get more powerful cuz
even with those two assumptions here,

511
00:32:53,594 --> 00:32:56,918
we have a very simple recurrent
neural network with just one layer,

512
00:32:56,918 --> 00:32:58,770
that's not going to cut it.

513
00:32:58,770 --> 00:33:03,490
So we'll use some of the extensions
we discussed in the last lecture,

514
00:33:03,490 --> 00:33:07,740
we'll actually have stacked
deep recurrent neural networks

515
00:33:07,740 --> 00:33:09,630
where we have multiple layers.

516
00:33:09,630 --> 00:33:15,140
And then we'll also have,
in some cases, this is not as common,

517
00:33:15,140 --> 00:33:19,652
but sometimes it's used,
we have a bidirectional encoder.

518
00:33:19,652 --> 00:33:23,920
Where you go from left to right,
and then we give both of,

519
00:33:23,920 --> 00:33:31,674
last hidden states of both directions
as input to every step of the decoder.

520
00:33:31,674 --> 00:33:34,181
And then this is kind
of almost an XOR here.

521
00:33:34,181 --> 00:33:38,981
If you don't do this, than another way
to improve your system slightly is by

522
00:33:38,981 --> 00:33:41,981
training the input
sequence in reverse order,

523
00:33:41,981 --> 00:33:45,510
because then you have a simpler
optimization problem.

524
00:33:45,510 --> 00:33:51,230
So especially for languages that align
reasonably well like English and French.

525
00:33:51,230 --> 00:33:55,485
You might instead of saying A,
B, C, the other word's A,

526
00:33:55,485 --> 00:34:00,264
the word B, or C goes to in the different
language the words X and Y.

527
00:34:00,264 --> 00:34:04,272
You'll say, C B A goes to X Y,
because as they align,

528
00:34:04,272 --> 00:34:08,563
A is more likely to translate to X,
and B is more like to Y.

529
00:34:08,563 --> 00:34:10,208
And as you have longer sequences,

530
00:34:10,208 --> 00:34:14,250
you basically bring the words that are
actually being translated closer together.

531
00:34:15,520 --> 00:34:20,560
And hence, you have less of a vanishing
gradient problems and so on, because where

532
00:34:20,560 --> 00:34:25,532
you want the work to be predicted, it's
closer to where it came in to the encoder.

533
00:34:25,532 --> 00:34:26,032
Yeah?

534
00:34:31,362 --> 00:34:36,055
That's right, but yeah,
it's still an average force.

535
00:34:44,907 --> 00:34:46,581
So how does reversing not mess it up?

536
00:34:46,581 --> 00:34:48,989
Cuz this sentence doesn't
make grammatical sense.

537
00:34:48,989 --> 00:34:54,599
So we never gave this model
an explicit grammar for

538
00:34:54,600 --> 00:34:56,330
the source language, or
the target language, right?

539
00:34:56,330 --> 00:35:01,549
It's essentially trying, in some really
deep, clever, continuous function,

540
00:35:01,549 --> 00:35:07,220
general function approximation kind of
way, just correlation, basically, right?

541
00:35:07,220 --> 00:35:12,211
And it doesn't have to know the grammar,
but as long as you're consistent and

542
00:35:12,211 --> 00:35:15,323
you just reverse every sequence,
the same way.

543
00:35:15,323 --> 00:35:17,361
It's still grammatical if you
read it from the other side.

544
00:35:17,361 --> 00:35:21,760
And the model reads it from
potentially both sides, and so on.

545
00:35:21,760 --> 00:35:24,750
So it doesn't really matter
to these learning models,

546
00:35:24,750 --> 00:35:29,580
as long as your transformation of the
input is consistent across training and

547
00:35:29,580 --> 00:35:30,390
testing times, and so on.

548
00:35:42,130 --> 00:35:44,986
So the question is,
he understands the argument, but

549
00:35:44,986 --> 00:35:47,330
it could still change the meaning.

550
00:35:47,330 --> 00:35:51,620
And it doesn't change the meaning if
you assume the model will always go

551
00:35:51,620 --> 00:35:53,060
from one direction to the other.

552
00:35:53,060 --> 00:35:55,390
If you start to sometimes do it and
sometimes not,

553
00:35:55,390 --> 00:35:57,250
then it will totally mess up the system.

554
00:35:57,250 --> 00:36:00,060
But as long as it's
a consistent transformation,

555
00:36:00,060 --> 00:36:02,420
it is still the same order and
so you're good.

556
00:36:08,889 --> 00:36:11,760
So why is reversing the order
a simpler optimization problem?

557
00:36:11,760 --> 00:36:14,100
Imagine, you had a very
long sequence here.

558
00:36:14,100 --> 00:36:18,800
And again, this is only the case
if the languages align well.

559
00:36:18,800 --> 00:36:21,410
As in usually,
the first capital words in one

560
00:36:21,410 --> 00:36:24,810
of the source language translated to first
capital words in the target language.

561
00:36:24,810 --> 00:36:29,892
Now, If you have a long sequence and
you try to translate

562
00:36:29,892 --> 00:36:35,856
it to another long sequence, and
say there are a lot of them here.

563
00:36:35,856 --> 00:36:41,149
Now, what that would mean is that this
word here is very far away from that word,

564
00:36:41,149 --> 00:36:45,410
cuz it has to go through
this entire transformation.

565
00:36:45,410 --> 00:36:47,960
And likewise,
these words are also very far away.

566
00:36:47,960 --> 00:36:54,019
So everything is far away from
everything in terms of the number

567
00:36:54,019 --> 00:37:01,320
of non-linear function applications
before you get to the actual output.

568
00:37:01,320 --> 00:37:06,295
Now, if you just reverse this one,
then this word, so

569
00:37:06,295 --> 00:37:09,768
let's call this a, b, c, d, e, f.

570
00:37:09,768 --> 00:37:14,684
Now, this is now f,

571
00:37:14,684 --> 00:37:19,030
e, d, c, b, a.

572
00:37:19,030 --> 00:37:21,405
Now, this word, it's here now.

573
00:37:21,405 --> 00:37:24,570
And now, this word translates
directly to that word, right?

574
00:37:24,570 --> 00:37:25,680
So in your decoder.

575
00:37:25,680 --> 00:37:29,090
So now, these two are very,
very close to one another.

576
00:37:29,090 --> 00:37:33,323
And so as you do back propagation and we
learn about the vanishing creating problem

577
00:37:33,323 --> 00:37:37,020
in the last lecture you have much
less of a vanishing creating problem.

578
00:37:37,020 --> 00:37:40,420
So at least in the beginning,
it'll be much better at translating those.

579
00:37:51,243 --> 00:37:56,270
So, how does this check work for
languages with different morphology?

580
00:37:56,270 --> 00:38:00,040
It doesn't actually matter, but
the sad truth is also that very few

581
00:38:00,040 --> 00:38:04,470
MT researchers work on languages
with super complex morphology.

582
00:38:04,470 --> 00:38:08,990
So like Finnish doesn't have
very large parallel corpora

583
00:38:08,990 --> 00:38:10,600
of tons of other languages.

584
00:38:10,600 --> 00:38:13,618
And so you don't sadly see
as many people work on that.

585
00:38:13,618 --> 00:38:14,657
German does work.

586
00:38:14,657 --> 00:38:17,380
And for German actually,
a lot of other tricks that we'll get to.

587
00:38:17,380 --> 00:38:22,860
And really these tricks are not as
important as the one as trick number six.

588
00:38:22,860 --> 00:38:24,539
But before that,
we'll have a research highlight.

589
00:38:24,539 --> 00:38:30,140
>> [LAUGH]
>> Give you a bit of a break, all right.

590
00:38:30,140 --> 00:38:33,876
Allen, take it away.

591
00:38:33,876 --> 00:38:34,744
>> This?
>> Yes.

592
00:38:34,744 --> 00:38:35,941
>> Okay.
Hi, everyone.

593
00:38:35,941 --> 00:38:36,751
My name is Allen.

594
00:38:36,751 --> 00:38:41,950
So I'm gonna talk about Building Towards
a Better Language Modeling.

595
00:38:41,950 --> 00:38:43,449
So as we've learned last week,

596
00:38:43,449 --> 00:38:46,288
language modeling is one of
the most canonical task in NLP.

597
00:38:46,288 --> 00:38:49,520
And there are three different ways
we can make it a little bit better.

598
00:38:49,520 --> 00:38:51,294
We can have better input representation.

599
00:38:51,294 --> 00:38:54,590
We can have better regularization or
preprocessing.

600
00:38:54,590 --> 00:38:57,760
And eventually,
we can have a better model.

601
00:38:57,760 --> 00:39:01,162
So for input, I know you guys
have all played with Glove, and

602
00:39:01,162 --> 00:39:03,530
that's a word level representation.

603
00:39:03,530 --> 00:39:05,070
And I heard morphemes.

604
00:39:05,070 --> 00:39:06,310
From you guys who are down there.

605
00:39:06,310 --> 00:39:09,790
So in fact,
you can code the word at a subword level.

606
00:39:09,790 --> 00:39:11,702
You can do morpheme encoding.

607
00:39:11,702 --> 00:39:13,111
You can do BPE.

608
00:39:13,111 --> 00:39:14,540
You can eventually do
character level embedding.

609
00:39:14,540 --> 00:39:18,110
What it does is that it drastically
reduce the size of your vocabulary,

610
00:39:18,110 --> 00:39:22,010
make the model prediction much easier.

611
00:39:22,010 --> 00:39:26,220
So as you can see, Tomas Mikolov in 2012,
and Yoon Kim in 2015,

612
00:39:26,220 --> 00:39:32,350
explored this route and got better results
compared to just plain word-based models.

613
00:39:33,880 --> 00:39:38,270
So another way to improve your model
is that one of the bigger problems for

614
00:39:38,270 --> 00:39:40,220
language modelling is over-fitting.

615
00:39:40,220 --> 00:39:44,050
And we know that we need to apply
regularization techniques when the model

616
00:39:44,050 --> 00:39:45,111
is over-fitting.

617
00:39:45,111 --> 00:39:46,752
So there are a bunch of them, but today,

618
00:39:46,752 --> 00:39:50,378
I'm gonna focus on preprocessing
because it's a little bit newer.

619
00:39:50,378 --> 00:39:52,840
What preprocessing does is
that we know that we're

620
00:39:54,950 --> 00:39:57,390
never gonna have unlimited training data.

621
00:39:57,390 --> 00:40:02,180
So in order to have our corpus look
more like the true distribution

622
00:40:02,180 --> 00:40:07,710
of the English language, what we can do is
quite similar to computer vision we can

623
00:40:07,710 --> 00:40:12,150
do this type of data augmentation
technique where we try to replace

624
00:40:12,150 --> 00:40:15,410
some words in our corpus
with some other words.

625
00:40:15,410 --> 00:40:16,180
So for example,

626
00:40:16,180 --> 00:40:19,740
your model during the first pass
you can see a word called New York,

627
00:40:19,740 --> 00:40:23,890
the next pass you can see New Zealand,
the next pass you can see New England.

628
00:40:23,890 --> 00:40:28,380
So by doing that, you're basically
generating this data by yourself and

629
00:40:28,380 --> 00:40:32,090
eventually you achieve
a smoothed out distribution.

630
00:40:32,090 --> 00:40:35,250
The reason this happens is
that more frequent word by

631
00:40:35,250 --> 00:40:37,300
replacing by dropping them.

632
00:40:37,300 --> 00:40:41,340
They appear less often and
rarer words by making them appear.

633
00:40:41,340 --> 00:40:43,010
They appear more often.

634
00:40:43,010 --> 00:40:47,810
So a smooth distribution allow us to
learn a better language model and

635
00:40:47,810 --> 00:40:51,880
the result is on the, I think is on
the right hand side of you guys.

636
00:40:51,880 --> 00:40:56,900
And the left hand side is what happen when
we apply better regularization techniques.

637
00:40:58,160 --> 00:41:03,630
So at last we can, wait,
that's it okay, awesome thank you guys.

638
00:41:09,101 --> 00:41:14,487
>> All right, now what you'll also see
in these tables is that the default for

639
00:41:14,487 --> 00:41:21,160
all these models is an LSTM and that's
exactly what we'll end up very soon with.

640
00:41:21,160 --> 00:41:24,800
Which is basically a better
type of recurrent unit.

641
00:41:25,850 --> 00:41:30,980
And so, we'll start with gated
recurrent units that were introduced

642
00:41:30,980 --> 00:41:33,460
by Cho just three years ago.

643
00:41:33,460 --> 00:41:37,430
And the main idea is that,
we wanna basically keep around

644
00:41:37,430 --> 00:41:40,470
memories that capture long
distance dependencies and

645
00:41:40,470 --> 00:41:44,110
you wanna have the model learn when and
how to do that.

646
00:41:44,110 --> 00:41:48,030
And with that,
you also allow your error messages to flow

647
00:41:48,030 --> 00:41:50,250
differently at different strengths,
depending on the input.

648
00:41:51,260 --> 00:41:52,900
So, how does this work?

649
00:41:52,900 --> 00:41:56,330
What is a GRU as our step to the LSDM?

650
00:41:56,330 --> 00:41:58,300
And sometimes you don't need
to go all the way to the LSDM.

651
00:41:58,300 --> 00:42:00,640
The GRU is a really good model by itself.

652
00:42:00,640 --> 00:42:03,030
In many cases already in its simpler.

653
00:42:03,030 --> 00:42:06,560
So let's start with our standard
recurrent neural network,

654
00:42:06,560 --> 00:42:11,780
which basically computes our hidden
layer at the next time step directly.

655
00:42:11,780 --> 00:42:16,990
So we just have again previous hidden
state recurring to our vector that's it.

656
00:42:16,990 --> 00:42:19,180
Now instead what we'll do for

657
00:42:19,180 --> 00:42:24,300
gated recurring units or GRUs,
is we'll compute to gates first.

658
00:42:24,300 --> 00:42:30,340
These gates are also just like ht,
continuous vectors of the same

659
00:42:30,340 --> 00:42:36,040
length as the hidden state, and
they are computed exactly the same way.

660
00:42:36,040 --> 00:42:40,930
And here, it's important to note that
the superscripts that's just basically

661
00:42:40,930 --> 00:42:44,210
are lined with the kind of
gate that you're computing.

662
00:42:44,210 --> 00:42:48,400
So we'll compute a so
called update gate and a reset gate.

663
00:42:49,830 --> 00:42:52,450
Now the inside here is
the exact same thing but

664
00:42:52,450 --> 00:42:55,780
is important to note that we
have here a sigmoid function.

665
00:42:55,780 --> 00:42:59,860
So we'll have elements of this vector
are exactly between zero and one.

666
00:42:59,860 --> 00:43:03,010
And we could interpret them as
probabilities if we want to.

667
00:43:04,610 --> 00:43:07,060
And it's also important to note that
the super scripts here are different.

668
00:43:07,060 --> 00:43:09,150
So the update gate of course,

669
00:43:09,150 --> 00:43:13,220
uses a different set of
weights to the reset gate.

670
00:43:13,220 --> 00:43:16,230
Now why are they called update and
reset gates, and how do we use them?

671
00:43:16,230 --> 00:43:18,630
It's relatively straight forward.

672
00:43:18,630 --> 00:43:25,250
We just introduced one new function
here just the element wise product.

673
00:43:25,250 --> 00:43:27,280
We've remember it from back propagation.

674
00:43:27,280 --> 00:43:29,020
We also call it the Hadamard
product sometimes.

675
00:43:29,020 --> 00:43:33,930
Where we just element wise multiply
this vector here from the reset

676
00:43:33,930 --> 00:43:39,056
gate with this,
which would be our new memory content.

677
00:43:39,056 --> 00:43:43,290
We call it ht,
this is our intermediate memory content,

678
00:43:43,290 --> 00:43:46,700
it has the standard tanh that
we also know as a [INAUDIBLE].

679
00:43:46,700 --> 00:43:51,250
This part here is exactly the same,
we just have to input our word vector and

680
00:43:51,250 --> 00:43:54,040
then transformed with a W.

681
00:43:54,040 --> 00:43:56,347
But what's going on in here?

682
00:43:56,347 --> 00:44:01,500
So intuitively right, this is just a long
vector of numbers between zero and one.

683
00:44:02,530 --> 00:44:07,920
Now intuitively,
if this reset gate at a certain unit,

684
00:44:07,920 --> 00:44:12,950
is around zero,
then we essentially ignore all the past.

685
00:44:12,950 --> 00:44:17,430
We ignore that entire computation
of the past, and we're just going

686
00:44:17,430 --> 00:44:22,540
to define that element where our zero,
with the current word vector.

687
00:44:22,540 --> 00:44:23,790
Now why would we want to do that?

688
00:44:23,790 --> 00:44:25,270
What's the intuition here?

689
00:44:25,270 --> 00:44:28,960
Let's take the task of sentiment analysis
cuz it's very simple and intuitive.

690
00:44:30,040 --> 00:44:35,750
If you were to say, you're talking
about a plot of a movie review.

691
00:44:35,750 --> 00:44:38,750
And you talk about the plot and
you know some girl falls in love for

692
00:44:38,750 --> 00:44:41,850
some guy who falls in love with her but
then they can't meet, blah, blah, blah.

693
00:44:41,850 --> 00:44:46,205
That's a long plot and in the end you say,
but the movie was really boring.

694
00:44:47,900 --> 00:44:51,060
Then really doesn't matter that
you keep around that whole plot.

695
00:44:51,060 --> 00:44:55,870
You wanna say boring as a really
negative strong word for sentiments, and

696
00:44:55,870 --> 00:45:02,322
you wanna basically be able to allow the
model to ignore the previous plot summary.

697
00:45:02,322 --> 00:45:06,930
Cuz for the task of sentiments
analysis it's irrelevant.

698
00:45:06,930 --> 00:45:10,090
Now this is essentially what
the reset gate will let you do, but

699
00:45:10,090 --> 00:45:11,700
of course not in this global fashion,

700
00:45:11,700 --> 00:45:17,290
where you update the entire hidden state,
but in a more subtle way, where you learn

701
00:45:17,290 --> 00:45:21,610
which of the units you actually will reset
and which ones you will keep around.

702
00:45:22,860 --> 00:45:25,020
So this will allow some
of the units to say,

703
00:45:25,020 --> 00:45:28,410
well maybe I want to be a plot unit and
I will keep around the plot.

704
00:45:28,410 --> 00:45:33,330
But other units learn, well if I see one
of the sentiment words, I will definitely

705
00:45:33,330 --> 00:45:37,892
set that reset gate to zero and I will
now make sure that I don't wash out,

706
00:45:37,892 --> 00:45:44,310
the content with previous stuff
by summing these two, right?

707
00:45:44,310 --> 00:45:47,070
You're sort of like, not quite
averaging but you're summing the two.

708
00:45:47,070 --> 00:45:50,380
So you wash out the content
from this word and

709
00:45:50,380 --> 00:45:54,410
instead it will set that to zero and take
only the content from that current word.

710
00:45:57,820 --> 00:46:03,592
Now the final memory it will compute,
we'll combine this with the update gate.

711
00:46:03,592 --> 00:46:06,860
And the update gate now,
there's something similar but

712
00:46:08,090 --> 00:46:13,060
basically allows us to keep around
only the past and not the future.

713
00:46:13,060 --> 00:46:14,400
Or not the current time steps.

714
00:46:14,400 --> 00:46:20,698
So intuitively here when you look at Z,
if Z is a vector of all ones,

715
00:46:20,698 --> 00:46:25,424
then what we would do is
essentially do ht = ht-1

716
00:46:25,424 --> 00:46:30,800
+ 1-1 is 0, so this term just falls away.

717
00:46:30,800 --> 00:46:37,170
Basically if zt was all ones we could
just copy over our previous time step.

718
00:46:37,170 --> 00:46:40,060
Super powerful,
if you copied over the previous time step

719
00:46:40,060 --> 00:46:42,530
you have no vanishing gradient problem,
right.

720
00:46:42,530 --> 00:46:45,100
Your vector just gets a bunch of ones.

721
00:46:45,100 --> 00:46:47,720
Nothing changes in your
gradient computation.

722
00:46:47,720 --> 00:46:52,590
So that's very powerful and intuitively
you can use that same sentiment example.

723
00:46:52,590 --> 00:46:55,410
But you say in the beginning man,
I love this movie so much,

724
00:46:55,410 --> 00:46:57,960
here's this beautiful love story.

725
00:46:57,960 --> 00:47:01,330
And now you go through the love story,
and really what's important for

726
00:47:01,330 --> 00:47:05,190
sentiment is not about the love story,
but it's about the person saying,

727
00:47:05,190 --> 00:47:06,630
I love this movie a lot.

728
00:47:06,630 --> 00:47:09,780
And you wanna make sure you
don't lose that information.

729
00:47:09,780 --> 00:47:11,580
And with the standard
recurring neural network,

730
00:47:11,580 --> 00:47:14,740
we update our hidden state,
every time, every word.

731
00:47:14,740 --> 00:47:18,350
No matter how unimportant a word is,
we're gonna sum up those two vectors,

732
00:47:18,350 --> 00:47:21,890
washing out the content as we
move further and further along.

733
00:47:21,890 --> 00:47:25,430
Here we can decide, and what's even
more amazing, you don't have to decide.

734
00:47:25,430 --> 00:47:28,870
You can say, this word is positive, so
I'm gonna set my reset gate manually.

735
00:47:28,870 --> 00:47:32,190
No, the model will learn when to reset and
when to update.

736
00:47:33,650 --> 00:47:38,786
So this is a very simple kind of
modification but extremely powerful.

737
00:47:41,920 --> 00:47:44,980
Now, we're gonna go through it and
explain it a couple more times.

738
00:47:44,980 --> 00:47:46,510
And we'll try to.

739
00:47:46,510 --> 00:47:49,320
Have an attempt here at
a clean illustration.

740
00:47:49,320 --> 00:47:52,930
Honestly, personally, I feel the equations
here are still straight forward, and

741
00:47:52,930 --> 00:47:55,900
very intuitive, that I don't know
if these illustrations always help,

742
00:47:55,900 --> 00:47:59,950
but some people like
them more than others.

743
00:47:59,950 --> 00:48:05,300
So intuitively here, you basically
see that only the final memory,

744
00:48:05,300 --> 00:48:08,880
that you computed is the one that's
actually used as input to the next step.

745
00:48:08,880 --> 00:48:14,360
So all of these are only
modifying through the final state.

746
00:48:14,360 --> 00:48:18,270
And now this one gets as input to
our reset gate or update gate,

747
00:48:18,270 --> 00:48:22,330
the intermediate state and
the final state of the memory.

748
00:48:22,330 --> 00:48:27,199
And so does our x vector the word vector
here also gets its input through the reset

749
00:48:27,199 --> 00:48:30,989
gate, the update gate, and
our intermediate memory state.

750
00:48:30,989 --> 00:48:35,674
And then, I tried to use this,
so the dotted line here,

751
00:48:35,674 --> 00:48:40,380
as basically gates that modify
how these two interact.

752
00:48:43,300 --> 00:48:48,297
All right, so I've said, I think,
most of these things already, but

753
00:48:48,297 --> 00:48:51,490
again, reset gate here is close to 0.

754
00:48:51,490 --> 00:48:53,757
We ignore our previous state.

755
00:48:53,757 --> 00:48:57,562
And that again, allows the model,
in general, to drop information that is

756
00:48:57,562 --> 00:49:01,460
irrelevant for the future
predictions that it wants to make.

757
00:49:01,460 --> 00:49:04,670
And if we update the gate z controls,

758
00:49:04,670 --> 00:49:07,730
how much of the past state should
matter at the current time stamp?

759
00:49:09,240 --> 00:49:12,040
And again, this is a huge improvement for
the vanishing gradient problem,

760
00:49:12,040 --> 00:49:15,850
which allows us to actually train these
models on nontrivial, long sequences.

761
00:49:19,524 --> 00:49:21,198
Any questions around the GRU?

762
00:49:21,198 --> 00:49:21,698
Yep?

763
00:49:25,309 --> 00:49:27,300
Does it matter if you reset first or
update first?

764
00:49:27,300 --> 00:49:31,830
Well, so you can't compute
h until you have h tilled.

765
00:49:31,830 --> 00:49:34,210
So the order of these two doesn't matter.

766
00:49:34,210 --> 00:49:39,050
You can compute that in peril, but
you first have to compute h tilled

767
00:49:39,050 --> 00:49:40,880
with the reset gate before
you can compute that one.

768
00:49:54,749 --> 00:49:57,679
So the question is,
does it matter to switch and

769
00:49:57,679 --> 00:50:01,727
use an equation like this first,
and then an equation like that?

770
00:50:01,727 --> 00:50:06,040
I guess it's just a different model.

771
00:50:06,040 --> 00:50:09,470
It's not one that I know
of people having tried.

772
00:50:10,790 --> 00:50:15,420
It's not super unreasonable,
I don't see a sort of reason why

773
00:50:15,420 --> 00:50:20,350
it would be illogical to ever to that,
but yeah, just not the GRU model.

774
00:50:22,560 --> 00:50:24,026
You will actually see,

775
00:50:24,026 --> 00:50:28,647
in [INAUDIBLE] she has a paper on a Search
Space Odyssey type paper where there

776
00:50:28,647 --> 00:50:33,070
are a thousand modifications you can
make to the next model, the LSTM.

777
00:50:33,070 --> 00:50:36,315
And people have tried a lot of them,
and it's not trivial.

778
00:50:36,315 --> 00:50:37,974
There are a lot of modifications.

779
00:50:37,974 --> 00:50:41,256
And a lot of times they
seem kind of intuitive, but

780
00:50:41,256 --> 00:50:46,660
don't actually change performance that
much across a bunch of different tasks.

781
00:50:46,660 --> 00:50:50,988
But sometimes, one modification improves
things a tiny bit on one of the tasks.

782
00:50:50,988 --> 00:50:54,798
It turns out the final model of GRU here
and the LSTM, are actually incredibly

783
00:50:54,798 --> 00:50:58,270
stable, they give good performance
across a lot of different tasks.

784
00:51:00,080 --> 00:51:04,110
But it can't ever hurt to, if you have
some intuition of why you want to have,

785
00:51:04,110 --> 00:51:06,300
make something different,
it can't hurt to try.

786
00:51:20,126 --> 00:51:24,005
So the question is,
is it important of how they're computed?

787
00:51:24,005 --> 00:51:26,878
I think there are some people who have
tried once to have a two layer neural

788
00:51:26,878 --> 00:51:28,125
network to compute.

789
00:51:28,125 --> 00:51:30,185
These a z and update, z and r.

790
00:51:30,185 --> 00:51:34,049
In general, it matters of course
a lot of how they're computed, but

791
00:51:34,049 --> 00:51:37,860
not in the sense that you have to
modify them manually or something.

792
00:51:37,860 --> 00:51:41,070
It just the model learns when to
update and when not to update.

793
00:52:01,876 --> 00:52:03,125
That's a good question.

794
00:52:03,125 --> 00:52:05,395
So what do I mean when I say unit.

795
00:52:05,395 --> 00:52:10,800
So in general, what you'll observe in
a slide that's coming up very soon is

796
00:52:10,800 --> 00:52:16,470
that we will kind of abstract away from
the details of what these equations are.

797
00:52:16,470 --> 00:52:22,243
And we're going to write that just ht

798
00:52:22,243 --> 00:52:27,280
equals GRU of xt and ht minus 1.

799
00:52:27,280 --> 00:52:32,530
And then we'll just say that GRU
abbreviation means all these other things,

800
00:52:32,530 --> 00:52:35,620
all these equations, and
we're going to abstract away from that.

801
00:52:35,620 --> 00:52:40,210
And that's something that you'll see even
more in subsequent lectures where you

802
00:52:40,210 --> 00:52:44,980
just say a whole recurrent
network with a five layer GRU and

803
00:52:44,980 --> 00:52:48,520
combine lots of different
ways is just one block.

804
00:52:48,520 --> 00:52:51,683
We often see this in computer vision too
where CNNs are now just like the CNN

805
00:52:51,683 --> 00:52:54,555
block, and you assume you've got
a feature vector out at the end.

806
00:52:54,555 --> 00:52:58,100
And people will start abstracting
away more and more from that.

807
00:52:58,100 --> 00:53:00,520
But yeah,
you'll always have to remember that, yes,

808
00:53:00,520 --> 00:53:04,230
there's a lot of complexity
inside that unit.

809
00:53:04,230 --> 00:53:09,240
Here's another attempt at an illustration
which I'm even less of a fan of,

810
00:53:09,240 --> 00:53:11,090
then the one I tried to come up with.

811
00:53:12,160 --> 00:53:17,020
Basically, how you have your z gate
that kind of can jump back and forth.

812
00:53:17,020 --> 00:53:19,650
But of course,
it's usually a continuous type thing.

813
00:53:19,650 --> 00:53:27,036
It's not a zero one type thing, so I'm not
a big fan of this kind of illustration.

814
00:53:27,036 --> 00:53:28,877
And so in terms of derivatives,

815
00:53:28,877 --> 00:53:33,250
we couldn't theory asks you to
derive all the details of the GRU.

816
00:53:33,250 --> 00:53:38,120
And the only change here is that we now
have the derivative of these element

817
00:53:38,120 --> 00:53:43,060
wise multiplications,
both of which I have parameters or inside.

818
00:53:43,060 --> 00:53:47,357
And we all should know what
derivative of this is, and

819
00:53:47,357 --> 00:53:51,190
the rest is again,
the same kind of chain rule.

820
00:53:51,190 --> 00:53:55,965
But again, now you're sort of realizing
why we wanna modularized this more and

821
00:53:55,965 --> 00:54:00,600
more, and abstract a way from actually
manually taking these instead having

822
00:54:00,600 --> 00:54:03,270
error messages and deltas sent around.

823
00:54:03,270 --> 00:54:03,770
Yeah?

824
00:54:08,872 --> 00:54:12,130
Explain why we have both update and reset.

825
00:54:12,130 --> 00:54:17,570
So basically, it helps the model
to have different mechanisms for

826
00:54:17,570 --> 00:54:22,650
when to memorize something and
keep it around, versus when to update it.

827
00:54:22,650 --> 00:54:27,660
You're right, in theory, you could try to
put both of those into one thing, right?

828
00:54:27,660 --> 00:54:35,921
In theory, you'd say, well,
if this was just my previous ht here,

829
00:54:35,921 --> 00:54:40,730
then this could say, well, I wanna keep
it around, or I wanna update it here.

830
00:54:40,730 --> 00:54:42,500
But now, this update here,

831
00:54:42,500 --> 00:54:46,610
if you just had an equation like this it
would be still be a sum of two things.

832
00:54:46,610 --> 00:54:52,030
So that means that xt here
does not have complete control

833
00:54:52,030 --> 00:54:57,040
over modifying the current
hidden state in its entirety.

834
00:54:57,040 --> 00:54:59,200
It would still be summed
up with something else,

835
00:54:59,200 --> 00:55:01,270
and that happens at
every single time stamp.

836
00:55:01,270 --> 00:55:04,165
So its only once you have
this reset gates are here.

837
00:55:04,165 --> 00:55:09,850
These reset gates are here,
that you would allow h

838
00:55:09,850 --> 00:55:14,020
to be completely dominated by the current
word vector, if the model so chooses.

839
00:55:21,850 --> 00:55:29,857
If the reset gates are all,
Okay, so if these are all ones,

840
00:55:29,857 --> 00:55:36,220
then you have here basically a standard
recurrent neural network type equation.

841
00:55:36,220 --> 00:55:39,220
And then if you just have zs, all 0s,

842
00:55:39,220 --> 00:55:41,780
then you take that exact equation and
you're right.

843
00:55:41,780 --> 00:55:43,610
Then you just have a standard RNN.

844
00:55:43,610 --> 00:55:46,120
It's also beautiful,
it's always nice to say my model

845
00:55:46,120 --> 00:55:48,903
Is a more general form of your model or-
>> [LAUGH]

846
00:55:48,903 --> 00:55:49,438
>> An opposite,

847
00:55:49,438 --> 00:55:51,139
you're model's a special case of my model.

848
00:55:51,139 --> 00:55:56,865
It was actually a couple years ago
that you could by and say that.

849
00:55:56,865 --> 00:55:59,670
>> [LAUGH]
>> It's good machine learning banter.

850
00:56:01,490 --> 00:56:02,590
So yeah, it's always good.

851
00:56:02,590 --> 00:56:08,860
And likewise, the inventor of this model
made exactly that statement about the GRU.

852
00:56:08,860 --> 00:56:12,040
Not knowing why anybody
had to publish a new paper

853
00:56:12,040 --> 00:56:17,300
about this instead of just referring to
this and the special cases of the LSTM.

854
00:56:17,300 --> 00:56:19,860
So if we have one more
question about the GRU, yeah?

855
00:56:19,860 --> 00:56:21,741
>> Is there a reason.

856
00:56:25,464 --> 00:56:26,960
>> Good question.

857
00:56:26,960 --> 00:56:27,622
Why tanh and sigmoid?

858
00:56:27,622 --> 00:56:32,353
So in theory, you could say the tan h
here could be a rectified linear unit or

859
00:56:32,353 --> 00:56:33,740
other kind of unit.

860
00:56:33,740 --> 00:56:39,459
In practice, you do want sigmoids here
because you have this plus 1 minus that.

861
00:56:39,459 --> 00:56:43,663
And so if they're all over the place then
everything will kind of be modified and

862
00:56:43,663 --> 00:56:47,802
it's less intuitive that you kind of have
a hard reset in sort of a hard sort of,

863
00:56:47,802 --> 00:56:49,710
yeah, hard reset or a hard update.

864
00:56:50,910 --> 00:56:53,121
And if this wasn't 10h and

865
00:56:53,121 --> 00:56:58,834
was rectified linear unit then these
two might be all over the place too and

866
00:56:58,834 --> 00:57:05,025
it might be kind of easy to potentially
have the sum also the not very synthecal.

867
00:57:05,025 --> 00:57:06,375
But at the same time,

868
00:57:06,375 --> 00:57:11,050
it's not unreasonable to try having
a rectified learning unit here.

869
00:57:11,050 --> 00:57:13,980
And maybe, if you combine it with
proper regularization and so on,

870
00:57:13,980 --> 00:57:17,645
you could get away with other kinds
of other kinds of linearities.

871
00:57:17,645 --> 00:57:20,093
That's unlike probabilistic
graphical models for

872
00:57:20,093 --> 00:57:21,766
certain things just make no sense.

873
00:57:21,766 --> 00:57:25,996
And you can't do them, deep learning
you can often try some things and

874
00:57:25,996 --> 00:57:29,450
sometimes even nonsensical
things surprisingly work.

875
00:57:29,450 --> 00:57:34,343
And then other people try to analyse why
that was the case in the first place.

876
00:57:34,343 --> 00:57:37,871
But yeah, there's no mathematical reasons
why you couldn't at all have a rectified

877
00:57:37,871 --> 00:57:38,660
linear unit here.

878
00:57:42,117 --> 00:57:48,638
All right, now on to a even more
complex sort of overall recurrent unit.

879
00:57:48,638 --> 00:57:53,022
Namely the long-short-term-memories or
LSTMs.

880
00:57:53,022 --> 00:57:56,922
So now this is the hippest
model of the day, and

881
00:57:56,922 --> 00:58:00,710
it's pretty important to know it well.

882
00:58:00,710 --> 00:58:03,766
Fortunately, it's again very similar
to the kinds of basic building blocks.

883
00:58:03,766 --> 00:58:08,481
But now we allow each of
the different steps to have again,

884
00:58:08,481 --> 00:58:11,840
we separate them out even more.

885
00:58:11,840 --> 00:58:13,130
So how do we separate them out?

886
00:58:13,130 --> 00:58:15,925
Basically, this is what's
going on at each time step.

887
00:58:15,925 --> 00:58:20,095
We will have an input gate,
forget gate, output gate, memory cell,

888
00:58:20,095 --> 00:58:22,583
final memory, and a final hidden state.

889
00:58:22,583 --> 00:58:24,814
Now let's gain a little
bit of intuition and

890
00:58:24,814 --> 00:58:27,358
there is good intuition of
why we want any of them.

891
00:58:27,358 --> 00:58:31,621
So the input gate will
basically determine how much we

892
00:58:31,621 --> 00:58:35,043
will care about the current vector at all.

893
00:58:35,043 --> 00:58:39,726
So how much does the current cell or
the current input word vector matter?

894
00:58:39,726 --> 00:58:44,194
The forget gate is a separate mechanism
that just says maybe I should forget,

895
00:58:44,194 --> 00:58:45,166
maybe I don't.

896
00:58:45,166 --> 00:58:48,218
In this case here, just kind of
counterintuitive sometimes and

897
00:58:48,218 --> 00:58:51,020
they're actually different
models in the literatures.

898
00:58:51,020 --> 00:58:53,543
Some have the one minus there and
others don't.

899
00:58:53,543 --> 00:58:55,980
But in general here,
we'll define our forget gate.

900
00:58:55,980 --> 00:58:58,220
If it's 0 then we're forgetting the past.

901
00:59:00,010 --> 00:59:04,560
Then we have an output gate,
basically when you have this output gate,

902
00:59:04,560 --> 00:59:09,775
you will separate out what
matters to a certain prediction

903
00:59:09,775 --> 00:59:16,030
versus what matters to being kept around
over the current recurrent time steps.

904
00:59:16,030 --> 00:59:19,054
So you might say at
this current time step,

905
00:59:19,054 --> 00:59:24,515
this particular cell is not important,
but it will become important later.

906
00:59:24,515 --> 00:59:28,580
And so I'm not going to output it,
to my final softmax for instance, but

907
00:59:28,580 --> 00:59:30,460
I'm still gonna keep it around.

908
00:59:31,470 --> 00:59:35,020
So it's yet another separate
mechanism to learn when to do that.

909
00:59:36,250 --> 00:59:41,241
And then we have our new memory cell here,
which is similar to what we had before.

910
00:59:41,241 --> 00:59:45,323
So in fact all these four here
have the same equation inside and

911
00:59:45,323 --> 00:59:49,420
just three sigmoid non linearity and
one tan h non linearity.

912
00:59:51,050 --> 00:59:54,770
So these are all just four
single layer neural nets.

913
00:59:56,710 --> 01:00:01,660
Now we'll put all of these gates together
when we compute the memory cell and

914
01:00:01,660 --> 01:00:02,620
the final hidden state.

915
01:00:02,620 --> 01:00:06,260
So the final memory cell now
basically separated out the input and

916
01:00:06,260 --> 01:00:07,680
the forget gate.

917
01:00:07,680 --> 01:00:10,920
Instead of just c and 1 minus c,
we have two separate mechanisms

918
01:00:10,920 --> 01:00:14,110
that can be trained and
learn slightly different things.

919
01:00:14,110 --> 01:00:18,817
And actually become also in some ways
counter intuitive like you say, I don't

920
01:00:18,817 --> 01:00:23,546
wanna forget but you do wanna forget,
but you also input something right now.

921
01:00:23,546 --> 01:00:28,771
But the model turns out to work very well.

922
01:00:28,771 --> 01:00:33,571
So basically here we have final hidden
state is just to forget gate how to

923
01:00:33,571 --> 01:00:38,464
mark product with the previous hidden
states final memory cell ct-1.

924
01:00:38,464 --> 01:00:42,433
So this again will determine,
how much do you wanna keep this around or

925
01:00:42,433 --> 01:00:45,600
how much do we wanna forget from the past?

926
01:00:45,600 --> 01:00:50,250
And then the new memory cell here,
this has a standard recurrent neural net.

927
01:00:50,250 --> 01:00:55,720
If i is all 1s,
then we really keep the input around.

928
01:00:55,720 --> 01:00:59,211
And if the input gate says no,
this one doesn't matter,

929
01:00:59,211 --> 01:01:02,946
then you just basically ignore
the current word back there.

930
01:01:06,475 --> 01:01:09,500
So in that sense,
this equation is quite intuitive, right?

931
01:01:09,500 --> 01:01:16,694
Forget the past or not, take the input or
not, that's basically it, yeah?

932
01:01:19,751 --> 01:01:20,469
So the secret question,

933
01:01:20,469 --> 01:01:23,996
once you forget the past does it mean
you forget grammar or something else?

934
01:01:23,996 --> 01:01:30,020
And the truth is we can think of these
forget gates as sort of absolutes.

935
01:01:30,020 --> 01:01:31,843
They're all vectors, and

936
01:01:31,843 --> 01:01:36,208
they will all forget only certain
elements of a long hidden unit.

937
01:01:36,208 --> 01:01:42,930
And so really, I can eventually show
you what these hidden states look like.

938
01:01:42,930 --> 01:01:46,030
And sometimes they're actually
more intuitive than others.

939
01:01:46,030 --> 01:01:50,280
But it's rare that you would find this
particular unit when it was turned off or

940
01:01:50,280 --> 01:01:53,410
on actually had like this
perfect interpretation that

941
01:01:53,410 --> 01:01:56,542
we as humans find intuitive and
think of as grammar.

942
01:01:56,542 --> 01:02:00,760
And also of course grammar is
a very complex kind of beast.

943
01:02:00,760 --> 01:02:05,327
And so it's hard to say any single unit
would capture any particular like entirety

944
01:02:05,327 --> 01:02:08,320
of a grammar,
it might only capture certain things.

945
01:02:08,320 --> 01:02:12,530
So it's not implausible to think
of these three cells together

946
01:02:12,530 --> 01:02:16,240
suggest that the next noun should be
a plural noun or something like that.

947
01:02:16,240 --> 01:02:18,422
But that's the most we could hope for
in many cases.

948
01:02:21,624 --> 01:02:24,662
All right, and then here,
the final hidden state again,

949
01:02:24,662 --> 01:02:26,589
we can keep these cs around, right?

950
01:02:26,589 --> 01:02:30,370
And cs will compute our
computer from other cs.

951
01:02:30,370 --> 01:02:34,840
But we might not want to expose
the content of this memory cell

952
01:02:34,840 --> 01:02:38,460
in order to compute the final
hidden state, ht minus 1.

953
01:02:43,175 --> 01:02:46,878
All right, now yeah,
this is it, this is the LSTM.

954
01:02:46,878 --> 01:02:50,240
It's a really powerful model, are there
any questions around the equations?

955
01:02:50,240 --> 01:02:54,150
We're gonna attempt at some illustrations,
but

956
01:02:54,150 --> 01:02:59,283
again I think the equations
are sometimes more intuitive.

957
01:03:03,853 --> 01:03:09,870
Does the LSTM and GRU completely liviate
or just help with an engine came problem?

958
01:03:09,870 --> 01:03:13,741
And the truth is they helped with it a
lot, but they don't completely obviate it.

959
01:03:13,741 --> 01:03:18,420
You do multiply here a bunch of
numbers that are often smaller than 1.

960
01:03:18,420 --> 01:03:23,620
And over time even if it would
have to be a perfect one,

961
01:03:23,620 --> 01:03:26,680
but that would mean that, that unit
is really, really strongly active.

962
01:03:26,680 --> 01:03:31,467
And then it's hard to sort of dies,
it's like the gradient,

963
01:03:31,467 --> 01:03:37,837
when you have unit that's really, really
active and looks something like this.

964
01:03:37,837 --> 01:03:41,314
Now the input is really large
to that unit and it's here,

965
01:03:41,314 --> 01:03:44,170
then grade in around here,
It's pretty much 0.

966
01:03:44,170 --> 01:03:45,510
So that unit's kind of dead.

967
01:03:45,510 --> 01:03:47,380
And then the model can't do
anything with it anymore.

968
01:03:47,380 --> 01:03:50,610
And so it happens, there are,
when you want to train these,

969
01:03:50,610 --> 01:03:53,830
you'll observe some units just sort
of die after training after awhile.

970
01:03:53,830 --> 01:03:58,340
And you'll just sort of keep around stuff,
or delete stuff at each time step.

971
01:03:58,340 --> 01:04:03,560
But in general most of the units
are somewhat small than 1, and

972
01:04:03,560 --> 01:04:10,280
so you still have a bit of a vanishing
creating problem but much less so.

973
01:04:10,280 --> 01:04:13,030
And intuitively you can
come up with final P for

974
01:04:13,030 --> 01:04:17,170
a lot of good ways to
think about this right?

975
01:04:17,170 --> 01:04:19,960
Maybe you want to predict different
things at different time steps.

976
01:04:19,960 --> 01:04:25,150
But you wanna keep around knowledge
through the memory cells but

977
01:04:25,150 --> 01:04:27,290
not expose it at a given prediction.

978
01:04:27,290 --> 01:04:27,790
Yeah.

979
01:04:32,166 --> 01:04:34,470
What is the point of the exposure gate
when it already had the forget gate?

980
01:04:34,470 --> 01:04:37,970
So basically, you want to,

981
01:04:37,970 --> 01:04:40,770
sort of forget gate will tell you whether
you keep something around or not.

982
01:04:41,780 --> 01:04:45,990
But exposure gate, will mean, does it
matter to this current time step or not.

983
01:04:47,000 --> 01:04:48,968
So you might not wanna forget something.

984
01:04:48,968 --> 01:04:51,783
But you also might not wanna
show it to the current output,

985
01:04:51,783 --> 01:04:53,920
because it's irrelevant for that output.

986
01:04:53,920 --> 01:04:57,453
And it would just confuse the Softmax
classifier at that output.

987
01:05:00,749 --> 01:05:01,485
Yeah?

988
01:05:11,171 --> 01:05:16,620
Does the exposure gate help you, or
do you mean the output gate here, right?

989
01:05:16,620 --> 01:05:19,615
So does the output gate,
does it help you to what exactly?

990
01:05:22,221 --> 01:05:24,404
To not have to forget everything forever.

991
01:05:28,493 --> 01:05:29,880
So, in some ways, yes.

992
01:05:29,880 --> 01:05:33,710
You can basically,
this model could decide that,

993
01:05:33,710 --> 01:05:38,170
while it doesn't wanna give as
output something for a long time.

994
01:05:38,170 --> 01:05:42,660
And hence it's basically
a temporal forgetting, right?

995
01:05:42,660 --> 01:05:46,240
It will only be forgotten at that time
set but actually be kept around in.

996
01:05:46,240 --> 01:05:49,660
I don't wanna use,
like anthropomorphize the models, but

997
01:05:49,660 --> 01:05:52,280
like the subconsciousness of this model or
whatever, right?

998
01:05:52,280 --> 01:05:55,080
Keeps it around but doesn't expose it.

999
01:05:55,080 --> 01:05:55,828
Don't quote me on that.

1000
01:06:00,521 --> 01:06:02,025
All right, one last question, yeah?

1001
01:06:07,252 --> 01:06:10,750
The initialization to all these models
matters, it matters quite significantly.

1002
01:06:10,750 --> 01:06:13,770
So, if you initialize all your weights,
for instance such that

1003
01:06:13,770 --> 01:06:16,760
whatever you do in the beginning,
all of the weights are super large.

1004
01:06:16,760 --> 01:06:19,450
Then your gradients are zero and
you're stuck in the optimization.

1005
01:06:19,450 --> 01:06:23,490
So you always have to
initialize them properly.

1006
01:06:23,490 --> 01:06:27,330
In most cases, as long as they're
relatively small, you can't go too wrong.

1007
01:06:27,330 --> 01:06:30,100
Eventually, it might slow down
your eventual convergence, but

1008
01:06:30,100 --> 01:06:31,980
as long as all your parameters, W here,

1009
01:06:31,980 --> 01:06:35,310
and your word vectors and so
on are initialized to very small numbers.

1010
01:06:35,310 --> 01:06:37,370
It will usually eventually
do it pretty well.

1011
01:06:40,421 --> 01:06:43,190
Yes you could use lots of different
strategies for initialization.

1012
01:06:44,270 --> 01:06:46,390
All right, now, some visualizations.

1013
01:06:46,390 --> 01:06:50,800
I like this one from Chris Olah on
his blog from not too long ago.

1014
01:06:50,800 --> 01:06:53,260
But again, I don't know.

1015
01:06:53,260 --> 01:06:55,890
I feel like the equations speak mostly for
themselves.

1016
01:06:55,890 --> 01:06:57,000
You can think of these.

1017
01:06:57,000 --> 01:07:00,840
I have four different neural network
layers, and then you combine them in

1018
01:07:00,840 --> 01:07:05,890
various ways with pointwise operations,
such as multiplication or addition.

1019
01:07:05,890 --> 01:07:08,080
And sometimes you know multiplication and
then addition,

1020
01:07:08,080 --> 01:07:11,340
and concatenation and copies and so on.

1021
01:07:11,340 --> 01:07:13,500
But, In the end you often observe,

1022
01:07:13,500 --> 01:07:17,590
this kind of thing where we'll
just write LSTM in this block.

1023
01:07:17,590 --> 01:07:19,354
And has an X and an H, and

1024
01:07:19,354 --> 01:07:24,578
we don't really look into too many
details of what's going on there.

1025
01:07:26,498 --> 01:07:31,699
And here's some, I think, even less
helpful [LAUGH] illustrations that,

1026
01:07:31,699 --> 01:07:35,740
yeah, I think are mostly
confusing to a lot of people.

1027
01:07:35,740 --> 01:07:39,180
I have the forget gates here,
output gates, input gates, and so on.

1028
01:07:39,180 --> 01:07:45,530
But and your memory cells as
they try to modify each other.

1029
01:07:45,530 --> 01:07:47,200
This one is a little cleaner.

1030
01:07:47,200 --> 01:07:49,000
You know you have some inputs, your gates,

1031
01:07:49,000 --> 01:07:54,100
you have your forget gates on top
of your memory cell and so on.

1032
01:07:54,100 --> 01:07:57,510
But in general I think the equations
are actually quite intuitive, right?

1033
01:07:57,510 --> 01:07:58,910
If you think of your extremes,

1034
01:07:58,910 --> 01:08:03,320
if this is zero, one, then this
input matters more to the output.

1035
01:08:05,890 --> 01:08:09,820
All right, now as I said,
LSTMs, currently super hip.

1036
01:08:09,820 --> 01:08:14,450
The en vogue model are for pretty
much all sequence labeling tasks and

1037
01:08:14,450 --> 01:08:17,430
sequence to sequence tasks
like machine translation.

1038
01:08:17,430 --> 01:08:21,740
Super powerful in many cases, you will
actually observe that we'll stack them.

1039
01:08:21,740 --> 01:08:26,510
So just like the other RNN architectures,
we'll have a whole LSTM block and

1040
01:08:26,510 --> 01:08:30,660
we put another LSTM block with different
sets of parameters on top of it.

1041
01:08:30,660 --> 01:08:33,740
And then the parameters
are shared over time, but

1042
01:08:33,740 --> 01:08:36,240
are different as you
have a very deep model.

1043
01:08:37,359 --> 01:08:42,249
And, of course, with all these
parameters here, we have essentially

1044
01:08:42,250 --> 01:08:45,300
many more parameters then the standard
recurrent neural network.

1045
01:08:45,300 --> 01:08:49,210
Where we only have two such parameters and
we update every time.

1046
01:08:49,210 --> 01:08:53,050
You wanna have more data especially
if you stack you now have

1047
01:08:53,050 --> 01:08:57,729
10x the parameters of standard RNN,
we wanna train this on a lot of data.

1048
01:08:57,729 --> 01:09:01,599
And in terms of amount of training
data available machine translation is

1049
01:09:01,600 --> 01:09:04,310
actually one of the best tasks for that.

1050
01:09:04,310 --> 01:09:10,920
And is also the one where these
model sort of shine the most.

1051
01:09:10,920 --> 01:09:15,647
And so in 2015, I think the first time I
gave the deep learning for NLP lecture,

1052
01:09:15,647 --> 01:09:17,790
the jury was still a little bit out.

1053
01:09:17,790 --> 01:09:21,330
The neural network models
came up fairly quickly.

1054
01:09:21,330 --> 01:09:26,580
But some different, more traditional
machine translation systems

1055
01:09:26,580 --> 01:09:30,640
were still slightly better,
like by half a BLEU point.

1056
01:09:30,640 --> 01:09:33,490
We haven't defined BLEU scores yet.

1057
01:09:33,490 --> 01:09:36,430
You can essentially think
of it as an engram overlap.

1058
01:09:36,430 --> 01:09:40,840
The more your translation overlaps
in terms of unigrams and bigrams and

1059
01:09:40,840 --> 01:09:45,870
trigrams, the better it likely is, period.

1060
01:09:45,870 --> 01:09:49,609
So you have this reference translation,
sometimes multiple reference translations.

1061
01:09:49,609 --> 01:09:52,729
You have your translation, you look
at engram overlap between the two.

1062
01:09:52,729 --> 01:09:53,539
So the higher the better.

1063
01:09:54,580 --> 01:09:58,350
And basically the neural network
models were often also just use it for

1064
01:09:58,350 --> 01:10:02,280
rescoring traditional MT model.

1065
01:10:02,280 --> 01:10:04,980
Now, just one year later, last year,

1066
01:10:04,980 --> 01:10:08,990
really a couple months ago,
the story was completely different.

1067
01:10:08,990 --> 01:10:15,008
So this is WMT, the worldwide
competition for machine translation.

1068
01:10:15,008 --> 01:10:18,210
And you have different universities,

1069
01:10:18,210 --> 01:10:22,550
and different companies and
so on, submit their systems.

1070
01:10:22,550 --> 01:10:28,320
And the top three systems were all
neural machine translation systems.

1071
01:10:28,320 --> 01:10:31,580
The jury is now basically not out anymore.

1072
01:10:31,580 --> 01:10:35,334
It's clear neural machine
translation is the most accurate

1073
01:10:35,334 --> 01:10:37,917
machine translation model in the world.

1074
01:10:41,427 --> 01:10:43,064
Yeah that number two was us, yeah.

1075
01:10:43,064 --> 01:10:46,819
>> [LAUGH]
>> James Bradbury and me worked on that.

1076
01:10:49,795 --> 01:10:53,576
James Bradbury was actually a linguistics
undergrad while he was doing that, but

1077
01:10:53,576 --> 01:10:54,684
now he's full-time.

1078
01:10:55,880 --> 01:10:59,990
So, yeah, basically we haven't talked
that much about ensembling and

1079
01:10:59,990 --> 01:11:01,610
ensembles of different models.

1080
01:11:01,610 --> 01:11:04,610
But you can also train
five of these monsters and

1081
01:11:04,610 --> 01:11:08,580
then average all the probabilities and
you'll usually get a little better.

1082
01:11:08,580 --> 01:11:09,960
We just, as general thing,

1083
01:11:09,960 --> 01:11:12,990
you'll observe for every competition
machine learning competition out there.

1084
01:11:12,990 --> 01:11:16,800
If you go on Kaggle, other machine
learning competitions usually train

1085
01:11:16,800 --> 01:11:18,510
even the same kind of model five times.

1086
01:11:18,510 --> 01:11:20,690
You end up in slightly different
local optimum average,

1087
01:11:20,690 --> 01:11:23,050
and you still do pretty well.

1088
01:11:23,050 --> 01:11:24,660
What's cool also though,

1089
01:11:24,660 --> 01:11:30,200
is that while we might not be able
to exactly recover grammar, or

1090
01:11:30,200 --> 01:11:37,110
have specific units be explicitly sort
of capturing very intuitive things.

1091
01:11:37,110 --> 01:11:39,641
As we project this down
similar to the word vectors,

1092
01:11:39,641 --> 01:11:42,760
we actually do observe some
pretty interesting regularities.

1093
01:11:42,760 --> 01:11:47,855
So this is a paper from Sutskever in 2014,

1094
01:11:47,855 --> 01:11:52,175
they projected different sentences.

1095
01:11:52,175 --> 01:11:57,079
They were trained basically with
a machine translation task and

1096
01:11:57,079 --> 01:12:01,270
basically observe quite
interesting regularities.

1097
01:12:01,270 --> 01:12:05,419
So John admires Mary is close
to John is in love with Mary and

1098
01:12:05,419 --> 01:12:07,120
to John respects Mary.

1099
01:12:07,120 --> 01:12:07,695
Now of course,

1100
01:12:07,695 --> 01:12:10,494
we have to be a little carefull here
to not over interpret the amazingness.

1101
01:12:10,494 --> 01:12:14,260
It's amazing, but
we also have a selection vice here, right?

1102
01:12:14,260 --> 01:12:20,080
Maybe if we just had
John did admire Mary or

1103
01:12:20,080 --> 01:12:21,830
something, it might also be close to it,
right?

1104
01:12:21,830 --> 01:12:23,310
And it might be closer too, but

1105
01:12:23,310 --> 01:12:29,030
if you just project these six particular
sentences into lower dimensional space.

1106
01:12:29,030 --> 01:12:33,790
Then you do see very nicely that whenever
John has some positive feelings for

1107
01:12:33,790 --> 01:12:36,790
Mary, all those sentences are in here.

1108
01:12:36,790 --> 01:12:42,410
And all the ones that are on this area
of the first two item vectors, Mary

1109
01:12:42,410 --> 01:12:46,810
admires John, Mary admires John, Mary is
in love with John, and Mary respects John.

1110
01:12:48,150 --> 01:12:49,150
They're all closer together,

1111
01:12:49,150 --> 01:12:52,670
which is kind of amazing cuz
some people are also worried.

1112
01:12:52,670 --> 01:12:54,160
Well it's a sequence model, so

1113
01:12:54,160 --> 01:12:58,370
how could it ever capture
that the word order changes?

1114
01:12:58,370 --> 01:13:01,288
And so this is a particularly
cool example of that.

1115
01:13:01,288 --> 01:13:02,361
So here we have,

1116
01:13:02,361 --> 01:13:07,202
she was given a card by me in the garden
versus in the garden I gave her a card.

1117
01:13:07,202 --> 01:13:09,514
And I gave her a card in the garden, and

1118
01:13:09,514 --> 01:13:13,000
despite the word order being
actually flipped, right?

1119
01:13:13,000 --> 01:13:16,737
In the garden is in the beginning here,
and in the end here.

1120
01:13:16,737 --> 01:13:20,740
These are still closer together
than the different ones where,

1121
01:13:20,740 --> 01:13:25,630
in the garden basically she gave me
a card verses I gave her a card.

1122
01:13:25,630 --> 01:13:29,481
So that shows that the semantics here
turn out to be more important than

1123
01:13:29,481 --> 01:13:30,405
the word order.

1124
01:13:30,405 --> 01:13:32,801
Despite the model just
going from left to right or

1125
01:13:32,801 --> 01:13:37,450
this one was still the trick where we
reversed the order of the input sentence.

1126
01:13:37,450 --> 01:13:41,800
But it choses that its
incredibly invariant and

1127
01:13:41,800 --> 01:13:43,480
variance is a pretty important concept,
right?

1128
01:13:43,480 --> 01:13:48,100
We want this model to be
invariant to simple syntactic

1129
01:13:48,100 --> 01:13:51,940
changes when the semantics
are actually kept the same.

1130
01:13:51,940 --> 01:13:55,313
It's pretty incredible, that it does that.

1131
01:13:55,313 --> 01:13:58,253
So this is also the power
I think of some of these.

1132
01:13:58,253 --> 01:14:02,578
This is a very deep LSTM model where
you have five different LSTM stacked in

1133
01:14:02,578 --> 01:14:04,965
the encoder and several in the decoder.

1134
01:14:04,965 --> 01:14:07,780
And they're all connected
in multiple places too.

1135
01:14:10,380 --> 01:14:14,109
All right, any questions around
those visualizations and LSTMs?

1136
01:14:19,392 --> 01:14:23,911
All right, you now have knowledge under
you belt that is super powerful and

1137
01:14:23,911 --> 01:14:25,670
very interesting.

1138
01:14:25,670 --> 01:14:27,945
I expected to maybe have
five minutes more of time.

1139
01:14:27,945 --> 01:14:31,746
So I'm going to talk to you about a recent
improvement, two recurrent neural networks

1140
01:14:31,746 --> 01:14:34,467
that I think is also very
applicable to machine translation.

1141
01:14:34,467 --> 01:14:37,625
But nobody has actually yet
applied it to machine translation.

1142
01:14:37,625 --> 01:14:42,245
And that is a general problem
with all softmax classification

1143
01:14:42,245 --> 01:14:44,755
that we do in all the models I've so
far described to you.

1144
01:14:44,755 --> 01:14:46,905
And really up until two or
three months ago,

1145
01:14:46,905 --> 01:14:50,170
that everybody in NLP
had as a major problem.

1146
01:14:50,170 --> 01:14:55,131
And that is you can only ever predict
answers if you saw that exact word at

1147
01:14:55,131 --> 01:14:56,295
training time.

1148
01:14:56,295 --> 01:15:00,090
And you have your cross entropy error
saying I wanna predict this word.

1149
01:15:00,090 --> 01:15:03,612
And if you've never predicted that word,
no matter how obvious it is for

1150
01:15:03,612 --> 01:15:05,950
the translation system it will
not be able to do it, right?

1151
01:15:05,950 --> 01:15:13,650
So we have some kind of translation,
and let's us say we have a new word,

1152
01:15:13,650 --> 01:15:18,120
like a new name or something that
we've never seen at training time.

1153
01:15:18,120 --> 01:15:23,223
And it is very obvious that this word
here should go at this location.

1154
01:15:23,223 --> 01:15:26,782
This is like Mrs. and
then maybe the new word is like yelling or

1155
01:15:26,782 --> 01:15:29,668
something like that,
it could be any other word.

1156
01:15:29,668 --> 01:15:32,766
And now let's say at training time,
we've never seen the word yelling.

1157
01:15:32,766 --> 01:15:37,046
But now, it's like vowel, German misses,

1158
01:15:37,046 --> 01:15:41,562
miss in, yeah,
German translation for this.

1159
01:15:41,562 --> 01:15:44,930
And now it's very obvious to
everybody that after this word,

1160
01:15:44,930 --> 01:15:47,865
it should be the next one,
the name of the of the miss.

1161
01:15:47,865 --> 01:15:53,050
And so these models would never
be able to do that, right?

1162
01:15:53,050 --> 01:15:58,213
And so one way to fix that is to think
about character meant translation models,

1163
01:15:58,213 --> 01:16:03,317
where the model's actually surprisingly
similar to what we described here.

1164
01:16:03,317 --> 01:16:09,560
Well many times it have to go, but instead
of having words we just have characters.

1165
01:16:09,560 --> 01:16:13,170
So that's one way, but
now we have very long sequences.

1166
01:16:13,170 --> 01:16:18,310
And at every character you have
a lot of matrix multiplications.

1167
01:16:18,310 --> 01:16:22,480
And these matrix multiplications
that we have in here are not

1168
01:16:23,720 --> 01:16:27,420
50 dimensional for really powerful MT
models, they're a 1,000 dimensional.

1169
01:16:28,490 --> 01:16:32,644
And now you have several thousand
by a thousand matrices here

1170
01:16:32,644 --> 01:16:36,151
multiplying with thousand
dimensional vectors.

1171
01:16:36,151 --> 01:16:38,999
And you stack them, so
you have to do it five times.

1172
01:16:38,999 --> 01:16:42,249
Doing that for every single character
actually gets really, really expensive.

1173
01:16:43,540 --> 01:16:47,950
So at the same time,
it's very intuitive that

1174
01:16:47,950 --> 01:16:51,366
after we see a new word at test time
we wanna be able to predict it.

1175
01:16:51,366 --> 01:16:55,022
And also in general when we have
the softmax, even for words that we do

1176
01:16:55,022 --> 01:16:58,817
see once or twice, it's hard for
the model to then still predict them.

1177
01:16:58,817 --> 01:17:02,230
It's this skewed data set
distribution problem.

1178
01:17:02,230 --> 01:17:07,012
But you have very rare, very infrequent
classes, our words are hard to predict for

1179
01:17:07,012 --> 01:17:07,773
the models.

1180
01:17:07,773 --> 01:17:13,225
So this is one attempt at fixing that,
which is essentially a mixture

1181
01:17:13,225 --> 01:17:18,880
model of using standard softmax and
what we call a pointer.

1182
01:17:18,880 --> 01:17:21,610
So what's a pointer?
It's essentially a mechanism to

1183
01:17:21,610 --> 01:17:26,484
say well maybe my next word is one of
the previous words in the context.

1184
01:17:26,484 --> 01:17:30,675
You say 100 words in the past,
and every time step you say,

1185
01:17:30,675 --> 01:17:35,370
maybe I just wanna copy a word
over from the last 100 words.

1186
01:17:35,370 --> 01:17:40,998
And if not, then I will use my
standard softmax for the rest.

1187
01:17:40,998 --> 01:17:43,021
So this is kind of this
sentinel idea here.

1188
01:17:43,021 --> 01:17:48,570
This is a paper by Stephen Merity and
some other folks.

1189
01:17:48,570 --> 01:17:52,870
And basically, we now have
a mixture model, where we combine

1190
01:17:52,870 --> 01:17:57,520
the probabilities from the standard
vocabulary and from this pointer.

1191
01:17:57,520 --> 01:17:58,835
And now how do we compute this pointer?

1192
01:17:58,835 --> 01:18:03,498
It's very straightforward,
we basically have a query.

1193
01:18:03,498 --> 01:18:09,380
This query is just a modification of
the last hidden layer that we have here.

1194
01:18:09,380 --> 01:18:12,260
And we pipe that through a standard
single layer neural network

1195
01:18:12,260 --> 01:18:14,910
to compute another hidden layer,
which we'll call q.

1196
01:18:14,910 --> 01:18:18,328
And then we'll do an inter
product between this q and

1197
01:18:18,328 --> 01:18:22,457
all the previous hidden states
of the last 100 timed steps.

1198
01:18:22,457 --> 01:18:25,241
And that will give us,
basically, the single number for

1199
01:18:25,241 --> 01:18:26,800
each of these interproducts.

1200
01:18:26,800 --> 01:18:30,496
And then we'll apply
a softmax on top of that.

1201
01:18:30,496 --> 01:18:33,582
And this gives us essentially,
a probability for

1202
01:18:33,582 --> 01:18:37,490
how likely do we wanna point
to each of these words.

1203
01:18:37,490 --> 01:18:41,810
Or the very last one is we
don't point to anything,

1204
01:18:41,810 --> 01:18:43,910
we just take the standard softmax.

1205
01:18:43,910 --> 01:18:47,035
So we keep one unit
around where we do this.

1206
01:18:47,035 --> 01:18:51,412
And now of course in the context,
the same word might appear multiple times.

1207
01:18:51,412 --> 01:18:55,410
And so you just sum up all
the probabilities for specific words.

1208
01:18:55,410 --> 01:18:58,980
If they appear multiple times,
you just sum them up.

1209
01:18:58,980 --> 01:19:05,242
With this simple modification, we now
have the ability to predict unseen words.

1210
01:19:05,242 --> 01:19:10,112
We can predict based on the pattern
of how rare words appear much more

1211
01:19:10,112 --> 01:19:11,490
similar things.

1212
01:19:11,490 --> 01:19:16,408
For instance, Fed, Chair, Janet,
Yellen, raised, rates and so on, Ms.

1213
01:19:16,408 --> 01:19:20,970
is very obvious that this is the same Ms.
that we're referring to here.

1214
01:19:22,150 --> 01:19:25,540
And you can base or
you combine this in this mixture model.

1215
01:19:25,540 --> 01:19:28,772
And now over many, many years for
language modeling.

1216
01:19:28,772 --> 01:19:35,203
The perplexity that we defined before
was sort of stock actually around 80.

1217
01:19:35,203 --> 01:19:36,514
And then in 2015,

1218
01:19:36,514 --> 01:19:40,820
we have a bunch of modifications
to LSTMs that were very powerful.

1219
01:19:40,820 --> 01:19:46,060
And lower this, and
now were down to the lowest 70s.

1220
01:19:46,060 --> 01:19:49,790
And was some modifications
will cover another class,

1221
01:19:49,790 --> 01:19:51,430
were actually down on the 60s now.

1222
01:19:51,430 --> 01:19:53,859
So it really had to told for
several years, and

1223
01:19:53,859 --> 01:19:56,429
now perplexity numbers
are really dropping in.

1224
01:19:56,429 --> 01:20:00,805
And this models are getting better and
bettered capturing, more and

1225
01:20:00,805 --> 01:20:03,831
more the semantics and
the syntax of language.

1226
01:20:03,831 --> 01:20:05,180
All right, so let's summarize.

1227
01:20:05,180 --> 01:20:07,190
Recurrent Neural Networks, super powerful.

1228
01:20:07,190 --> 01:20:11,654
You now know the best ones in
that family to use in LSTMs.

1229
01:20:11,654 --> 01:20:15,110
This is a pretty advanced lecture,
I hope you gained some of the intuition.

1230
01:20:15,110 --> 01:20:20,659
Again, most of the math falls out from the
same basic building blocks we had before.

1231
01:20:20,659 --> 01:20:25,582
And next week or no next Thursday,
we'll do midterm review.

1232
01:20:25,582 --> 00:00:00,000
All right, thank you.

