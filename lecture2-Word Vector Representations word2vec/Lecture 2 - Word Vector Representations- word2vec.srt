1
00:00:00,000 --> 00:00:06,314
[MUSIC]

2
00:00:06,314 --> 00:00:07,713
Stanford University.

3
00:00:11,743 --> 00:00:14,257
>> Okay, so let's get going.

4
00:00:14,257 --> 00:00:18,609
Welcome back to the second
class of CS224N /Ling 284,

5
00:00:18,609 --> 00:00:22,570
Natural Language Processing
with Deep Learning.

6
00:00:22,570 --> 00:00:28,115
So this class is gonna be almost
the complete opposite of the last class.

7
00:00:28,115 --> 00:00:29,837
So in the last class,

8
00:00:29,837 --> 00:00:35,600
it was a very high level picture of
sort of trying from the very top down.

9
00:00:35,600 --> 00:00:39,462
Sort of say a little bit about what
is natural language processing,

10
00:00:39,462 --> 00:00:43,866
what is deep learning, why it's exciting,
why both of them are exciting and

11
00:00:43,866 --> 00:00:46,220
how I'd like to put them together?

12
00:00:46,220 --> 00:00:50,687
So for today's class we're gonna go
completely to the opposite extreme.

13
00:00:50,687 --> 00:00:54,324
We're gonna go right down
to the bottom of words,

14
00:00:54,324 --> 00:00:58,928
and we're gonna have vectors,
and we're gonna do baby math.

15
00:00:58,928 --> 00:01:04,965
Now for some of you this will seem
like tedious repetitive baby math.

16
00:01:04,965 --> 00:01:08,479
But I think that there are probably
quite a few of you for

17
00:01:08,479 --> 00:01:12,157
which having some math review
is just going to be useful.

18
00:01:12,157 --> 00:01:17,200
And this is really the sort of foundation
on which everything else builds.

19
00:01:17,200 --> 00:01:21,853
And so if you don't have sort of straight
the fundamentals right at the beginning of

20
00:01:21,853 --> 00:01:26,306
how you can use neural networks on the
sort of very simplest kind of structures,

21
00:01:26,306 --> 00:01:28,652
it's sort of really all over from there.

22
00:01:28,652 --> 00:01:32,599
So what I'd like to do today is
sort of really go slowly and

23
00:01:32,599 --> 00:01:38,034
carefully through the foundations of how
you can start to do things with neural

24
00:01:38,034 --> 00:01:43,250
networks in this very simple case of
learning representations for words.

25
00:01:43,250 --> 00:01:47,850
And hope that's kind of a good foundation
that we can build on forwards.

26
00:01:47,850 --> 00:01:51,060
And indeed that's what we're gonna
keep on doing, building forward.

27
00:01:51,060 --> 00:01:56,188
So next week Richard is gonna keep on
doing a lot of math from the ground up to

28
00:01:56,188 --> 00:02:01,593
try, and really help get straight some
of the foundations of Deep Learning.

29
00:02:01,593 --> 00:02:05,620
Okay, so this is basically the plan.

30
00:02:05,620 --> 00:02:11,463
So tiny bit word meaning and

31
00:02:11,463 --> 00:02:14,261
no, [LAUGH].

32
00:02:14,261 --> 00:02:19,793
>> [LAUGH]
>> Tiny bit on word meaning then start to

33
00:02:19,793 --> 00:02:25,160
introduce this model of learning
word vectors called Word2vec.

34
00:02:25,160 --> 00:02:29,196
And this was a model that was
introduced by Thomas Mikolov and

35
00:02:29,196 --> 00:02:31,582
colleagues at Google in 2013.

36
00:02:31,582 --> 00:02:35,278
And so there are many other
ways that you could think about

37
00:02:35,278 --> 00:02:37,815
having representations of words.

38
00:02:37,815 --> 00:02:41,417
And next week, Richard's gonna talk
about some of those other mechanisms.

39
00:02:41,417 --> 00:02:45,005
But today, I wanna sort of avoid
having a lot of background and

40
00:02:45,005 --> 00:02:46,810
comparative commentary.

41
00:02:46,810 --> 00:02:50,305
So I'm just gonna present
this one way of doing it.

42
00:02:50,305 --> 00:02:52,948
And you'd also pretty study
the good way of doing it, so

43
00:02:52,948 --> 00:02:55,220
it's not a bad one to know.

44
00:02:55,220 --> 00:02:58,663
Okay, so then after that,
we're gonna have the first or

45
00:02:58,663 --> 00:03:01,614
was it gonna be one of
the features of this class.

46
00:03:01,614 --> 00:03:07,811
We decided that all the evidence says that
students can't concentrate for 75 minutes.

47
00:03:07,811 --> 00:03:11,971
So we decided we'd sort of mix
it up a little, and hopefully,

48
00:03:11,971 --> 00:03:16,932
also give people an opportunity to sort
of get more of a sense of what some of

49
00:03:16,932 --> 00:03:22,100
the exciting new work that's coming
out every month in Deep Learning is.

50
00:03:22,100 --> 00:03:27,730
And so what we're gonna do is have one TA
each time, do a little research highlight.

51
00:03:27,730 --> 00:03:30,903
Which will just be sort of a like
a verbal blog post of telling

52
00:03:30,903 --> 00:03:35,163
you a little bit about some recent paper
and why it's interesting, exciting.

53
00:03:35,163 --> 00:03:38,334
We're gonna start that today with Danqi.

54
00:03:38,334 --> 00:03:39,343
Then after that,

55
00:03:39,343 --> 00:03:44,066
I wanna go sort of carefully through the
word to vec objective function gradients.

56
00:03:44,066 --> 00:03:47,102
Refresher little on optimization,
mention the assignment,

57
00:03:47,102 --> 00:03:51,180
tell you all about Word2vec
that's basically the plan, okay?

58
00:03:51,180 --> 00:03:55,528
So we kinda wonder sort
of have word vectors as I

59
00:03:55,528 --> 00:03:59,995
mentioned last time as
a model of word meaning.

60
00:03:59,995 --> 00:04:02,525
That's a pretty
controversial idea actually.

61
00:04:02,525 --> 00:04:07,335
And I just wanna give kind of a few words
of context before we dive into that and

62
00:04:07,335 --> 00:04:08,650
do it anyway.

63
00:04:08,650 --> 00:04:12,795
Okay, so
if you look up meaning in a dictionary cuz

64
00:04:12,795 --> 00:04:17,548
a dictionary is a storehouse
of word meanings after all.

65
00:04:17,548 --> 00:04:21,922
What the Webster's dictionary says is
meaning is the idea that is represented by

66
00:04:21,922 --> 00:04:24,020
a word, phrase, etc.

67
00:04:24,020 --> 00:04:29,448
The idea that a person wants to express
by using words, signs, etc, etc.

68
00:04:30,660 --> 00:04:34,210
In some sense, this is fairly close

69
00:04:34,210 --> 00:04:38,260
to what is the commonest linguistic
way of thinking of meaning.

70
00:04:38,260 --> 00:04:45,058
So standardly in linguistics,
you have a linguistic sign like a word,

71
00:04:45,058 --> 00:04:50,176
and then it has things that
it signifies in the world.

72
00:04:50,176 --> 00:04:55,651
So if I have a word like glasses then
it's got a signification which includes

73
00:04:55,651 --> 00:05:01,487
these and there are lots of other pairs of
glasses I can see in front of me, right?

74
00:05:01,487 --> 00:05:05,533
And those things that it signifies,

75
00:05:05,533 --> 00:05:09,585
the denotation of the term glasses.

76
00:05:09,585 --> 00:05:15,352
That hasn't proven to be a notion of
meaning that's been very easy for people

77
00:05:15,352 --> 00:05:21,260
to make much use of in computational
systems for dealing with language.

78
00:05:21,260 --> 00:05:25,730
So in practice, if you look at what
computational systems have done for

79
00:05:25,730 --> 00:05:29,310
meanings of words over
the last several decades.

80
00:05:29,310 --> 00:05:33,760
By far the most common thing that's
happened is, people have tried

81
00:05:33,760 --> 00:05:38,600
to deal with the meaning of words by
making use of taxonomic resources.

82
00:05:38,600 --> 00:05:42,800
And so if they're English, the most
famous taxonomic resource is WordNet.

83
00:05:42,800 --> 00:05:46,481
And it's famous,
maybe not like Websters is famous.

84
00:05:46,481 --> 00:05:48,629
But it's famous among
computational linguists.

85
00:05:48,629 --> 00:05:51,160
Because it's free to download a copy and

86
00:05:51,160 --> 00:05:55,713
that's much more useful than having
a copy of Webster's on your shelf.

87
00:05:55,713 --> 00:06:00,180
And it provides a lot of
taxonomy information about words.

88
00:06:01,500 --> 00:06:03,490
So this little bit of Python code.

89
00:06:03,490 --> 00:06:07,892
This is showing you getting a hold of
word net using the nltk which is one of

90
00:06:07,892 --> 00:06:10,240
the main Python packages for nlp.

91
00:06:10,240 --> 00:06:13,801
And so then I'm asking it for
the word panda,

92
00:06:13,801 --> 00:06:17,568
not the Python package Panda,
the word panda.

93
00:06:17,568 --> 00:06:18,648
Then I'm saying,

94
00:06:18,648 --> 00:06:23,190
well tell me about the hypernym the kind
of things that it's the kind of.

95
00:06:23,190 --> 00:06:28,003
And so for Panda it's sort of heading
up through carnivores, placentals,

96
00:06:28,003 --> 00:06:31,399
mammals up into sort of
abstract types like objects.

97
00:06:31,399 --> 00:06:34,412
Or on the right hand side,
I'm sort of asking for

98
00:06:34,412 --> 00:06:38,130
the word good,
will tell me about synonyms of good.

99
00:06:38,130 --> 00:06:42,890
And part of what your finding there is,
well WordNet is saying,

100
00:06:42,890 --> 00:06:45,987
well the word good has different senses.

101
00:06:45,987 --> 00:06:49,870
So for each sense, let me tell
you some synonyms for each sense.

102
00:06:49,870 --> 00:06:54,460
So one sense, the second one is sort
of the kind of good person sense.

103
00:06:54,460 --> 00:06:58,928
And they're suggesting synonyms
like honorable and respectable.

104
00:06:58,928 --> 00:07:04,087
But there are other ones here
where this pair is good to eat and

105
00:07:04,087 --> 00:07:06,930
that's sort of meaning is ripe.

106
00:07:08,750 --> 00:07:11,280
Okay, so
you get this sort of sense of meaning.

107
00:07:12,402 --> 00:07:16,043
That's been,
that's been a great resource, but

108
00:07:16,043 --> 00:07:20,437
it's also been a resource that
people have found in practice.

109
00:07:20,437 --> 00:07:26,030
It's hard to get nearly as much value out
of it as you'd like to get out of it.

110
00:07:26,030 --> 00:07:27,939
And why is that?

111
00:07:27,939 --> 00:07:30,210
I mean there are a whole bunch of reasons.

112
00:07:30,210 --> 00:07:35,534
I mean one reason is that at this level
of this sort of taxonomic relationships,

113
00:07:35,534 --> 00:07:38,161
you lose an enormous amount of nuance.

114
00:07:38,161 --> 00:07:43,146
So one of those synonym sets for
good was adept, expert, good, practiced,

115
00:07:43,146 --> 00:07:44,844
proficient, skillful.

116
00:07:44,844 --> 00:07:49,458
But I mean, it seems like those mean
really different things, right?

117
00:07:49,458 --> 00:07:52,820
It seems like saying I'm
an expert at deep learning.

118
00:07:53,965 --> 00:07:59,510
Means something slightly different
to I'm good at deep learning.

119
00:07:59,510 --> 00:08:01,538
So there's a lot of nuance there.

120
00:08:01,538 --> 00:08:06,018
There's a lot of incompleteness in WordNet
so for a lot of the ways that people,

121
00:08:06,018 --> 00:08:10,797
Use words more flexibly.

122
00:08:10,797 --> 00:08:14,166
So if I say I'm a deep-learning ninja, or

123
00:08:14,166 --> 00:08:18,765
something like that,
that that's not in WordNet at all.

124
00:08:18,765 --> 00:08:23,438
What kind of things you put into these
synonym sets ends up very subjective,

125
00:08:23,438 --> 00:08:23,956
right?

126
00:08:23,956 --> 00:08:27,238
Which sense distinctions you make and
which things you do and

127
00:08:27,238 --> 00:08:29,950
don't say are the same,
it's all very unclear.

128
00:08:29,950 --> 00:08:32,540
It requires,
even to the extent that it's made,

129
00:08:32,539 --> 00:08:37,049
it's required many person
years of human labor.

130
00:08:37,049 --> 00:08:43,029
And at the end of the day,
it's sort of, it's kind

131
00:08:43,030 --> 00:08:47,750
of hard to get anything accurate out of it
in the way of sort of word similarities.

132
00:08:47,750 --> 00:08:53,830
Like I kind of feel that proficient is
more similar to expert than good, maybe.

133
00:08:53,830 --> 00:08:57,085
But you can't get any of this
kind of stuff out of WordNet.

134
00:08:58,330 --> 00:09:03,740
Okay, so therefore,
that's sort of something of a problem.

135
00:09:03,740 --> 00:09:08,610
And it's part of this
general problem of discrete,

136
00:09:08,610 --> 00:09:13,190
or categorical, representations
that I started on last time.

137
00:09:13,190 --> 00:09:16,660
So, the fundamental thing
to note is that for

138
00:09:16,660 --> 00:09:21,930
sorta just about all NLP,
apart from both modern deep learning and

139
00:09:21,930 --> 00:09:25,670
a little bit of neural net work
NLP that got done in the 1980s,

140
00:09:25,670 --> 00:09:31,560
that it's all used atomic symbols
like hotel, conference, walk.

141
00:09:31,560 --> 00:09:36,720
And if we think of that from our kind
of jaundiced neural net direction,

142
00:09:36,720 --> 00:09:40,500
using atomic symbols is kind of like using

143
00:09:40,500 --> 00:09:45,100
big vectors that are zero everywhere
apart from a one and one position.

144
00:09:45,100 --> 00:09:49,080
So what we have, is we have a lot of words
in the language that are equivalent to our

145
00:09:49,080 --> 00:09:53,060
symbols and we're putting a one
in the position, in the vector,

146
00:09:53,060 --> 00:09:55,935
that represents the particular symbol,
perhaps hotel.

147
00:09:55,935 --> 00:09:59,220
And these vectors are going to be really,
really long.

148
00:09:59,220 --> 00:10:01,400
I mean,
how long depends on how you look at it.

149
00:10:01,400 --> 00:10:05,500
So sometimes a speech recognizer
might have a 20,000 word vocabulary.

150
00:10:05,500 --> 00:10:07,250
So it'd be that long.

151
00:10:07,250 --> 00:10:10,910
But, if we're kinda building
a machine translation system,

152
00:10:10,910 --> 00:10:16,000
we might use a 500,000 word vocabulary,
so that's very long.

153
00:10:16,000 --> 00:10:21,130
And Google released sort of
a 1-terabyte corpus of web crawl.

154
00:10:21,130 --> 00:10:24,470
That's a resource that's been
widely used for a lot of NLP.

155
00:10:24,470 --> 00:10:27,900
And while the size of the vocabulary
in that is 13 million words, so

156
00:10:27,900 --> 00:10:29,690
that's really, really long.

157
00:10:29,690 --> 00:10:33,968
So, it's a very, very big vector.

158
00:10:33,968 --> 00:10:37,730
And so, why are these vectors problematic?

159
00:10:37,730 --> 00:10:43,150
I'm sorry, I'm not remembering my slides,
so I should say my slides first.

160
00:10:43,150 --> 00:10:47,890
Okay, so this is referred to in
neural net land as one-hot in coding

161
00:10:47,890 --> 00:10:51,647
because there's just this
one on zero in the vector.

162
00:10:51,647 --> 00:10:55,580
And so, that's the example of
a localist representation.

163
00:10:57,000 --> 00:10:59,270
So why is this problematic?

164
00:10:59,270 --> 00:11:04,000
And the reason why it's
problematic is it doesn't give any

165
00:11:04,000 --> 00:11:07,870
inherent notion of
relationships between words.

166
00:11:07,870 --> 00:11:12,390
So, very commonly what we want
to know is when meanings and

167
00:11:12,390 --> 00:11:15,106
words and
phrases are similar to each other.

168
00:11:15,106 --> 00:11:19,520
So, for example, in a web search
application, if the user searches for

169
00:11:19,520 --> 00:11:21,670
Dell notebook battery size,

170
00:11:21,670 --> 00:11:26,120
we'd like to match a document that
says Dell laptop battery capacity.

171
00:11:26,120 --> 00:11:29,650
So we sort of want to know that
notebooks and laptops are similar,

172
00:11:29,650 --> 00:11:33,750
and size and capacity are similar,
so this will be equivalent.

173
00:11:33,750 --> 00:11:37,480
We want to know that hotels and
motels are similar in meaning.

174
00:11:37,480 --> 00:11:42,397
And the problem is that if we're
using one-hot vector encodings,

175
00:11:42,397 --> 00:11:45,600
they have no natural notion of similarity.

176
00:11:45,600 --> 00:11:48,075
So if we take these two vectors and say,

177
00:11:48,075 --> 00:11:51,909
what is the dot product between
those vectors, it's zero.

178
00:11:51,909 --> 00:11:56,050
They have no inherent
notion of similarity.

179
00:11:56,050 --> 00:12:00,888
And, something I just wanna stress
a little, since this is important,

180
00:12:00,888 --> 00:12:05,806
is note this problem of symbolic
encoding applies not only to traditional

181
00:12:05,806 --> 00:12:10,329
rule base logical approaches to
natural language processing, but

182
00:12:10,329 --> 00:12:15,246
it also applies to basically all of
the work that was done in probabilistic

183
00:12:15,246 --> 00:12:20,790
statistical conventional machine learning
base natural language processing.

184
00:12:20,790 --> 00:12:25,498
Although those Latin models normally had
real numbers, they had probabilities of

185
00:12:25,498 --> 00:12:29,871
something occurring in the context of
something else, that nevertheless,

186
00:12:29,871 --> 00:12:32,960
they were built over
symbolic representations.

187
00:12:32,960 --> 00:12:37,900
So that you weren't having any kind of
capturing relationships between words and

188
00:12:37,900 --> 00:12:42,190
the models,
each word was a nation to itself.

189
00:12:42,190 --> 00:12:46,200
Okay, so that's bad, and
we have to do something about it.

190
00:12:46,200 --> 00:12:51,316
Now, as I've said, there's more than
one thing that you could do about it.

191
00:12:53,451 --> 00:12:56,008
And so, one answer is to say, okay gee,

192
00:12:56,008 --> 00:12:59,715
we need to have a similarity
relationship between words.

193
00:12:59,715 --> 00:13:00,695
Let's go over here and

194
00:13:00,695 --> 00:13:05,165
start building completely separately
a similarity relationship between words.

195
00:13:05,165 --> 00:13:06,782
And, of course, you could do that.

196
00:13:06,782 --> 00:13:09,542
But I'm not gonna talk about that here.

197
00:13:09,542 --> 00:13:14,662
What instead I'm going to talk about and
suggest is that

198
00:13:14,662 --> 00:13:19,932
what we could do is we could
explore this direct approach,

199
00:13:19,932 --> 00:13:25,172
where the representation of
a word encodes its meaning

200
00:13:25,172 --> 00:13:29,860
in such a way that you can
just directly read off

201
00:13:29,860 --> 00:13:34,220
from these representations,
the similarity between words.

202
00:13:34,220 --> 00:13:36,300
So what we're gonna do is
have these vectors and

203
00:13:36,300 --> 00:13:38,720
do something like a dot product.

204
00:13:38,720 --> 00:13:42,620
And that will be giving us a sense
of the similarity between words.

205
00:13:44,050 --> 00:13:46,850
Okay, so how do we go about doing that?

206
00:13:46,850 --> 00:13:51,930
And so the way we gonna go about
doing that is by making use of this

207
00:13:53,320 --> 00:13:57,350
very simple, but
extremely profound and widely used,

208
00:13:57,350 --> 00:14:00,930
NLP idea called distributional similarity.

209
00:14:00,930 --> 00:14:03,630
So this has been a really powerful notion.

210
00:14:03,630 --> 00:14:09,460
So the notion of distributional similarity
is that you can get a lot of value for

211
00:14:09,460 --> 00:14:14,040
representing the meaning of a word
by looking at the context in

212
00:14:14,040 --> 00:14:18,130
which it appears and
doing something with those contexts.

213
00:14:19,860 --> 00:14:22,950
So, if I want to know what
the word banking means,

214
00:14:22,950 --> 00:14:28,030
what I'm gonna do is find thousands of
instances of the word banking in text and

215
00:14:28,030 --> 00:14:31,850
I'm gonna look at the environment
in which each one appeared.

216
00:14:31,850 --> 00:14:35,250
And I'm gonna see debt problems,
governments,

217
00:14:35,250 --> 00:14:39,240
regulation, Europe, saying unified and

218
00:14:39,240 --> 00:14:43,590
I'm gonna start counting up all of these
things that appear and by some means,

219
00:14:43,590 --> 00:14:48,470
I'll use those words in the context
to represent the meaning of banking.

220
00:14:49,630 --> 00:14:55,280
The most famous slogan that you
will read everywhere if you look

221
00:14:55,280 --> 00:15:01,380
into distributional similarity is this one
by JR Firth, who was a British linguist,

222
00:15:01,380 --> 00:15:06,358
who said, you shall know a word
by the company it keeps.

223
00:15:06,358 --> 00:15:11,825
But this is also really exactly the same
notion that Wittgenstein proposed in

224
00:15:11,825 --> 00:15:17,035
his later writings where he
suggested a use theory of meaning.

225
00:15:17,035 --> 00:15:21,840
Where, somewhat controversially,
this not the main stream in semantics,

226
00:15:21,840 --> 00:15:26,980
he suggested that the right way to
think about the meaning of words is

227
00:15:26,980 --> 00:15:29,120
understanding their uses in text.

228
00:15:29,120 --> 00:15:33,570
So, essentially,
if you could predict which textual context

229
00:15:33,570 --> 00:15:37,490
the word would appear in, then you
understand the meaning of the word.

230
00:15:38,680 --> 00:15:41,110
Okay, so that's what we're going to do.

231
00:15:41,110 --> 00:15:46,348
So what we want to do is say for
each word we're going to come up for

232
00:15:46,348 --> 00:15:52,850
it a vector and that dense vector
is gonna be chosen so that

233
00:15:52,850 --> 00:15:58,690
it'll be good at predicting other words
that appear in the context of this word.

234
00:15:58,690 --> 00:15:59,770
Well how do we do that?

235
00:15:59,770 --> 00:16:03,918
Well, each of those other words will also
have a word that are attached to them and

236
00:16:03,918 --> 00:16:07,090
then we'll be looking at sort
of similarity measures like dot

237
00:16:07,090 --> 00:16:08,981
product between those two vectors.

238
00:16:08,981 --> 00:16:11,867
And we're gonna change
them as well to make it so

239
00:16:11,867 --> 00:16:14,330
that good at being able to be predicted.

240
00:16:14,330 --> 00:16:17,881
So it all kind off gets a little
bit recursive or circular, but

241
00:16:17,881 --> 00:16:21,431
we're gonna come up with this
clever algorithm to do that, so

242
00:16:21,431 --> 00:16:25,650
that words will be able to predict
their context words and vice-versa.

243
00:16:25,650 --> 00:16:30,880
And so I'm gonna go on and
say a little bit more about that.

244
00:16:30,880 --> 00:16:34,480
But let me just underline one bit

245
00:16:34,480 --> 00:16:39,220
of terminology that was
appearing before in the slide.

246
00:16:39,220 --> 00:16:42,359
So we saw two keywords.

247
00:16:42,359 --> 00:16:47,710
One was distributional, which was here.

248
00:16:47,710 --> 00:16:51,880
And then we've had
distributed representations

249
00:16:51,880 --> 00:16:55,440
where we have these dense vectors to
represent the meaning of the words.

250
00:16:55,440 --> 00:16:59,810
Now people tend to
confuse those two words.

251
00:16:59,810 --> 00:17:02,950
And there's sort of two
reasons they confuse them.

252
00:17:02,950 --> 00:17:08,190
One is because they both start with
distribute and so they're kind of similar.

253
00:17:08,190 --> 00:17:16,388
And the second reason people confuse them
is because they very strongly co-occur,.

254
00:17:16,387 --> 00:17:22,069
So that distributed representations and
meaning have almost always,

255
00:17:22,069 --> 00:17:26,159
up until now, been built by
using distributional similarity.

256
00:17:26,160 --> 00:17:31,520
But I did just want people to gather
that these are different notions, right?

257
00:17:31,520 --> 00:17:37,420
So the idea of distributional similarity
is a theory about semantics of word

258
00:17:37,420 --> 00:17:42,640
meaning that you can describe the meaning
of words by as a use theory of meaning,

259
00:17:42,640 --> 00:17:45,380
understanding the context
in which they appear.

260
00:17:45,380 --> 00:17:50,157
So distributional contrasts with,
way back here when I said but

261
00:17:50,157 --> 00:17:53,967
didn't really explain,
denotational, right?

262
00:17:53,967 --> 00:17:57,634
The denotational idea of
word meaning is the meaning

263
00:17:57,634 --> 00:18:02,310
of glasses is the set of pairs of
glasses that are around the place.

264
00:18:02,310 --> 00:18:05,160
That's different from
distributional meaning.

265
00:18:05,160 --> 00:18:10,752
And distributed then contrasts
with our one-hot word vector.

266
00:18:10,752 --> 00:18:15,740
So the one-hot word vectors are localist
representation where you're storing in

267
00:18:15,740 --> 00:18:16,460
one place.

268
00:18:16,460 --> 00:18:19,000
You're saying here is the symbol glasses.

269
00:18:19,000 --> 00:18:23,610
It's stored right here whereas
in distributed representations

270
00:18:23,610 --> 00:18:27,058
we're smearing the meaning of
something over a large vector space.

271
00:18:27,058 --> 00:18:32,950
Okay, so that's part one.

272
00:18:32,950 --> 00:18:38,534
And we're now gonna sorta be heading into
part two, which is what is Word2vec?

273
00:18:38,534 --> 00:18:42,570
Okay, and so
I'll go almost straight into this.

274
00:18:42,570 --> 00:18:47,176
But this is sort of the recipe in
general for what we're doing for

275
00:18:47,176 --> 00:18:49,990
learning neural word embeddings.

276
00:18:49,990 --> 00:18:55,000
So we're gonna define a model
that aims to predict between

277
00:18:55,000 --> 00:19:00,010
a center word and
words that appear in it's context.

278
00:19:00,010 --> 00:19:03,460
Kind of like we are here,
the distributional wording.

279
00:19:03,460 --> 00:19:06,972
And we'll sort of have some,
perhaps probability measure or

280
00:19:06,972 --> 00:19:10,292
predicts the probability of
the context given the words.

281
00:19:10,292 --> 00:19:14,811
And then once we have that we can
have a loss function as to whether

282
00:19:14,811 --> 00:19:17,200
we do that prediction well.

283
00:19:17,200 --> 00:19:21,950
So ideally we'd be able to perfectly
predict the words around the word so

284
00:19:21,950 --> 00:19:27,260
the minus t means the words that aren't
word index t so the words around t.

285
00:19:27,260 --> 00:19:31,260
If we could predict those perfectly
from t we'd have probability one so

286
00:19:31,260 --> 00:19:34,350
we'd have no loss but
normally we can't do that.

287
00:19:34,350 --> 00:19:37,604
And if we give them probability a quarter
then we'll have sort of three quarters

288
00:19:37,604 --> 00:19:38,780
loss or something, right?

289
00:19:38,780 --> 00:19:40,677
So we'll have a loss function and

290
00:19:40,677 --> 00:19:44,270
we'll sort of do that in many
positions in a large corpus.

291
00:19:44,270 --> 00:19:49,270
And so our goal will be to change
the representations of words so

292
00:19:49,270 --> 00:19:51,720
as to minimize our loss.

293
00:19:51,720 --> 00:19:55,670
And at this point sort
of a miracle occurs.

294
00:19:55,670 --> 00:20:00,690
It's sort of surprising, but
true that you can do no more

295
00:20:01,830 --> 00:20:05,130
than set up this kind of
prediction objective.

296
00:20:05,130 --> 00:20:10,370
Make it the job of every words word
vectors to be such that they're

297
00:20:10,370 --> 00:20:15,680
good at predicting their words that
appear in their context or vice versa.

298
00:20:15,680 --> 00:20:20,778
You just have that very simple goal and
you say nothing else about how this is

299
00:20:20,778 --> 00:20:26,880
gonna be achieved, but you just pray and
depend on the magic of deep learning.

300
00:20:26,880 --> 00:20:28,825
And this miracle happens and

301
00:20:28,825 --> 00:20:33,490
outcome these word vectors that
are just amazingly powerful

302
00:20:33,490 --> 00:20:38,480
at representing the meaning of words and
are useful for all sorts of things.

303
00:20:38,480 --> 00:20:43,698
And so that's where we want to get into
more detail and say how that happens.

304
00:20:43,698 --> 00:20:44,577
Okay.

305
00:20:48,986 --> 00:20:54,065
So that representation was
meant to be meaning all words

306
00:20:54,065 --> 00:20:59,090
apart from the wt, yes,
what is this w minus t mean?

307
00:20:59,090 --> 00:21:01,710
I'm actually not gonna use that
notation again in this lecture.

308
00:21:01,710 --> 00:21:06,560
But the w minus t, minus is sometimes
used to mean everything except t.

309
00:21:06,560 --> 00:21:12,470
So wt is my focus word, and w minus
t is in all the words in the context.

310
00:21:14,615 --> 00:21:19,009
Okay, so this idea that you can
learn low dimensional vector

311
00:21:19,009 --> 00:21:23,975
representations is an idea that
has a history in neural networks.

312
00:21:23,975 --> 00:21:26,165
It was certainly present in the 1980s,

313
00:21:26,165 --> 00:21:30,800
parallel distributed processing
era including work by

314
00:21:30,800 --> 00:21:34,760
Rumelhart on learning representations
by back-propagating errors.

315
00:21:34,760 --> 00:21:42,080
It really was demonstrated for
word representations in this pioneering

316
00:21:42,080 --> 00:21:47,660
early paper by Yoshua Bengio in 2003 and
neural probabilistic language model.

317
00:21:47,660 --> 00:21:52,870
I mean, at the time, sort of not so many
people actually paid attention to this

318
00:21:52,870 --> 00:21:58,260
paper, this was sort of before
the deep learning boom started.

319
00:21:58,260 --> 00:22:03,390
But really this was the paper
where the sort of showed

320
00:22:03,390 --> 00:22:08,800
how much value you could get from having
distributed representations of words and

321
00:22:08,800 --> 00:22:11,800
be able to predict other words in context.

322
00:22:11,800 --> 00:22:17,338
But then as things started to take off
that idea was sort of built on and

323
00:22:17,338 --> 00:22:18,207
revived.

324
00:22:18,207 --> 00:22:23,504
So in 2008, Collobert and Weston started
in the sort of modern direction by saying,

325
00:22:23,504 --> 00:22:27,809
well, if we just want good word
representations, we don't even have to

326
00:22:27,809 --> 00:22:31,906
necessarily make a probabilistic
language model that can predict,

327
00:22:31,906 --> 00:22:35,756
we just need to have a way of
learning our word representations.

328
00:22:35,756 --> 00:22:40,054
And that's something that's then being
continued in the model that I'm gonna look

329
00:22:40,054 --> 00:22:42,040
at now, the word2vec model.

330
00:22:42,040 --> 00:22:47,195
That the emphasis of the word2vec model
was how can we build a very simple,

331
00:22:47,195 --> 00:22:53,550
scalable, fast to train model
that we can run over billions

332
00:22:53,550 --> 00:22:57,700
of words of text that will produce
exceedingly good word representations.

333
00:23:00,190 --> 00:23:03,130
Okay, word2vec, here we come.

334
00:23:03,130 --> 00:23:08,440
The basic thing word2vec is trying
to do is use theory of meaning,

335
00:23:08,440 --> 00:23:12,600
predict between every word and
its context words.

336
00:23:12,600 --> 00:23:15,450
Now word2vec is a piece of software,
I mean,

337
00:23:15,450 --> 00:23:19,350
actually inside word2vec it's kind
of a sort of a family of things.

338
00:23:19,350 --> 00:23:24,230
So there are two algorithms inside it for
producing word vectors and

339
00:23:24,230 --> 00:23:28,240
there are two moderately
efficient training methods.

340
00:23:28,240 --> 00:23:32,310
So for this class what I'm
going to do is tell you about

341
00:23:32,310 --> 00:23:36,480
one of the algorithms which
is a skip-gram method and

342
00:23:36,480 --> 00:23:40,100
about neither of the moderately
efficient training algorithms.

343
00:23:40,100 --> 00:23:43,920
Instead I'm gonna tell you about
the hopelessly inefficient training

344
00:23:43,920 --> 00:23:48,990
algorithm but is sort of the conceptual
basis of how this is meant to work and

345
00:23:48,990 --> 00:23:52,640
that the moderately efficient ones,
which I'll mention at the end.

346
00:23:52,640 --> 00:23:55,920
And then what you'll have to do to
actually make this a scalable process

347
00:23:55,920 --> 00:23:57,500
that you can run fast.

348
00:23:57,500 --> 00:24:01,300
And then, today is also the day
when we're handing out assignment

349
00:24:02,400 --> 00:24:07,289
one and Major part of what you
guys get to do in assignment

350
00:24:07,289 --> 00:24:11,682
one is to implement one of
the efficient training algorithms, and

351
00:24:11,682 --> 00:24:16,481
to work through the method one of
those efficient training algorithms.

352
00:24:16,481 --> 00:24:19,770
So this is the picture
of the skip-gram model.

353
00:24:19,770 --> 00:24:24,130
So the idea of the skip-gram model is for

354
00:24:24,130 --> 00:24:30,130
each estimation step,
you're taking one word as the center word.

355
00:24:30,130 --> 00:24:36,100
So that's here, is my word banking and
then what you're going to do

356
00:24:36,100 --> 00:24:42,160
is you're going to try and predict words
in its context out to some window size.

357
00:24:42,160 --> 00:24:46,880
And so, the model is going to define
a probability distribution that

358
00:24:46,880 --> 00:24:52,470
is the probability of a word appearing
in the context given this center word.

359
00:24:52,470 --> 00:24:57,106
And we're going to choose vector
representations of words so

360
00:24:57,106 --> 00:25:01,395
we can try and
maximize that probability distribution.

361
00:25:01,395 --> 00:25:04,085
And the thing that we'll come back to.

362
00:25:04,085 --> 00:25:09,225
But it's important to realize is there's
only one probability distribution,

363
00:25:09,225 --> 00:25:10,175
this model.

364
00:25:10,175 --> 00:25:12,625
It's not that there's
a probability distribution for

365
00:25:12,625 --> 00:25:16,190
the word one to the left and the word
one to the right, and things like that.

366
00:25:16,190 --> 00:25:20,493
We just have one probability
distribution of a context word,

367
00:25:20,493 --> 00:25:24,713
which we'll refer to as the output,
because it's what we,

368
00:25:24,713 --> 00:25:29,866
produces the output, occurring in
the context close to the center word.

369
00:25:29,866 --> 00:25:32,318
Is that clear?

370
00:25:32,318 --> 00:25:35,530
Yeah, okay.

371
00:25:37,070 --> 00:25:42,420
So that's what we kinda wanna do so
we're gonna have a radius m and

372
00:25:42,420 --> 00:25:47,390
then we're going to predict
the surrounding words from sort of

373
00:25:47,390 --> 00:25:52,410
positions m before our center
word to m after our center word.

374
00:25:52,410 --> 00:25:56,540
And we're gonna do that a whole bunch
of times in a whole bunch of places.

375
00:25:56,540 --> 00:26:00,620
And we want to choose word

376
00:26:00,620 --> 00:26:05,870
vectors such as that we're maximizing
the probability of that prediction.

377
00:26:05,870 --> 00:26:14,080
So what our loss function or objective
function is is really this J prime here.

378
00:26:14,080 --> 00:26:18,980
So the J prime is saying we're going to,
so we're going to take a big

379
00:26:18,980 --> 00:26:23,730
long amount of text, we take the whole
of Wikipedia or something like that so

380
00:26:23,730 --> 00:26:28,240
we got big long sequence of words, so
there are words in the context and

381
00:26:28,240 --> 00:26:33,140
real running text, and we're going to
go through each position in the text.

382
00:26:33,140 --> 00:26:37,340
And then, for each position in the text,
we're going to have a window

383
00:26:37,340 --> 00:26:41,720
of size 2m around it,
m words before and m words after it.

384
00:26:41,720 --> 00:26:47,180
And we're going to have a probability
distribution that will give a probability

385
00:26:47,180 --> 00:26:51,400
to a word appearing in
the context of the center word.

386
00:26:52,400 --> 00:26:57,560
And what we'd like to do is set
the parameters of our model so

387
00:26:57,560 --> 00:27:00,170
that these probabilities
of the words that actually

388
00:27:00,170 --> 00:27:04,100
do appear in the context of the center
word are as high as possible.

389
00:27:05,625 --> 00:27:10,895
So the parameters in this model of these
theta here that I show here and here.

390
00:27:10,895 --> 00:27:13,140
After this slide,
I kinda drop the theta over here.

391
00:27:13,140 --> 00:27:17,055
But you can just assumed
that there is this theta.

392
00:27:17,055 --> 00:27:17,555
What is this theta?

393
00:27:17,555 --> 00:27:20,335
What is theta is?

394
00:27:20,335 --> 00:27:24,455
It's going to be the vector
representation of the words.

395
00:27:24,455 --> 00:27:29,770
The only parameters in this model of
the vector representations of each word.

396
00:27:29,770 --> 00:27:34,024
There are no other parameters whatsoever
in this model as you'll see pretty

397
00:27:34,024 --> 00:27:34,630
quickly.

398
00:27:34,630 --> 00:27:39,670
So conceptually this is
our objective function.

399
00:27:39,670 --> 00:27:45,000
We wanna maximize the probability
of this predictions.

400
00:27:45,000 --> 00:27:47,950
In practice, we just slightly tweak that.

401
00:27:47,950 --> 00:27:52,328
Firstly, almost unbearably when
we're working with probabilities and

402
00:27:52,328 --> 00:27:57,059
we want to do maximization, we actually
turn things into log probabilities cuz

403
00:27:57,059 --> 00:27:59,458
then all that products turn into sums and

404
00:27:59,458 --> 00:28:04,140
our math gets a lot easier to work with
and so that's what I've done down here.

405
00:28:09,405 --> 00:28:10,611
Good points.

406
00:28:10,611 --> 00:28:14,115
And the question is, hey, wait a minute
you're cheating, windows size,

407
00:28:14,115 --> 00:28:16,070
isn't that a parameter of the model?

408
00:28:16,070 --> 00:28:18,600
And you are right,
this is the parameter of the model.

409
00:28:18,600 --> 00:28:22,320
So I guess I was a bit loose there.

410
00:28:22,320 --> 00:28:26,100
Actually, it turns out that there are
several hyper parameters of the model, so

411
00:28:26,100 --> 00:28:26,720
I did cheat.

412
00:28:26,720 --> 00:28:31,690
It turns out that there are a few
hyper parameters of the model.

413
00:28:31,690 --> 00:28:35,340
One is Windows sized and it turns out
that we'll come across a couple of

414
00:28:35,340 --> 00:28:37,750
other fudge factors later in the lecture.

415
00:28:37,750 --> 00:28:42,110
And all of those things are hyper
parameters that you could adjust.

416
00:28:42,110 --> 00:28:43,920
But let's just ignore those for
the moment,

417
00:28:43,920 --> 00:28:46,370
let's just assume those are constant.

418
00:28:46,370 --> 00:28:49,150
And given those things
aren't being adjusted,

419
00:28:49,150 --> 00:28:54,990
the only parameters in the model,
the factor representations of the words.

420
00:28:54,990 --> 00:28:59,401
What I'm meaning is that there's
sort of no other probability

421
00:28:59,401 --> 00:29:02,239
distribution with its own parameters.

422
00:29:02,239 --> 00:29:03,370
That's a good point.

423
00:29:03,370 --> 00:29:05,221
I buy that one.

424
00:29:05,221 --> 00:29:11,349
So we've gone to the log probability and
the sums now and,

425
00:29:11,349 --> 00:29:18,230
and then rather than having
the probability of the whole corpus,

426
00:29:18,230 --> 00:29:26,155
we can sort of take the average over
each positions so I've got 1 on T here.

427
00:29:26,155 --> 00:29:32,850
And that's just sort of a making it per
word as sort of a kinda normalization.

428
00:29:32,850 --> 00:29:35,280
So that doesn't affect what's the maximum.

429
00:29:35,280 --> 00:29:39,480
And then, finally,
the machine learning people

430
00:29:39,480 --> 00:29:43,300
really love to minimize things
rather than maximizing things.

431
00:29:43,300 --> 00:29:46,990
And so, you can always swap
between maximizing and minimizing,

432
00:29:46,990 --> 00:29:51,430
when you're in plus minus land, by
putting a minus sign in front of things.

433
00:29:51,430 --> 00:29:55,950
And so, at this point,
we get the negative log likelihood,

434
00:29:55,950 --> 00:29:59,050
the negative log probability
according to our model.

435
00:29:59,050 --> 00:30:03,563
And so, that's what we will be formally

436
00:30:03,563 --> 00:30:08,090
minimizing as our objective function.

437
00:30:08,090 --> 00:30:12,858
So if there were objective function, cost
function, loss function, all the same,

438
00:30:12,858 --> 00:30:17,365
this negative log likelihood criterion
really that means that we're using this

439
00:30:17,365 --> 00:30:21,141
our cross-entropy loss which is
gonna come back to this next week so

440
00:30:21,141 --> 00:30:23,100
I won't really go through it now.

441
00:30:23,100 --> 00:30:26,890
But the trick is since we
have a one hot target,

442
00:30:26,890 --> 00:30:30,130
which is just predict the word
that actually occurred.

443
00:30:30,130 --> 00:30:34,796
Under that criteria the only
thing that's left in cross

444
00:30:34,796 --> 00:30:39,814
entropy loss is the negative
probability of the true class.

445
00:30:39,814 --> 00:30:43,210
Well, how are we gonna actually do this?

446
00:30:43,210 --> 00:30:47,777
How can we make use of
these word vectors to

447
00:30:47,777 --> 00:30:52,381
minimize that negative log likelihood?

448
00:30:52,381 --> 00:30:55,690
Well, the way we're gonna
do it is we're gonna come

449
00:30:55,690 --> 00:31:00,400
with the probably
distribution of context word,

450
00:31:00,400 --> 00:31:06,050
given the center word, which is
constructed out of our word vectors.

451
00:31:06,050 --> 00:31:09,990
And so, this is what our probability
distribution is gonna look like.

452
00:31:09,990 --> 00:31:14,655
So just to make sure we're clear on
the terminology I'm gonna use forward

453
00:31:14,655 --> 00:31:15,415
from here.

454
00:31:15,415 --> 00:31:22,845
So c and o are indices in the space
of the vocabulary, the word types.

455
00:31:22,845 --> 00:31:29,380
So up here, the t and the t plus j, where
in my text there are positions in my text.

456
00:31:29,380 --> 00:31:34,700
Those are sort of words,
763 in words 766 in my text.

457
00:31:34,700 --> 00:31:39,988
But here o and c in my vocabulary
words I have word types and

458
00:31:39,988 --> 00:31:45,730
so I have my p for words 73 and
47 in my vocabulary words.

459
00:31:45,730 --> 00:31:53,140
And so, each word type they're going to
have a vector associated with them so

460
00:31:53,140 --> 00:31:59,410
u o is the vector associated
with context word in index o and

461
00:31:59,410 --> 00:32:04,600
vc is the vector that's
associated with the center word.

462
00:32:04,600 --> 00:32:10,460
And so, how we find this probability
distribution is we're going to use this,

463
00:32:10,460 --> 00:32:16,540
what's called a Softmax form,
where we're taking dot products between

464
00:32:16,540 --> 00:32:22,510
the the two word vectors and then we're
putting them into a Softmax form.

465
00:32:22,510 --> 00:32:25,890
So just to go through that kind
of maximally slowly, right?

466
00:32:25,890 --> 00:32:30,890
So we've got two word vectors and
we're gonna dot product them,

467
00:32:30,890 --> 00:32:33,930
which means that we so
take the corresponding terms and

468
00:32:33,930 --> 00:32:37,490
multiply them together and
sort of sum them all up.

469
00:32:37,490 --> 00:32:42,170
So may adopt product is sort of like
a loose measure of similarity so

470
00:32:42,170 --> 00:32:46,320
the contents of the vectors
are more similar to each other

471
00:32:46,320 --> 00:32:48,386
the number will get bigger.

472
00:32:48,386 --> 00:32:52,790
So that's kind of a similarity
measure through the dot product.

473
00:32:52,790 --> 00:32:56,730
And then once we've worked out
dot products between words

474
00:32:56,730 --> 00:33:00,000
we're then putting it
in this Softmax form.

475
00:33:00,000 --> 00:33:03,454
So this Softmax form is a standard way to

476
00:33:03,454 --> 00:33:07,757
turn numbers into
a probability distribution.

477
00:33:07,757 --> 00:33:12,460
So when we calculate dot products,
they're just numbers, real numbers.

478
00:33:12,460 --> 00:33:14,740
They could be minus 17 or 32.

479
00:33:14,740 --> 00:33:19,990
So we can't directly turn those
into a probability distribution so

480
00:33:19,990 --> 00:33:23,290
an easy thing that we can
do is exponentiate them.

481
00:33:23,290 --> 00:33:27,266
Because if you exponentiate things
that puts them into positive land so

482
00:33:27,266 --> 00:33:29,020
it's all gonna be positive.

483
00:33:29,020 --> 00:33:34,510
And that's a good basis for
having a probability distribution.

484
00:33:34,510 --> 00:33:39,170
And if you have a bunch of numbers that
come from anywhere that are positive and

485
00:33:39,170 --> 00:33:43,190
you want to turn them into a probability
distribution that's proportional to

486
00:33:43,190 --> 00:33:47,260
the size of those numbers,
there's a really easy way to do that.

487
00:33:47,260 --> 00:33:52,110
Which is you sum all the numbers together
and you divide through by the sum and

488
00:33:52,110 --> 00:33:55,430
that then instantly gives you
a probability distribution.

489
00:33:55,430 --> 00:34:00,260
So that's then denominated that is
normalizing to give a probability and so

490
00:34:00,260 --> 00:34:05,450
when you put those together, that then
gives us this form that we're using

491
00:34:05,450 --> 00:34:10,450
as our Softmax form which is now
giving us a probability estimate.

492
00:34:10,449 --> 00:34:13,459
So that's giving us this
probability estimate

493
00:34:13,460 --> 00:34:18,060
here built solely in terms of
the word vector representations.

494
00:34:18,060 --> 00:34:19,630
Is that good?

495
00:34:19,630 --> 00:34:20,130
Yeah.

496
00:34:24,766 --> 00:34:29,525
That is an extremely good question and
I was hoping to delay saying that for

497
00:34:29,525 --> 00:34:33,369
just a minute but you've asked and
so I will say it.

498
00:34:33,370 --> 00:34:41,820
Yes, you might think that one word should
only have one vector representation.

499
00:34:41,820 --> 00:34:47,980
And if you really wanted to you could
do that, but it turns out you can make

500
00:34:47,980 --> 00:34:53,400
the math considerably easier by
saying now actually each word has two

501
00:34:53,400 --> 00:34:57,920
vector representation that has one vector
representation when it synthesis the word.

502
00:34:57,920 --> 00:35:02,050
And it has another vector representation
when it's a context word.

503
00:35:02,050 --> 00:35:04,690
So that's formally what we have here.

504
00:35:04,690 --> 00:35:10,770
So the v is the center word vectors,
and the u are the context word vectors.

505
00:35:10,770 --> 00:35:13,920
And it turns out not only does
that make the math a lot easier,

506
00:35:13,920 --> 00:35:16,870
because the two
representations are separated

507
00:35:16,870 --> 00:35:19,990
when you do optimization rather
than tied to each other.

508
00:35:19,990 --> 00:35:24,160
It's actually in practice empirically
works a little better as well,

509
00:35:24,160 --> 00:35:28,900
so if your life is easier and
better, who would not choose that?

510
00:35:28,900 --> 00:35:31,170
So yes, we have two vectors for each word.

511
00:35:32,290 --> 00:35:33,251
Any other questions?

512
00:35:52,837 --> 00:35:55,696
Yeah, so the question is,
well wait a minute,

513
00:35:55,696 --> 00:35:59,340
you just said this was a way to
make everything positive, but

514
00:35:59,340 --> 00:36:03,627
actually you also simultaneously
screwed with the scale of things a lot.

515
00:36:03,627 --> 00:36:05,280
And that's true, right?

516
00:36:05,280 --> 00:36:09,420
The reason why this is called a Softmax
function is because it's kind of

517
00:36:09,420 --> 00:36:14,260
close to a max function,
because when you exponentiate things,

518
00:36:14,260 --> 00:36:18,325
the big things get way bigger and
so they really dominate.

519
00:36:18,325 --> 00:36:23,805
And so this really sort of blows out
in the direction of a max function,

520
00:36:23,805 --> 00:36:24,945
but not fully.

521
00:36:24,945 --> 00:36:27,055
It's still a sort of a soft thing.

522
00:36:27,055 --> 00:36:30,180
So you might think that
that's a bad thing to do.

523
00:36:30,180 --> 00:36:34,070
Doing things like this is the most
standard underlying a lot of math,

524
00:36:34,070 --> 00:36:37,780
including all those super
common logistic regressions,

525
00:36:37,780 --> 00:36:40,120
you see another class's
way of doing things.

526
00:36:40,120 --> 00:36:41,390
So it's a good way to know,

527
00:36:41,390 --> 00:36:44,030
but people have certainly worked
on a whole bunch of other ways.

528
00:36:44,030 --> 00:36:46,480
And there are reasons that you might
think they're interesting, but

529
00:36:46,480 --> 00:36:48,190
I won't do them now.

530
00:36:48,190 --> 00:36:48,718
Yes?

531
00:37:00,734 --> 00:37:04,610
Yeah, so the question was,
when I'm dealing with the context words,

532
00:37:04,610 --> 00:37:08,050
am I paying attention to where they are or
just their identity?

533
00:37:08,050 --> 00:37:11,730
Yeah, where they are has nothing
to do with it in this model.

534
00:37:11,730 --> 00:37:15,940
It's just, what is the identity of
the word somewhere in the window?

535
00:37:15,940 --> 00:37:19,660
So there's just one
probability distribution and

536
00:37:19,660 --> 00:37:21,710
one representation of the context word.

537
00:37:21,710 --> 00:37:25,335
Now you know, it's not that
that's necessarily a good idea.

538
00:37:25,335 --> 00:37:30,925
There are other models which absolutely
pay attention to position and distance.

539
00:37:30,925 --> 00:37:34,415
And for some purposes,
especially more syntactic

540
00:37:34,415 --> 00:37:38,455
purposes rather than semantic purposes,
that actually helps a lot.

541
00:37:38,455 --> 00:37:43,189
But if you're sort of more interested
in just sort of word meaning,

542
00:37:43,189 --> 00:37:45,847
it turns out that not paying attention

543
00:37:45,847 --> 00:37:50,348
to position actually tends to
help you rather than hurting you.

544
00:37:50,348 --> 00:37:51,269
Yeah.

545
00:38:05,663 --> 00:38:10,610
Yeah, so the question is how, wait
a minute, is there a unique solution here?

546
00:38:10,610 --> 00:38:14,280
Could there be different rotations
that would be equally good?

547
00:38:15,310 --> 00:38:19,880
And the answer is yes, there can be.

548
00:38:19,880 --> 00:38:24,730
I think we should put off discussing
this cuz actually there's a lot to

549
00:38:24,730 --> 00:38:29,770
say about optimization in neural networks,
and there's a lot of exciting new work.

550
00:38:29,770 --> 00:38:34,860
And the one sentence headline is
it's all good news, people spent

551
00:38:34,860 --> 00:38:39,630
years saying that minimal work ought to be
a big problem and it turns out it's not.

552
00:38:39,630 --> 00:38:40,820
It all works.

553
00:38:40,820 --> 00:38:45,760
But I think we better off talking
about that in any more detail.

554
00:38:45,760 --> 00:38:50,990
Okay, so

555
00:38:50,990 --> 00:38:56,660
yeah this is my picture of what the skip
gram model ends up looking like.

556
00:38:56,660 --> 00:38:59,020
It's a bit confusing and hard to read, but

557
00:38:59,020 --> 00:39:01,500
also I've got it thrown
from left to right.

558
00:39:01,500 --> 00:39:05,110
Right, so we have the center
word that's a one hot vector.

559
00:39:06,260 --> 00:39:13,808
We then have a matrix of
the representations of center words.

560
00:39:13,808 --> 00:39:22,700
So if we kind of do a multiplication
of this matrix by that vector.

561
00:39:22,700 --> 00:39:26,970
We just sort of actually select
out the column of the matrix

562
00:39:26,970 --> 00:39:30,670
which is then the representation
of the center word.

563
00:39:31,870 --> 00:39:35,050
Then what we do is we have a second matrix

564
00:39:35,050 --> 00:39:39,660
which stores the representations
of the context words.

565
00:39:39,660 --> 00:39:42,850
And so for each position in the context,

566
00:39:42,850 --> 00:39:46,070
I show three here because
that was confusing enough.

567
00:39:46,070 --> 00:39:50,590
We're going to multiply
the vector by this matrix

568
00:39:51,620 --> 00:39:55,960
which is the context word representations.

569
00:39:55,960 --> 00:40:00,250
And so
we will be picking out sort of the dot

570
00:40:00,250 --> 00:40:04,830
products of the center word
with each context word.

571
00:40:04,830 --> 00:40:07,880
And it's the same matrix for
each position, right?

572
00:40:07,880 --> 00:40:10,766
We only have one context word matrix.

573
00:40:10,766 --> 00:40:12,752
And then these dot products,

574
00:40:12,752 --> 00:40:17,132
we're gonna soft max then turn
into a probability distribution.

575
00:40:17,132 --> 00:40:22,535
And so our model, as a generative model,
is predicting the probability of

576
00:40:22,535 --> 00:40:29,250
each word appearing in the context given
that a certain word is the center word.

577
00:40:29,250 --> 00:40:32,907
And so if we are actually using
it generatively, it would say,

578
00:40:32,907 --> 00:40:35,948
well, the word you should
be using is this one here.

579
00:40:35,948 --> 00:40:41,061
But if there is sort of actual ground
truth as to what was the context word,

580
00:40:41,061 --> 00:40:46,520
we can sort of say, well, the actual
ground truth was this word appeared.

581
00:40:46,520 --> 00:40:50,350
And you gave a probability
estimate of 0.1 to that word.

582
00:40:50,350 --> 00:40:54,185
And so that's the basis, so if you
didn't do a great job at prediction,

583
00:40:54,185 --> 00:40:57,580
then there's going to be some loss, okay?

584
00:40:57,580 --> 00:40:59,780
But that's the picture of our model.

585
00:40:59,780 --> 00:41:03,420
Okay, and so what we wanna do is now learn

586
00:41:04,890 --> 00:41:09,790
parameters, these word vectors,
in such a way that we

587
00:41:09,790 --> 00:41:14,620
do as good a job at prediction
as we possibly can.

588
00:41:16,670 --> 00:41:21,530
And so standardly when we do these things,
what we do

589
00:41:21,530 --> 00:41:26,760
is we take all the parameters in our model
and put them into a big vector theta.

590
00:41:26,760 --> 00:41:31,730
And then we're gonna say we're gonna do
optimization to change those parameters so

591
00:41:31,730 --> 00:41:35,408
as to maximize objective
function of our model.

592
00:41:35,408 --> 00:41:38,990
So what our parameters are is that for

593
00:41:38,990 --> 00:41:43,470
each word, we're going to have
a little d dimensional vector,

594
00:41:43,470 --> 00:41:47,480
when it's a center word and
when it's a context word.

595
00:41:47,480 --> 00:41:50,176
And so
we've got a vocabulary of some size.

596
00:41:50,176 --> 00:41:54,923
So we're gonna have a vector for
aardvark as a context word,

597
00:41:54,923 --> 00:41:57,740
a vector for art as a context word.

598
00:41:57,740 --> 00:42:00,330
We're going to have a vector
of aardvark as a center word,

599
00:42:00,330 --> 00:42:02,240
a vector of art as a center word.

600
00:42:02,240 --> 00:42:06,398
So our vector in total is
gonna be of length 2dV.

601
00:42:06,398 --> 00:42:10,783
There's gonna be a big long vector that
has everything that was in what was shown

602
00:42:10,783 --> 00:42:12,560
in those matrices before.

603
00:42:12,560 --> 00:42:15,630
And that's what we then gonna
be saying about optimizing.

604
00:42:15,630 --> 00:42:19,870
And so after the break, I'm going to be so

605
00:42:19,870 --> 00:42:23,570
going through concretely how
we do that optimization.

606
00:42:23,570 --> 00:42:25,384
But before the break,

607
00:42:25,384 --> 00:42:30,552
we have the intermission with
our special guest, Danqi Chen.

608
00:42:30,552 --> 00:42:32,425
>> Hi, everyone.

609
00:42:32,425 --> 00:42:36,080
I'm Danqi Chen, and
I'm the head TA of this class.

610
00:42:36,080 --> 00:42:39,210
So today I will start our first
research highlight session,

611
00:42:39,210 --> 00:42:42,390
and I will introduce you
a paper from Princeton.

612
00:42:42,390 --> 00:42:46,920
The title is A Simple but Tough-to-beat
Baseline for Sentence Embeddings.

613
00:42:46,920 --> 00:42:50,205
So today we are learning the word
vector representations, so

614
00:42:50,205 --> 00:42:53,750
we hope these vectors can
encode the word meanings.

615
00:42:53,750 --> 00:42:58,405
But our central question in natural
language processing, and also this class,

616
00:42:58,405 --> 00:43:02,924
is that how we could have the vector
representations that encode the meaning of

617
00:43:02,924 --> 00:43:06,159
sentences like,
natural language processing is fun.

618
00:43:08,120 --> 00:43:12,695
So with these sentence representations,
we can compute

619
00:43:12,695 --> 00:43:18,260
the sentence similarity using
the inner product of the two vectors.

620
00:43:18,260 --> 00:43:22,753
So, for example, Mexico wishes to
guarantee citizen's safety, and,

621
00:43:22,753 --> 00:43:25,510
Mexico wishes to avoid more violence.

622
00:43:25,510 --> 00:43:29,898
So we can use the vector
representation to predict these two

623
00:43:29,898 --> 00:43:32,302
sentences are pretty similar.

624
00:43:32,302 --> 00:43:35,970
We can also use this sentence
representation to use as

625
00:43:35,970 --> 00:43:39,935
features to do some sentence
classification task.

626
00:43:39,935 --> 00:43:41,885
For example, sentiment analysis.

627
00:43:41,885 --> 00:43:45,485
So given a sentence like,
natural language processing is fun,

628
00:43:45,485 --> 00:43:49,152
we can put our classifier on top
of the vector representations and

629
00:43:49,152 --> 00:43:51,525
predict if sentiment is positive.

630
00:43:51,525 --> 00:43:54,318
Hopefully this is right, so.

631
00:43:54,318 --> 00:43:58,408
So there are a wide range of
measures that compose word vector

632
00:43:58,408 --> 00:44:02,920
representations into sentence
vector representations.

633
00:44:02,920 --> 00:44:06,164
So the most simple way is
to use the bag-of-words.

634
00:44:06,164 --> 00:44:09,750
So the bag-of-words is just like
the vector representation of

635
00:44:09,750 --> 00:44:11,520
the natural language processing.

636
00:44:11,520 --> 00:44:15,764
It's a average of the three single
word vector representations,

637
00:44:15,764 --> 00:44:18,577
the natural, language, and processing.

638
00:44:18,577 --> 00:44:24,059
Later in this quarter, we'll learn a bunch
of complex models, such as recurrent

639
00:44:24,059 --> 00:44:29,394
neural nets, the recursing neural nets,
and the convolutional neural nets.

640
00:44:29,394 --> 00:44:34,202
But today, for this paper from Princeton,
I want to introduce

641
00:44:34,202 --> 00:44:39,115
that this paper introduces a very
simple unsupervised method.

642
00:44:39,115 --> 00:44:43,787
That is essentially just
a weighted bag-of-words sentence

643
00:44:43,787 --> 00:44:47,930
representation plus remove
some special direction.

644
00:44:47,930 --> 00:44:49,600
I will explain this.

645
00:44:50,690 --> 00:44:52,160
So they have two steps.

646
00:44:52,160 --> 00:44:57,040
So the first step is that just like how
we compute the average of the vector

647
00:44:57,040 --> 00:45:03,450
representations, they also do this,
but each word has a separate weight.

648
00:45:03,450 --> 00:45:05,493
Now here, a is a constant.

649
00:45:05,493 --> 00:45:09,690
And the p(w),
it means the frequency of this word.

650
00:45:09,690 --> 00:45:12,230
So this basically means that

651
00:45:12,230 --> 00:45:15,930
the average representation down
weight the frequent words.

652
00:45:15,930 --> 00:45:19,064
That's the very simple Step 1.

653
00:45:19,064 --> 00:45:23,924
So for the Step 2, after we compute
all of these sentence vector

654
00:45:23,924 --> 00:45:28,965
representations, we compute
the first principal components and

655
00:45:28,965 --> 00:45:34,290
also subtract the projections onto
this first principle component.

656
00:45:35,600 --> 00:45:40,582
You might be familiar with this
if you have ever taken CS 229 and

657
00:45:40,582 --> 00:45:42,043
also learned PCA.

658
00:45:42,043 --> 00:45:43,127
So that's it.

659
00:45:43,127 --> 00:45:44,461
That's their approach.

660
00:45:46,010 --> 00:45:50,378
So in this paper,
they also give a probabilistic

661
00:45:50,378 --> 00:45:55,148
interpretation about why
they want to do this.

662
00:45:55,148 --> 00:46:00,160
So basically, the idea is that given the
sentence representation, the probability

663
00:46:00,160 --> 00:46:05,790
of the limiting or single word, they're
related to the frequency of the word.

664
00:46:05,790 --> 00:46:12,085
And also related to how close the word is
related to this sentence representation.

665
00:46:12,085 --> 00:46:17,043
And also there's a C0 term that
means common discourse vector.

666
00:46:17,043 --> 00:46:19,538
That's usually related to some syntax.

667
00:46:21,774 --> 00:46:24,510
So, finally, the results.

668
00:46:24,510 --> 00:46:29,583
So first, they take context parents
on the sentence similarity and

669
00:46:29,583 --> 00:46:34,567
they show that this simple approach
is much better than the average

670
00:46:34,567 --> 00:46:37,860
of word vectors, all the TFIDF rating, and

671
00:46:37,860 --> 00:46:43,020
also all the performance of
other sophisticated models.

672
00:46:43,020 --> 00:46:47,429
And also for some supervised tasks
like sentence classification,

673
00:46:47,429 --> 00:46:52,390
they're also doing pretty well,
like the entailment and sentiment task.

674
00:46:52,390 --> 00:46:54,392
So that's it, thanks.

675
00:46:54,392 --> 00:46:55,790
>> Thank you.

676
00:46:55,790 --> 00:47:00,581
[LAUGH]
>> [APPLAUSE]

677
00:47:00,581 --> 00:47:08,829
>> Okay, Okay,

678
00:47:08,829 --> 00:47:13,662
so, and we'll go back from there.

679
00:47:17,970 --> 00:47:23,985
All right, so now we're wanting to sort
of actually work through our model.

680
00:47:23,985 --> 00:47:26,860
So this is what we had, right?

681
00:47:26,860 --> 00:47:33,485
We had our objective function where we
wanna minimize negative log likelihood.

682
00:47:33,485 --> 00:47:38,438
And this is the form of the probability
distribution up there, where we have these

683
00:47:38,438 --> 00:47:44,663
sort of word vectors with both center
word vectors and context word vectors.

684
00:47:44,663 --> 00:47:50,157
And the idea is we want to change
our parameters, these vectors, so

685
00:47:50,157 --> 00:47:56,637
as to minimize the negative log likelihood
item, maximize the probability we predict.

686
00:47:56,637 --> 00:48:00,667
So if that's what we want to do,

687
00:48:00,667 --> 00:48:06,057
how can we work out how
to change our parameters?

688
00:48:11,254 --> 00:48:13,951
Gradient, yes,
we're gonna use the gradient.

689
00:48:13,951 --> 00:48:18,880
So, what we're gonna have to do
at this point is to start to do

690
00:48:18,880 --> 00:48:24,160
some calculus to see how
we can change the numbers.

691
00:48:24,160 --> 00:48:29,710
So precisely, what we'll going
to want to do is to say, well,

692
00:48:29,710 --> 00:48:36,600
we have this term for
working out log probabilities.

693
00:48:36,600 --> 00:48:44,366
So, we have the log of the probability
of the word t plus j word t.

694
00:48:44,366 --> 00:48:46,010
Well, what is the form of that?

695
00:48:46,010 --> 00:48:47,710
Well, we've got it right here.

696
00:48:47,710 --> 00:48:52,800
So, we have the log of v
maybe I can save a line.

697
00:48:54,020 --> 00:49:00,050
We've got this log of this.

698
00:49:00,050 --> 00:49:06,840
And then, what we're gonna want to do is
that we're going to want to change this so

699
00:49:06,840 --> 00:49:11,470
that we have, I'm sorry,
minimized in this objective.

700
00:49:11,470 --> 00:49:15,790
So, let's suppose we sort of
look at these center vectors.

701
00:49:15,790 --> 00:49:21,137
So, what we're gonna want to do is start
working out the partial derivatives

702
00:49:21,137 --> 00:49:26,260
of this with respect to the center
vector which is then, going to give us,

703
00:49:26,260 --> 00:49:32,790
how we can go about working out,
in which way to change this vector

704
00:49:34,570 --> 00:49:38,240
to minimize our objective function.

705
00:49:38,240 --> 00:49:40,590
Okay, so, we want to deal with this.

706
00:49:41,610 --> 00:49:44,720
So, what's the first thing we can
do with that to make it simpler?

707
00:49:47,810 --> 00:49:49,840
Subtraction, yeah.

708
00:49:49,840 --> 00:49:55,855
So, this is a log of a division so, we can
turn that into a log of a subtraction,

709
00:49:55,855 --> 00:50:00,340
and then, we can do the partial
derivatives separately.

710
00:50:00,340 --> 00:50:05,749
So, we have the derivative

711
00:50:05,749 --> 00:50:11,034
with Vc of the log of the exp of

712
00:50:11,034 --> 00:50:16,563
u0^T vc and then, we've got

713
00:50:16,563 --> 00:50:21,734
minus the log of the sum of w

714
00:50:21,734 --> 00:50:27,165
equals 1 to V of exp of u w^T vc.

715
00:50:27,165 --> 00:50:32,041
And at that point,
we can separate it into two pieces, right,

716
00:50:32,041 --> 00:50:37,760
cuz when there's addition or
subtraction we can do them separately.

717
00:50:37,760 --> 00:50:42,254
So, we can do this piece 1 and
we can do the,

718
00:50:42,254 --> 00:50:47,492
work out the partial
derivatives of this piece 2.

719
00:50:47,492 --> 00:50:52,465
So, piece 1 looks kind of easy so,
let's start here.

720
00:50:52,465 --> 00:50:55,065
So, what's the first thing I
should do to make this simpler?

721
00:50:57,490 --> 00:51:00,179
Easy question.

722
00:51:01,850 --> 00:51:08,621
Cancel some things out, log and x inverses
of each other so, they can just go away.

723
00:51:08,621 --> 00:51:13,275
So, for 1,
we can say that this is going to be

724
00:51:13,275 --> 00:51:18,810
the partial derivative with
respect to Vc of u0^T vc.

725
00:51:18,810 --> 00:51:25,848
Okay, that's looking kind of simpler so,

726
00:51:25,848 --> 00:51:30,793
what is the partial derivative

727
00:51:30,793 --> 00:51:35,180
of this with respect to vc?

728
00:51:35,180 --> 00:51:38,640
u0, so, this just comes out as u0.

729
00:51:40,478 --> 00:51:47,370
Okay, and so, I mean, effectively, this is
the kind of level of calculus that you're

730
00:51:47,370 --> 00:51:52,850
gonna have to be able to do to be okay on
assignment one that's coming out today.

731
00:51:52,850 --> 00:51:58,350
So, it's nothing that life threatening,
hopefully, you've seen this before.

732
00:51:58,350 --> 00:52:04,830
But nevertheless, we are here using
calculus with vectors, right?

733
00:52:04,830 --> 00:52:09,730
So, vc here is not just a single number,
it's a whole vector.

734
00:52:09,730 --> 00:52:16,550
So, that's sort of the Math 51,
CME 100 kind of content.

735
00:52:16,550 --> 00:52:21,860
Now, if you want to,
you can pull it all apart.

736
00:52:21,860 --> 00:52:26,564
And you can work out
the partial derivative

737
00:52:26,564 --> 00:52:30,618
with respect to Vc, some index, k.

738
00:52:30,618 --> 00:52:33,301
And then,

739
00:52:33,301 --> 00:52:38,668
you could have this as

740
00:52:38,668 --> 00:52:44,032
the sum of l = 1 to d of

741
00:52:44,032 --> 00:52:49,112
(u0)l (Vc)l.

742
00:52:49,112 --> 00:52:55,032
And what will happen then is if you're
working out of with respect to only one

743
00:52:55,032 --> 00:53:01,610
index, then, all of these terms will go
away apart from the one where k equals l.

744
00:53:01,610 --> 00:53:09,720
And you'll sort of end up with
that being the (uo)k term.

745
00:53:09,720 --> 00:53:14,745
And I mean, if things get confusing and
complicated, I think it can actually,

746
00:53:14,745 --> 00:53:19,695
and your brain is small like mine, it can
actually be useful to sort of go down to

747
00:53:19,695 --> 00:53:24,570
the level of working it out with real
numbers and actually have all the indices

748
00:53:24,570 --> 00:53:28,750
there and you can absolutely do that and
it comes out the same.

749
00:53:28,750 --> 00:53:32,750
But a lot of the time it's sort
of convenient if we can just

750
00:53:32,750 --> 00:53:37,400
stay at this vector level and
work out vector derivatives, okay.

751
00:53:37,400 --> 00:53:41,110
So, now, this was the easy part and

752
00:53:41,110 --> 00:53:44,760
we've got it right there and
we'll come back to that, okay.

753
00:53:44,760 --> 00:53:49,355
So then, the trickier part is we then,
go on to number 2.

754
00:53:52,675 --> 00:53:58,127
So now, if we just ignore the minus

755
00:53:58,127 --> 00:54:02,451
sign for a little bit, so,

756
00:54:02,451 --> 00:54:07,715
we'll subtract it afterwards,

757
00:54:07,715 --> 00:54:14,107
we've then got the partial derivatives

758
00:54:14,107 --> 00:54:19,935
with respect to vc of the log of the sum

759
00:54:19,935 --> 00:54:26,160
from w = 1 to v of the exp of uw^T vc,
okay.

760
00:54:26,160 --> 00:54:28,870
Well, how can we make
progress with this half?

761
00:54:39,296 --> 00:54:44,252
Yeah, so that's right,
before you're going to do that?

762
00:54:44,252 --> 00:54:49,966
The chain rule, okay, so, our key tool
that we need to know how to use and

763
00:54:49,966 --> 00:54:55,180
we'll just use everywhere
is the chain rule, right?

764
00:54:55,180 --> 00:55:00,530
So, neural net people talk all
the time about backpropagation,

765
00:55:00,530 --> 00:55:07,380
it turns out that backpropagation
is nothing more than the chain rule

766
00:55:07,380 --> 00:55:12,560
with some efficient storage
of partial quantities so

767
00:55:12,560 --> 00:55:16,980
that you don't keep on calculating
the same quantity over and over again.

768
00:55:16,980 --> 00:55:20,010
So, it's sort of like chain
rule with memorization,

769
00:55:20,010 --> 00:55:22,990
that is the backpropagation algorithm.

770
00:55:22,990 --> 00:55:30,080
So, now, key tool is the chain rule so,
what is the chain rule?

771
00:55:30,080 --> 00:55:34,180
So, within saying, okay, well,

772
00:55:34,180 --> 00:55:38,968
what overall are we going to have
is some function where we're taking

773
00:55:38,968 --> 00:55:45,290
f(g(u)) of something.

774
00:55:45,290 --> 00:55:49,630
And so, we have this inside part z and so,

775
00:55:49,630 --> 00:55:54,720
what we're going to be doing is that
we're going to be taking the derivative

776
00:55:54,720 --> 00:56:01,610
of the outside part then,
with the value of the inside.

777
00:56:01,610 --> 00:56:06,230
And then, we're gonna be taking
the derivative of the inside part So for

778
00:56:06,230 --> 00:56:11,490
this here, so the outside part,
here's our F.

779
00:56:11,490 --> 00:56:14,430
And then here's our inside part Z.

780
00:56:14,430 --> 00:56:19,090
So the outside part is F,
which is a log function.

781
00:56:19,090 --> 00:56:23,444
And so the derivative of a log
function is the one on X function.

782
00:56:23,444 --> 00:56:29,276
So that we're then gonna be having

783
00:56:29,276 --> 00:56:34,301
that this is 1 over the sum of w

784
00:56:34,301 --> 00:56:39,542
equals 1 to V of the exp of uw^T vc.

785
00:56:39,542 --> 00:56:45,257
And then we're going to be multiplying
it by, what do we get over there.

786
00:56:54,407 --> 00:57:00,499
So we get the partial
derivative with respect to

787
00:57:06,824 --> 00:57:10,567
With respect to vc,

788
00:57:10,567 --> 00:57:17,143
of This inside part.

789
00:57:17,143 --> 00:57:23,130
The sum of, and it's a little trickier.

790
00:57:23,130 --> 00:57:25,930
We really need to be careful of indices so

791
00:57:25,930 --> 00:57:32,020
we're gonna get in the bad mess if
we have W here, and we reuse W here.

792
00:57:32,020 --> 00:57:34,505
We really need to change
it into something else.

793
00:57:34,505 --> 00:57:37,448
So we're gonna have X equals 1 to V.

794
00:57:37,448 --> 00:57:45,629
And then we've got the exp of UX,

795
00:57:45,629 --> 00:57:49,900
transpose VC.

796
00:57:49,900 --> 00:57:52,640
So that's made a little bit of progress.

797
00:57:52,640 --> 00:57:56,320
We want to make a bit more progress here.

798
00:57:56,320 --> 00:57:57,703
So what's the next thing we're gonna do.

799
00:58:04,118 --> 00:58:06,570
Distribute the derivative.

800
00:58:06,570 --> 00:58:08,350
This is just adding some stuff.

801
00:58:09,930 --> 00:58:15,750
We can do the same trick of we can do
each part of the derivative separately.

802
00:58:15,750 --> 00:58:20,469
So X equals 1 to big V of
the partial derivative

803
00:58:20,469 --> 00:58:24,598
with respect to VC of the exp of ux^T vc.

804
00:58:24,598 --> 00:58:31,472
Okay, now we wanna keep
going What can we do next.

805
00:58:33,923 --> 00:58:35,460
The chain rule again.

806
00:58:36,580 --> 00:58:40,280
This is also the form of here's our F and
here's our

807
00:58:40,280 --> 00:58:45,580
inner values V which is in
turn sort of a function.

808
00:58:45,580 --> 00:58:50,200
Yeah, so we can apply the chain
rule a second time and

809
00:58:50,200 --> 00:58:55,790
so we need the derivative of X.

810
00:58:55,790 --> 00:58:57,212
What's the derivative of X.

811
00:58:57,212 --> 00:59:02,350
X, so this part here is gonna be staying.

812
00:59:02,350 --> 00:59:06,990
The sum of X equals 1 to V
of the partial derivative.

813
00:59:06,990 --> 00:59:08,440
Hold on no.

814
00:59:08,440 --> 00:59:10,520
Not that one, moving that inside.

815
00:59:10,520 --> 00:59:18,461
So it's still exp at its value of UX T VC.

816
00:59:18,461 --> 00:59:24,203
And then we're having the partial

817
00:59:24,203 --> 00:59:30,941
derivative with respect to VC of UXT VC.

818
00:59:30,941 --> 00:59:33,460
And then we've got a bit
more progress to make.

819
00:59:33,460 --> 00:59:36,980
So we now need to work out what this is.

820
00:59:36,980 --> 00:59:37,667
So what's that.

821
00:59:40,102 --> 00:59:43,356
Right, so
that's the same as sort of back over here.

822
00:59:43,356 --> 00:59:49,384
At this point this is just going to be,
that' s coming out as UX.

823
00:59:49,384 --> 00:59:54,939
And here we still have the sum

824
00:59:54,939 --> 01:00:01,670
of X equals 1 to V of the X of UX T VC.

825
01:00:01,670 --> 01:00:08,630
So at this point we kind of wanna
put this together with that.

826
01:00:08,630 --> 01:00:11,540
Cuz we're still, I stopped writing that.

827
01:00:11,540 --> 01:00:15,860
But we have this one over

828
01:00:15,860 --> 01:00:20,612
the sum of W equals 1 to V of

829
01:00:20,612 --> 01:00:26,041
the exp of UW, transpose VC.

830
01:00:26,041 --> 01:00:34,615
Can we put those things together
in a way that makes it prettier.

831
01:00:50,406 --> 01:00:54,492
So I can move this inside this sum.

832
01:00:54,492 --> 01:01:00,933
Cuz this is just the sort of number that's
a multiplier that's distributed through.

833
01:01:00,933 --> 01:01:05,928
And in particular when I do that,
I can start to sort of

834
01:01:05,928 --> 01:01:10,701
notice this interesting
thing that I'm going to be

835
01:01:10,701 --> 01:01:15,807
reconstructing a form that
looks very like this form.

836
01:01:15,807 --> 01:01:17,881
Sorry, leaving this part up aside.

837
01:01:17,881 --> 01:01:23,820
It looks very like the Softmax
form that I started off with.

838
01:01:23,820 --> 01:01:28,901
And so I can then be saying that

839
01:01:28,901 --> 01:01:33,982
this is the sum from X equals 1

840
01:01:33,982 --> 01:01:39,265
to V of the exp of UX transpose VC

841
01:01:39,265 --> 01:01:44,720
over the sum of W equals 1 to V.

842
01:01:44,720 --> 01:01:50,940
So this is where it's important that I
have X and W with different variables

843
01:01:50,940 --> 01:01:57,434
of the X of U W transpose VC times U of X.

844
01:01:59,560 --> 01:02:03,740
And so well, at that point,
that's kind of interesting cuz,

845
01:02:03,740 --> 01:02:09,430
this is kind of exactly the form
that I started of with,

846
01:02:09,430 --> 01:02:13,160
for my softmax probability distribution.

847
01:02:13,160 --> 01:02:15,113
So what we're doing is we.

848
01:02:19,840 --> 01:02:26,779
What we're doing is that that part is then

849
01:02:26,779 --> 01:02:32,133
being the sum over X equals one to

850
01:02:32,133 --> 01:02:38,096
V of the probability of [INAUDIBLE].

851
01:02:38,096 --> 01:02:39,873
It was wait.

852
01:02:39,873 --> 01:02:44,176
The probability of O given

853
01:02:44,176 --> 01:02:50,600
the probability of X given C times UX.

854
01:02:50,600 --> 01:02:54,230
So that's what we're getting
from the denominator.

855
01:02:54,230 --> 01:02:56,940
And then we still had the numerator.

856
01:02:56,940 --> 01:02:58,800
The numerator was U zero.

857
01:03:00,220 --> 01:03:07,369
What we have here is our
final form is U0 minus that.

858
01:03:07,369 --> 01:03:12,460
And if you look at this a bit
it's sort of a form that you

859
01:03:12,460 --> 01:03:17,870
always get from these
softmax style formulations.

860
01:03:17,870 --> 01:03:19,980
So this is what we observed.

861
01:03:19,980 --> 01:03:25,720
There was the actual output
context word appeared.

862
01:03:25,720 --> 01:03:28,960
And this has the form of an expectation.

863
01:03:28,960 --> 01:03:31,330
So what we're doing is right here.

864
01:03:31,330 --> 01:03:35,350
We're calculating expectation
though we're working out

865
01:03:35,350 --> 01:03:39,900
the probability of every possible
word appearing in the context, and

866
01:03:39,900 --> 01:03:44,800
based on that probability we get
taking that much of that UX.

867
01:03:44,800 --> 01:03:49,210
So this is in some,
this is the expectation vector.

868
01:03:49,210 --> 01:03:52,710
It's the average over all
the possible context vectors,

869
01:03:52,710 --> 01:03:54,640
weighted by their
likelihood of occurrence.

870
01:03:56,880 --> 01:03:59,610
That's the form of our derivative.

871
01:03:59,610 --> 01:04:05,620
What we're going to want to be doing is
changing the parameters in our model.

872
01:04:05,620 --> 01:04:10,743
In such a way that these become
equal cause that's when we're

873
01:04:10,743 --> 01:04:15,485
then finding the maximum and
minimum for us to minimize.

874
01:04:15,485 --> 01:04:21,860
[INAUDIBLE] Okay and so that gives
us the derivatives in that model.

875
01:04:21,860 --> 01:04:25,490
Does that make sense?

876
01:04:25,490 --> 01:04:27,548
Yeah, that's gonna be question.

877
01:04:27,548 --> 01:04:28,705
Anyway, so

878
01:04:28,705 --> 01:04:34,280
precisely doing things like this is what
will expect you to do for assignment one.

879
01:04:34,280 --> 01:04:37,610
And I'll take the question, but
let me just mention one point.

880
01:04:37,610 --> 01:04:41,520
So in this case,
I've only done this for the VC,

881
01:04:41,520 --> 01:04:46,420
the center vectors.

882
01:04:46,420 --> 01:04:49,360
We do this to every
parameter of the model.

883
01:04:49,360 --> 01:04:53,650
In this model, our only other
parameters are the context vectors.

884
01:04:53,650 --> 01:04:55,880
We're also gonna do it for those.

885
01:04:55,880 --> 01:04:59,280
It's very similar cuz if you look
at the form of the equation,

886
01:04:59,280 --> 01:05:01,930
there's a certain
symmetry between the two.

887
01:05:01,930 --> 01:05:05,400
But we're gonna do it for that as well but
I'm not gonna do it here.

888
01:05:05,400 --> 01:05:07,500
That's left to you guys.

889
01:05:07,500 --> 01:05:08,000
Question.

890
01:05:09,220 --> 01:05:15,298
Yeah.
>> [INAUDIBLE]

891
01:05:24,580 --> 01:05:25,623
>> From here to here.

892
01:05:25,623 --> 01:05:26,880
Okay.

893
01:05:26,880 --> 01:05:27,380
So.

894
01:05:28,450 --> 01:05:32,550
So, right, so this is a sum right?

895
01:05:32,550 --> 01:05:36,190
And this is just the number
at the end of the day.

896
01:05:36,190 --> 01:05:41,790
So I can divide every term in
this sum through by that number.

897
01:05:41,790 --> 01:05:43,080
So that's what I'm doing.

898
01:05:43,080 --> 01:05:48,500
So now I've got my sum with every term
in that divided through by this number.

899
01:05:48,500 --> 01:05:54,080
And then I say, wait a minute,
the form of this piece here

900
01:05:54,080 --> 01:05:58,530
is precisely my softmax
probably distribution,

901
01:05:58,530 --> 01:06:02,210
where this is the probability
of x given C.

902
01:06:02,210 --> 01:06:06,890
And so then I'm just rewriting
it as probability of x given c.

903
01:06:06,890 --> 01:06:10,310
Where that is meaning,
I kind of did double duty here.

904
01:06:10,310 --> 01:06:14,920
But that's sort of meaning that you're
using this probability of x given c

905
01:06:14,920 --> 01:06:16,565
using this probability form.

906
01:06:16,565 --> 01:06:22,200
>> [INAUDIBLE]

907
01:06:26,367 --> 01:06:27,019
>> Yeah,

908
01:06:27,019 --> 01:06:32,905
the probability that x occurs as
a context word of center word c.

909
01:06:32,905 --> 01:06:36,697
>> [INAUDIBLE]
>> Well,

910
01:06:36,697 --> 01:06:39,673
we've just assumed some
fixed window size M.

911
01:06:39,673 --> 01:06:44,735
So maybe our window size is five and so
we're considering sort of ten words,

912
01:06:44,735 --> 01:06:47,200
five to the left, five to the right.

913
01:06:48,580 --> 01:06:52,470
So that's a hypergrameter,
and that stuff's nowhere.

914
01:06:52,470 --> 01:06:55,513
We're not dealing with that, we just
assume that God's fixed that for us.

915
01:06:59,506 --> 01:07:02,371
The problem, so
it's done at each position.

916
01:07:02,371 --> 01:07:09,513
So for any position, and
all of them are treated equivalently,

917
01:07:09,513 --> 01:07:14,539
for any position,
the probability that word

918
01:07:14,539 --> 01:07:19,565
x is the word that occurs
within this window at

919
01:07:19,565 --> 01:07:24,475
any position given
the center word was of C.

920
01:07:26,452 --> 01:07:27,509
Yeah?

921
01:07:27,509 --> 01:07:30,943
>> [INAUDIBLE]

922
01:07:40,475 --> 01:07:43,029
>> All right, so the question is,

923
01:07:43,029 --> 01:07:46,720
why do we choose the dot
product as our basis for

924
01:07:46,720 --> 01:07:50,138
coming up with this probability measure?

925
01:07:50,138 --> 01:07:58,109
And you know I think the answer
is there's no necessary reason,

926
01:07:58,109 --> 01:08:02,537
that there are clearly other things

927
01:08:02,537 --> 01:08:07,620
that you could have done and might do.

928
01:08:07,620 --> 01:08:12,010
On the other hand,
I kind of think in terms of

929
01:08:13,030 --> 01:08:20,290
Vector Algebra it's sort of the most
obvious and simple thing to do.

930
01:08:20,290 --> 01:08:28,420
Because it's sort of a measure of
the relatedness and similarity.

931
01:08:28,420 --> 01:08:32,520
I mean I sort of said loosely it was
a measure of similarity between vectors.

932
01:08:32,520 --> 01:08:37,200
Someone could have called me on that
because If you say, well wait a minute.

933
01:08:37,200 --> 01:08:41,380
If you don't control for
the scale of the vectors,

934
01:08:41,380 --> 01:08:44,720
you can make that number as big
as you want, and that is true.

935
01:08:44,720 --> 01:08:50,030
So really the common measure of similarity
between vectors is the cosine measure.

936
01:08:50,029 --> 01:08:53,369
Where what you do is in the numerator.

937
01:08:53,370 --> 01:08:58,170
You take a dot product and then you divide
through by the length of the vectors.

938
01:08:58,170 --> 01:08:59,970
So you've got scale and variance and

939
01:08:59,970 --> 01:09:02,054
you can't just cheat by
making the vectors bigger.

940
01:09:02,054 --> 01:09:08,400
And so, that's a bigger,
better measure of similarity.

941
01:09:08,399 --> 01:09:12,389
But to do that you have to
do a whole lot more math and

942
01:09:12,390 --> 01:09:15,760
it's not actually necessary here
because since you're sort of

943
01:09:15,760 --> 01:09:18,990
predicting every word
against every other word.

944
01:09:18,990 --> 01:09:22,812
If you sort of made one
vector very big to try and

945
01:09:22,812 --> 01:09:26,547
make some probability
of word k being large.

946
01:09:26,546 --> 01:09:30,176
Well the consequence would be it would
make the probability of every other word

947
01:09:30,176 --> 01:09:31,001
be large as well.

948
01:09:31,002 --> 01:09:34,000
So you kind of can't cheat
by lengthening the vectors.

949
01:09:34,000 --> 01:09:38,396
And therefore you can get away with
just using the dot product as a kind of

950
01:09:38,395 --> 01:09:39,869
a similarity measure.

951
01:09:39,870 --> 01:09:41,799
Does that sort of satisfy?

952
01:09:56,556 --> 01:09:58,487
So yes.

953
01:09:58,487 --> 01:10:01,430
I mean, it's not necessary, right?

954
01:10:01,430 --> 01:10:04,430
And if we were going to argue,
you could sort of argue with me and

955
01:10:04,430 --> 01:10:08,950
say no look, this is crazy,
because by construction,

956
01:10:08,950 --> 01:10:13,869
this means the most likely word to appear
in the context of a word is itself.

957
01:10:15,520 --> 01:10:17,715
That doesn't seem like a good result,

958
01:10:17,715 --> 01:10:21,399
[LAUGH] because presumably
different words occur.

959
01:10:21,399 --> 01:10:28,320
And you could then go from there and say
well no let's do something more complex.

960
01:10:28,320 --> 01:10:32,110
Why don't we put a matrix to mediate
between the two vectors to express what

961
01:10:32,110 --> 01:10:39,420
appears in the context of each other,
it turns out you don't need to.

962
01:10:39,420 --> 01:10:44,420
Now one thing of course is since we
have different representations for

963
01:10:44,420 --> 01:10:49,400
the context and center word vectors, it's
not necessarily true that the same word

964
01:10:49,400 --> 01:10:53,740
would be highest because there're
two different representations.

965
01:10:53,740 --> 01:10:57,410
But in practice they often have a lot
of similarity between themselves not

966
01:10:57,410 --> 01:11:00,210
really that that's the reason.

967
01:11:00,210 --> 01:11:04,800
It's more that it's sort
of works out pretty well.

968
01:11:04,800 --> 01:11:07,340
Because although it is true
that you're not likely to

969
01:11:07,340 --> 01:11:10,020
get exactly the same word in the context,

970
01:11:10,020 --> 01:11:13,800
you're actually very likely to get words
that are pretty similar in meaning.

971
01:11:13,800 --> 01:11:18,640
And are strongly associated and when
those words appear as the center word,

972
01:11:18,640 --> 01:11:22,290
you're likely to get your
first word as a context word.

973
01:11:22,290 --> 01:11:25,340
And so at a sort of a macro level,
you are actually

974
01:11:25,340 --> 01:11:28,540
getting this effect that the same
words are appearing on both sides.

975
01:11:30,370 --> 01:11:32,390
More questions, yeah,
there are two of them.

976
01:11:32,390 --> 01:11:32,950
I don't know.

977
01:11:32,950 --> 01:11:34,455
Do I do the behind person first and
then the in front person?

978
01:11:34,455 --> 01:11:35,120
[LAUGH]

979
01:11:52,809 --> 01:11:54,250
So I haven't yet done gradient descent.

980
01:11:54,250 --> 01:11:58,730
And maybe I should do that in a minute and
I will see try then.

981
01:11:58,730 --> 01:12:00,460
Okay?
>> [INAUDIBLE]

982
01:12:00,460 --> 01:12:00,960
>> Yeah

983
01:12:11,612 --> 01:12:12,961
>> So that truth is well,

984
01:12:12,961 --> 01:12:15,960
we've just clicked to
the huge amount text.

985
01:12:15,960 --> 01:12:20,189
So if our word at any position, we know
what are the five words to the left and

986
01:12:20,189 --> 01:12:23,027
the five words to the right and
that's the truth.

987
01:12:23,027 --> 01:12:26,581
And so we're actually giving some
probability estimate to every

988
01:12:26,581 --> 01:12:28,454
word appearing in that context and

989
01:12:28,454 --> 01:12:32,472
we can say, well, actually the word
that appeared there was household.

990
01:12:32,472 --> 01:12:36,590
What probability did you give to that and
there's some answer.

991
01:12:36,590 --> 01:12:38,830
And so, that's our truth.

992
01:12:38,830 --> 01:12:43,520
Time is running out, so maybe I'd sort
of just better say a little bit more

993
01:12:43,520 --> 01:12:48,300
before we finish which is sort of
starting to this optimization.

994
01:12:48,300 --> 01:12:53,274
So this is giving us our derivatives,
we then want to use our

995
01:12:53,274 --> 01:12:57,664
derivatives to be able to
work out our word vectors.

996
01:12:57,664 --> 01:13:03,214
And I mean, I'm gonna spend
a super short amount time on this,

997
01:13:03,214 --> 01:13:08,055
the hope is through 221,
229 or similar class.

998
01:13:08,055 --> 01:13:15,457
You've seen a little bit of optimization
and you've seen some gradient descent.

999
01:13:15,457 --> 01:13:18,195
And so, this is just a very quick review.

1000
01:13:18,195 --> 01:13:22,951
So the idea is once we have gradient
set at point x that if what we

1001
01:13:22,951 --> 01:13:27,250
do is we subtract off a little
fraction of the gradient,

1002
01:13:27,250 --> 01:13:31,010
that will move us downhill
towards the minimum.

1003
01:13:31,010 --> 01:13:36,067
And so if we then calculate the gradient
there again and subtract off

1004
01:13:36,067 --> 01:13:42,305
a little fraction of it, we'll sort of
start walking down towards the minimum.

1005
01:13:42,305 --> 01:13:46,187
And so,
that's the algorithm of gradient descent.

1006
01:13:46,187 --> 01:13:51,838
So once we have an objective function and
we have the derivatives of the objective

1007
01:13:51,838 --> 01:13:56,741
function with respect to all of
the parameters, our gradient descent

1008
01:13:56,741 --> 01:14:02,335
algorithm would be to say,
you've got some current parameter values.

1009
01:14:02,335 --> 01:14:05,082
We've worked out the gradient
at that position.

1010
01:14:05,082 --> 01:14:09,007
We subtract off a little
fraction of that and

1011
01:14:09,007 --> 01:14:14,204
that will give us new parameter
values which we will expect

1012
01:14:14,204 --> 01:14:21,660
to be give us a lower objective value,
and we'll walk towards the minimum.

1013
01:14:21,660 --> 01:14:25,720
And in general, that is true and
that will work.

1014
01:14:28,850 --> 01:14:31,551
So then, to write that up as Python code,

1015
01:14:31,551 --> 01:14:36,278
it's really sort of super simple that
you just go in this while true loop.

1016
01:14:36,278 --> 01:14:41,048
You have to have some stopping condition
actually where you evaluating the gradient

1017
01:14:41,048 --> 01:14:45,889
of given your objective function, your
corpus and your current parameters, so

1018
01:14:45,889 --> 01:14:50,659
you have the theta grad and then you're
sort of subtracting a little fraction of

1019
01:14:50,659 --> 01:14:55,960
the theta grad after the current
parameters and then you just repeat over.

1020
01:14:55,960 --> 01:15:00,884
And so the picture is, so the red lines
that are sort of the contour lines of

1021
01:15:00,884 --> 01:15:03,435
the value of the objective function.

1022
01:15:03,435 --> 01:15:07,993
And so what you do is when you
calculate the gradient, it's giving you

1023
01:15:07,993 --> 01:15:12,784
the direction of the steepest descent and
you walk a little bit each time in

1024
01:15:12,784 --> 01:15:18,145
that direction and you will hopefully
walk smoothly towards the minimum.

1025
01:15:18,145 --> 01:15:22,509
Now the reason that might not work is
if you actually take a first step and

1026
01:15:22,509 --> 01:15:26,900
you go from here to over there,
you've greatly overshot the minimum.

1027
01:15:26,900 --> 01:15:31,822
So, it's important that alpha be small
enough that you're still walking calmly

1028
01:15:31,822 --> 01:15:35,082
down towards the minimum and
then all work.

1029
01:15:35,082 --> 01:15:40,407
And so, gradient descent is the most
basic tool to minimize functions.

1030
01:15:40,407 --> 01:15:46,640
So it's the conceptually first thing to
know, but then the sort of last minute.

1031
01:15:46,640 --> 01:15:49,763
What I wanted to explain is actually,

1032
01:15:49,763 --> 01:15:54,850
we might have 40 billion tokens
in our corpus to go through.

1033
01:15:54,850 --> 01:15:59,470
And if you have to work out
the gradient of your objective function

1034
01:15:59,470 --> 01:16:04,426
relative to a 40 billion word corpus,
that's gonna take forever,

1035
01:16:04,426 --> 01:16:09,740
so you'll wait for an hour before
you make your first gradient update.

1036
01:16:09,740 --> 01:16:13,654
And so, you're not gonna be able train
your model in a realistic amount of time.

1037
01:16:13,654 --> 01:16:17,549
So for basically,
all neural nets doing naive batch

1038
01:16:17,549 --> 01:16:22,160
gradient descent hopeless algorithm,
you can't use that.

1039
01:16:22,160 --> 01:16:24,125
It's not practical to use.

1040
01:16:24,125 --> 01:16:28,724
So instead, what we do Is used
stochastic gradient descent.

1041
01:16:28,724 --> 01:16:34,100
So, the stochastic gradient descent or
SGD is our key tool.

1042
01:16:34,100 --> 01:16:40,263
And so what that's meaning is, so
we just take one position in the text.

1043
01:16:40,263 --> 01:16:45,253
So we have one center word and
the words around it and we say, well,

1044
01:16:45,253 --> 01:16:49,979
let's adjust it at that one
position work out the gradient with

1045
01:16:49,979 --> 01:16:52,830
respect to all of our parameters.

1046
01:16:52,830 --> 01:16:57,422
And using that estimate of
the gradient in that position,

1047
01:16:57,422 --> 01:17:00,902
we'll work a little bit in that direction.

1048
01:17:00,902 --> 01:17:05,047
If you think about it for
doing something like word vector learning,

1049
01:17:05,047 --> 01:17:09,551
this estimate of the gradient is
incredibly, incredibly noisy, because

1050
01:17:09,551 --> 01:17:14,580
we've done it at one position which just
happens to have a few words around it.

1051
01:17:14,580 --> 01:17:18,724
So the vast majority of the parameters
of our model, we didn't see at all.

1052
01:17:18,724 --> 01:17:22,489
So, it's a kind of incredibly
noisy estimate of the gradient.

1053
01:17:22,489 --> 01:17:26,656
walking a little bit in that direction
isn't even guaranteed to have make you

1054
01:17:26,656 --> 01:17:29,910
walk downhill,
because it's such a noisy estimate.

1055
01:17:29,910 --> 01:17:33,317
But in practice, this works like a gem.

1056
01:17:33,317 --> 01:17:36,049
And in fact, it works better.

1057
01:17:36,049 --> 01:17:37,622
Again, it's a win, win.

1058
01:17:37,622 --> 01:17:42,115
It's not only that doing things
this way is orders of magnitude

1059
01:17:42,115 --> 01:17:44,829
faster than batch gradient descent,

1060
01:17:44,829 --> 01:17:50,410
because you can do an update after you
look at every center word position.

1061
01:17:50,410 --> 01:17:55,200
It turns out that neural
network algorithms love noise.

1062
01:17:55,200 --> 01:18:00,965
So the fact that this gradient descent,
the estimate of the gradient is noisy,

1063
01:18:00,965 --> 01:18:05,784
actually helps SGD to work better
as an optimization algorithm and

1064
01:18:05,784 --> 01:18:07,775
neural network learning.

1065
01:18:07,775 --> 01:18:09,900
And so, this is what we're
always gonna use in practice.

1066
01:18:09,900 --> 01:18:14,129
I have to stop there for today even
though the fire alarm didn't go off.

1067
01:18:14,129 --> 01:18:14,866
Thanks a lot.

1068
01:18:14,866 --> 01:18:17,940
>> [APPLAUSE]

